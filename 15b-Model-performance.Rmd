```{r load_models_VIb, warning=FALSE, message=FALSE, echo=FALSE}
source("models/models_titanic.R")
```

# Model performance {#modelPerformance}

## Introduction {#modelPerformanceIntro}

In this chapter, we present methods that are useful for the evaluation of a model performance. The methods may be applied for several purposes.
 
* Model assessment; we want to know how good is the model and how reliable are model predictions. How frequent and how large errors we may expect.
* Model comparison; we want to compare two or more models in order to choose the best one.
* Out-of-sample and out-of-time comparisons; we want to monitor model performance on a new data to check if the performance is stable.

Depending of the model objective (classification, regression, survival) different model performance metrics are being used. Moreover the list of useful metrics is growing as new applications emerge. In this chapter we will focus on the most common measures of model performance. 


## Intuition {#modelPerformanceIntuition}

Most model performance measures are based on comparison of model predictions versus known values of a target variable. An ideal model will have predictions equal to true values of a target variable. In practice it is never the case and we need to quantify the disagreement.

Here we consider only models that output a single numeric value per observation, so the prediction may be equal, lower of higher than the true value. In applications one can penalize model differently for large or small errors, or differently for under/over predictions. This is why we need different performance measures.
In the best possible scenario we can specify a single model performance measure before the model is created and then we optimize model for this measure. But in practice the more common scenario is to have few performance measures that are often selected after the model is created.


## Method {#modelPerformanceMethod}

Here we assume that we have data split into a train and test sets. Model is created on the train set and the independent test set is used to assess the model performance. For more sophisticated validation strategies (cross validation, repeated cross validation, two level cross validation) see [@AppliedPredictiveModeling2013].

Let the test data $X^*$ have $n^*$ observations for a set of $p$ explanatory variables. Denote by  $\widetilde{y}=(f(x_1),\ldots,f(x_n))$ the vector of predictions for model $f()$ for all the observations. Let $y$ denote the vector of observed values of the dependent variable $Y$.

### Regression

The most popular model performance measure for regression problems is the mean square error defined as

$$
MSE(f, X^*) = \frac{1}{n^*} \sum_{i}^{n^*} (f(X^*_i) - y_i)^2.
$$

MSE is a convex differentiable function so it has good properties from the optimization perspective.
Large differences between $f(X^*_i) - y_i$ have high impact of the MSE so it is sensitive to outliers.

MSE is not on the same scale as the original target variable, so more interpretable variant of this measure is the Root Mean Square Error (RMSE)

$$
RMSE(f, X^*) = \sqrt{MSE(f, X^*)}.
$$

RMSE has the same unit as the target variable. A popular variation of the RMSE is it's normalized version called $R^2$ defined as

$$
R^2(f, X^*) = 1 - \frac{MSE(f, X^*)}{MSE(baseline, X^*)}.
$$

Here the $baseline$ model is some naive solution, like an average value of a target variable. $R^2$ is normalized in a sense that best possible model will have $R^2 = 1$ while $R^2 = 0$ means that we are not better than a baseline (still we can do worse). Another interpretation of $R^2$ is that it's a ratio of variance explained by a model over total variance.


### Classification

Classification models are a bit harder to evaluate since the target variable takes values in some finite set of possible classes, while model predictions are numeric scores (often probabilities). For a moment let's assume that we are dealing with binary classification, i.e. two classes, let's call them `has` / `has not`. Model predictions are discretized with some given threshold $t$. If model prediction for a class $C$ is higher than $t$ then we will say that model predicts $C$.

After such discretisation the model results on a test data $X^*$ can be summarised by a following table:

|                       | True value: `has`                 | True value: `has not`             | Total                      |
|-----------------------|-----------------------------------|-----------------------------------|----------------------------|
|  Predicted: `has`     | True Positive: TP                 | False Positive: FP (type I error) | #predicted as `has`: P     |
|  Predicted: `has not` | False Negative FN (type II error) | True Negative TN                  | #predicted as `has not`: N |
| Total                 | #truly `has`                      | #truly `has has not`              | $n^*$                      |


The simplest measure of performance is the model accuracy
$$
ACC = \frac{TP+TN}{n^*}.
$$
The measure has clear interpretation, but it is not working well if classes are imbalanced.

In machine learning there popular measures are:
$$
Precision = \frac{TP}{TP + FP},
$$
which measure how frequent are true positive predictions among all predictions,
$$
Recall = \frac{TP}{TP + FN},
$$
which measure how frequent are true positive predictions among truly positive cases, and their harmonic mean F1 score
$$
F1\  score = \frac{Precision * Recall}{Precision + Recall}.
$$

In statistics, especially in applications to medicine, popular measures are
$$
Sensitivity = \frac{TP}{TP + FN},
$$
which measure how frequent are true positive predictions among truly positive cases (it's the Recall in the ML world),
$$
Specificity = \frac{TN}{FP + TN},
$$
which measure how frequent are true negative predictions among truly negative cases.


Of course all these measures depend on the choice of $t$. To get rid of this additional free parameter a common approach to summarise results for classification is the Receiver Operating Characteristic curve (ROC curve) that summarise Sensitivity and 1-Specificity for all possible values of $t$ in a single plot.


```{r titanicROC, warning=FALSE, message=FALSE, echo=FALSE, fig.width=5, fig.height=5, fig.cap="(fig:titanicROC) Blue curve: ROC for random forest model and for the Titanic dataset. Black line: diagonal."}
library("ingredients")
library("auditor")
library("randomForest")
set.seed(1313)
titanic_rf_v6 <- randomForest(survived ~ class + gender + age + sibsp + parch + fare + embarked, 
                           data = titanic_imputed)

explain_titanic_rf_v6 <- explain(model = titanic_rf_v6, 
                                 data = titanic[, -9],
                                 y = titanic$survived == "yes", 
                                 label = "Random Forest v6")

eva_rf <- model_evaluation(explain_titanic_rf_v6)

# plot results
plot_roc(eva_rf) + coord_fixed() + geom_abline(slope = 1, intercept = 0)
```


ROC curve is very informative, but for model comparison it is better to have one of jsr few numbers that summarise model performance. So a very popular coefficient that summarises the ROC curve is the AUC value, which stands for Area Under the ROC Curve and is calculated as the area under the ROC.

 


## Example

### Titanic data {#modelPerformanceTitanic}

Let us consider again the random-forest model `titanic_rf_v6` (see Section 4.1.3) for the Titanic data.

Once the explainer is created we can now use `score_*` functions from the `auditor` package to calculate variouse scores. E.g. `score_auc` calculates AUC while `score_f1` calculates `F1` measure (you need to specify a threshold that will turn numeric predictions into a binary responses).

```
library("ingredients")
library("auditor")
library("randomForest")
set.seed(1313)
titanic_rf_v6 <- randomForest(survived ~ class + gender + age + sibsp + parch + fare + embarked, 
                           data = titanic_imputed)

explain_titanic_rf_v6 <- explain(model = titanic_rf_v6, 
                                 data = titanic[, -9],
                                 y = titanic$survived == "yes", 
                                 label = "Random Forest v6")

score_auc(explain_titanic_rf_v6)
score_f1(explain_titanic_rf_v6, cutoff = 0.5)
model_performance(explain_titanic_rf_v6, cutoff = 0.5)


# plot results
eva_rf <- model_evaluation(explain_titanic_rf_v6)
plot_roc(eva_rf)
plot_lift(eva_rf)

```


### Appartments data {#modelPerformanceApartments}

Let us consider the random-forest model `apartments_rf_v6` (see Section 4.2.3) for the apartments data.




## Pros and cons {#modelPerformanceProsCons}


In this chapter we described a number of measures for model performance. Below you will find brief pros vs cons discussion for each measure separately.

Root Mean Square Error (RMSE) is frequently used and reported for regression models. A disadvantage is that single outliers may affect this measure a lot.

R-square $R^2$ measure is commonly used for linear models. It shares same disadvantages as the RMSE.

Descriptive statistics like box-plot or histogram for residuals are informative. But they are hard to summarize with a single number therefore they are hard to compare between models.


For classification the AUC is a very popular measure of model performance. But it is related with an unrealistic scenario that we consider all possible cutoffs while in applications in most cases we work with a single specified cutoff.

On the other hand measures like Accuracy, F1, Precision and Recall are easy to interpret, but they depend on a single specific cutoff.

Fr this reason it is common to report and comare few measures of model performance, each reflects different aspect of the model.





## Code snippets for R {#modelPerformanceR}


In this section, we present the key features of the `auditor` R package [@auditor] which is a part of the DrWhy.AI universe. The package covers all methods presented in this chapter. It is available on CRAN and GitHub. More details and examples can be found at https://modeloriented.github.io/auditor/.

Note that there are also other R packages that offer similar functionality, like `mlr` [@mlr], `caret` [@caret], `tidymodels` [@tidymodels], `ROCR` [@ROCR].


```
library(auditor)
score_rmse(rf_apartments)

apartments_lm_v5 <- lm(m2.price ~ ., data = apartments)
apartments_lm_v5
library("randomForest")
set.seed(72)
apartments_rf_v5 <- randomForest(m2.price ~ ., data = apartments)
apartments_rf_v5

explain_apartment_lm_v5 <- explain(model = apartments_lm_v5, 
                                 data = apartments,
                                 y = apartments$m2.price, 
                                 label = "Linear model v5")

explain_apartment_rf_v5 <- explain(model = apartments_rf_v5, 
                                 data = apartments,
                                 y = apartments$m2.price, 
                                 label = "Random Forest v5")


score_rmse(explain_apartment_lm_v5)
score_rmse(explain_apartment_rf_v5)

# plot results
eva_rf <- model_evaluation(explain_titanic_rf_v6)
plot_roc(eva_rf)

```

