# Feature effects {#variableEngeneering}

In following chapters we introduce tools for extraction of the information between model response and individual model inputs. These tools are useful to summarize how ,,in general'' model responds to the input of interest. All presented approaches are based on Ceteris Ceteris Paribus Profiles introduced in Chapter \@ref{ceterisParibus} but they differ in a way how individual profiles are merged into a global model response.

We use the term ,,feature effect'' to refer to global model response as a function of single or small number of model features. 
Methods presented in this chapter are useful for extraction information of feature effect, i.e. how a feature is linked with model response. There are many possible applications of such methods, for example:

* Feature effect may be used for feature engineering. The crude approach to modeling is to fit some elastic model on raw data and then use feature effects to understand the relation between a raw feature and model output and then to transform model input to better fit the model output. Such procedure is called surrogate training. In this procedure an elastic model is trained to learn about link between a feature and the target. Then a new feature is created in a way to better utilized the feature in a simpler model [@SAFE-arxiv]. In the next chapters we will show how feature effects can be used to transform a continuous variable in to a categorical one in order to improve the model behavior.
* Feature effect may be used for model validation.  Understanding how a model utilizes a feature  may be used as a validation of a model against domain knowledge. For example if we expect monotonic relation or linear relation then such expectations can be verified. Also if we expect smooth relation between model and its inputs then the smoothness can be visually examined. In the next chapters we will show how feature effects can be used to warn a model developer that model is unstable and should be regularized.
*  In new domains an understanding of a link between model output and the feature of interest may increase our domain knowledge. It may give quick insights related to the strength or character of the relation between a feature of interest and the model output. 
* The comparison of feature effects between different models may help to understand how different models handle particular features. In the next chapters we will show how feature effects can be used learn limitations of particular classes of models.


```{r, warning=FALSE, message=FALSE, echo=FALSE}
source("models/models_titanic.R")
```

## Global level vs instance level explanations

The plot below shows Ceteris Paribus Profiles for the random forest `rf_5` for 10 selected passengers. 
Different profiles behave differently. In following chapter we discuss different approaches to aggregation of such profiles into model level feature effects.

```{r pdp_part_1, warning=FALSE, message=FALSE, echo=FALSE, fig.width=6, fig.height=4, fig.cap="Ceteris Paribus profiles for 10 passangers and the random forest model"}
library("ingredients")
set.seed(1313)

selected_passangers <- select_sample(titanic, n = 10)
cp_rf <- ceteris_paribus(explain_titanic_rf, selected_passangers, variables = "age")

plot(cp_rf) +
  show_observations(cp_rf, selected_variables = "age") +
  ggtitle("Predicted survival probability", "For a random forest model 'rf_5' / the 'titanic' dataset") +
  scale_y_continuous(label=scales::percent, limits = c(0,1)) 
```
