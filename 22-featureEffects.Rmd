# Feature effects {#variableEngeneering}

Methods presented in this chapter are useful for extraction information of feature effect, i.e. how a feature is linked with model response. There are many possible applications of such methods, for example:

* Feature effect may be used for feature engineering. Surrogate training is a procedure in which an elastic model is trained to learn about link between a feature and the target. Then a new feature is created in a way to better utilized the feature in a simpler model.
* Understanding how the model utilize a feature  may be used as a validation of a model against domain knowledge. For example if we expect monotonic relation or linear relation then such assumptions may be testes
* Understanding of a link between target and the feature may increase our domain knowledge.
* Comparison of feature effects between different models may help us to understand how different models handle particular features.



```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(titanic)
library(randomForest)
library(DALEX)
library(dplyr)

titanic_small <- titanic_train[,c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")]
titanic_small$Survived <- factor(titanic_small$Survived)
titanic_small$Sex <- factor(titanic_small$Sex)
titanic_small$Embarked <- factor(titanic_small$Embarked)
titanic_small <- na.omit(titanic_small)
rf_model <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
                         data = titanic_small)
predict_fuction <- function(m,x) predict(m, x, type="prob")[,2]
rf_explain <- explain(rf_model, data = titanic_small, 
                      y = titanic_small$Survived == "1", label = "RF",
                      predict_function = predict_fuction)


#
# TWORZYMY MODELE

## random forest
rf_model <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                         data = titanic_small)
predict_rf_fuction <- function(m,x) predict(m, x, type="prob")[,2]
explainer_rf <- explain(rf_model, data = titanic_small,
                      y = titanic_small$Survived == "1", label = "RF",
                      predict_function = predict_rf_fuction)

## GLM
glm_model <- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                         data = titanic_small, family = "binomial")
explainer_glm <- explain(glm_model, data = titanic_small,
                      y = titanic_small$Survived == "1", label = "GLM")


## splines
library("rms")
rms_model <- lrm(Survived == "1" ~ Pclass + Sex + rcs(Age) + SibSp +
                   Parch + Fare + Embarked, titanic_small)
predict_rms_fuction <- function(m,x) predict(m, x, type="fitted")
explainer_rms <- explain(rms_model, data = titanic_small,
                         y = titanic_small$Survived == "1", label = "RMS",
                         predict_function = predict_rms_fuction)

## GBM
library("gbm")
#titanic_gbm <- gbm(Survived == "1" ~ Age + Pclass + Sex, data = titanic_small, n.trees = 1000)
titanic_gbm <- gbm(Survived == "1" ~ Pclass + Sex + Age + SibSp +
                     Parch + Fare + Embarked, data = titanic_small, n.trees = 15000)
predict_gbm_fuction <- function(m,x) predict(m, x,
                                             n.trees = 15000, type = "response")
explainer_gbm <- explain(titanic_gbm,
                         data = titanic_small, y = titanic_small$Survived == "1",
                         label = "GBM",
                         predict_function = predict_gbm_fuction)

```



## Partial Dependency Plots {#partialDependence}

One of the first and the most popular tools for model understanding
are Partial Dependence Plots (sometimes named Partial Dependence Profiles) [@Friedman00greedyfunction]. 

PDP was introduced by Friedman in 2000 in the paper devoted to Gradient Boosting Machines (GBM) - new type of complex yet effective models. For many years PDP as sleeping beauties stay in the shadow of the boosting. It has changed in recent years.

General idea is to show how the expected model response behaves as a function of a selected feature. Here the word ,,expected''  means averaged over the population.
We can think about them as about an average from Ceteris Paribus Profiles introduced in \@ref{ceterisParibus}.

Let's see how they are constructed step by step. 
Here we will use a *random forest model* created for the *titanic* dataset. Examples are related to a single variable *Age*.

1. Calculate Ceteris Paribus profiles for observations from the dataset

As it was introduced in \@ref{ceterisParibus} Ceteris Paribus profiles are calculated for observations. They show how model response change is a selected variable in this observation is modified.

$$
CP^{f, j, x}(z) := f(x|^j = z).
$$

Such profiles can be calculated for example with the `ceteris_paribus{ingredients}` function.

```{r, eval=FALSE}
library("ingredients")
selected_passangers <- select_sample(titanic_small, n = 100)
cp_rf <- ceteris_paribus(explainer_rf, selected_passangers)
```

So for a single model and a single variable we get a bunch of *what-if* profiles. In the figure \@ref{pdp_part_1} we show an example for 100 observations. Despite some variation (random forest are not as stable as we would hope) we see that most profiles are decreasing. So the older the passengers is the lower is the survival probability.

```{r pdp_part_1, warning=FALSE, message=FALSE, echo=FALSE, fig.width=5, fig.height=5, fig.cap="Ceteris Paribus profiles for 100 observations, the Age variable and the random forest model"}
library("ingredients")
set.seed(1313)

selected_passangers <- select_sample(titanic_small, n = 100)
cp_rf <- ceteris_paribus(explainer_rf, selected_passangers)

plot(cp_rf, selected_variables = "Age", color = "grey") +
  xlab("Passanger Age") + ylab("Survival probability") +
  ggtitle("Ceteris Paribus profiles", "For a random forest model / Titanic data") +
  scale_y_continuous(label=scales::percent) 
```

2. Aggregate Ceteris Paribus into a single Partial Dependency Profile

In the most common formulation Partial Dependency Plots are expected values for CP profiles (see `pdp` package [@pdp]).


$$
g_i(z) = E_{x_{-i}}[ f(x|^i = z, x^{-i}) ].
$$

Of course, this expectation cannot be calculated directly as we do not know fully neither the distribution of $x_{-i}$ nor the $f()$. Yet this value may be estimated by 

$$
\hat g_i(z) = \frac{1}{n} \sum_{j=1}^{n} f(x|^i = z, x_j^{-i}).
$$

Such average can be calculated with the `aggregate_profiles{ingredients}` function.

```{r, eval=FALSE}
library("ingredients")
selected_passangers <- select_sample(titanic_small, n = 100)
cp_rf <- ceteris_paribus(explainer_rf, selected_passangers)
pdp_rf <- aggregate_profiles(cp_rf, selected_variables = "Age")
```

So for a single model and a single variable we get a profile. See an example in figure \@ref{pdp_part_2}. It is much easier than following 100 separate curves, and in cases in which Ceteris Paribus are more or less parallel, the Partial Dependency is a good summary of them. 

The average response is of course more stable (as it's an average) and in this case is more or less a decreasing curve. It's much easier to notice that the older the passenger is the lower the survival probability. 
Moreover it is easier to notice that the largest drop in survival changes happen for teenagers. On average the survival for adults is 30 percent points smaller than for kids.

```{r pdp_part_2, warning=FALSE, message=FALSE, echo=FALSE, fig.width=5, fig.height=5, fig.cap="Partial Dependency profile as an average for 100 observations"}

cp_rf <- ceteris_paribus(explainer_rf, selected_passangers)
pdp_rf <- aggregate_profiles(cp_rf, selected_variables = "Age")

plot(cp_rf, selected_variables = "Age", color = "grey") +
   show_aggreagated_profiles(pdp_rf, size = 3) +
  xlab("Passanger Age") + ylab("Survival probability") +
  ggtitle("Partial Dependency profile", "For a random forest model / Titanic data") +
  scale_y_continuous(label=scales::percent) 
```

### Interactions and Partial Dependency profiles

As we said in the previous section, Partial Dependency is a good summary if Ceteris Paribus profiles are similar, i.e. parallel. But it may happen that the variable of interest is in interaction with some other variable. Then profiles are not parallel because the effect of variable of interest depends on some other variables.

So on one hand it would be good to summaries all this Ceteris Paribus profiles with smaller number of profiles. But on another hand a single aggregate may not be enough.
To deal with this problem we propose to cluster Ceteris Paribus profiles and check how homogenous are these profiles.

The most straightforward approach would be to use a method for clustering, like k-means algorithm or hierarchical clustering, and see how these cluster of profiles behave. Once clusters are established we can aggregate within clusters in the same way as in case of Partial Dependency Plots.

Such clusters can be calculated with the `cluster_profiles{ingredients}` function. We choose the hierarchical clustering with Ward linkage as it gives most stable results.

```{r, eval=FALSE}
library("ingredients")
selected_passangers <- select_sample(titanic_small, n = 100)
cp_rf <- ceteris_paribus(explainer_rf, selected_passangers)
clust_rf <- cluster_profiles(cp_rf, k = 3, selected_variables = "Age")
```

So for a single model and a single variable we get $k$ profiles. The common problem in clustering is the selection of $k$. However in our case, as it's an exploration, the problem is simpler, as we are interesting if $k=1$ (Partial Dependency is a good summary) or not (there are some interactions). 

See an example in Figure \@ref{pdp_part_4}. It is easier to notice that Ceteris Paribus profiles can be groups in three clusters. Group of passengers with a very large drop in the survival (cluster 1), moderate drop (cluster 2) and almost no drop in survival (cluster 3). Here we do not know what other factors are linked with these clusters, but some additional exploratory analysis can be done to identify these factors.

```{r pdp_part_4, warning=FALSE, message=FALSE, echo=FALSE, fig.width=5, fig.height=5, fig.cap="Cluster profiles for 3 clusters over 100 Ceteris Paribus profiles"}

clust_rf <- cluster_profiles(cp_rf, k = 3, selected_variables = "Age")

plot(cp_rf, color = "grey", selected_variables = "Age") +
  show_aggreagated_profiles(clust_rf, color = "_label_", size = 2) +
  xlab("Passanger Age") + ylab("Survival probability") +
  ggtitle("Profiles of 3 clusters", "For a random forest model / Titanic data") +
  scale_y_continuous(label = scales::percent) 
```

### Groups of Partial Dependency profiles

Once we see that variable of interest may be in interaction with some other variable, it is tempting to look for the factor that distinguish clusters.

The most straightforward approach is to use some other variable as a grouping variable. This can be done by setting the `groups` argument in the `aggregate_profiles{ingredients}` function. 

```{r, eval=FALSE}
library("ingredients")
selected_passangers <- select_sample(titanic_small, n = 100)
cp_rf <- ceteris_paribus(explainer_rf, selected_passangers)
pdp_Sex_rf <- aggregate_profiles(cp_rf, selected_variables = "Age",
				groups = "Sex")
```

See an example in Figure \@ref{pdp_part_5}. Clearly there is an interaction between Age and Sex. The survival for woman is more stable, while for man there is more sudden drop in Survival for older passengers.
Check how the interaction for `Pclass` (passenger class) looks like.

```{r pdp_part_5, warning=FALSE, message=FALSE, echo=FALSE, fig.width=5, fig.height=5, fig.cap="Grouped profiles with respect to the Sex variable"}

pdp_Sex_rf <- aggregate_profiles(cp_rf, selected_variables = "Age",
				groups = "Sex")

plot(cp_rf, color = "grey", selected_variables = "Age") +
  show_aggreagated_profiles(pdp_Sex_rf, color = "_label_", size = 2) +
  xlab("Passanger Age") + ylab("Survival probability") +
  ggtitle("Groups of Ceteris Paribus Profiles defined by the Sex feature", "For a random forest model / Titanic data") +
  scale_y_continuous(label = scales::percent) 
```


### Model comparisons with Partial Dependency Plots

Contrastive comparisons of Partial Dependency Plots are useful not only for subgroups of observations but also for model comparisons.

Why one would like to compare models? There are at least three reasons for it.

* *Agreement of models will calm us.* Some models are known to be more stable other to be more elastic. If profiles for models from these two classes are not far from each other we can be more convinced that elastic model is not over-fitted.
* *Disagreement of models helps to improve.* If simpler interpretable model disagree with an elastic model, this may suggest a feature transformation that can be used to improve the interpretable model. For example if random forest learned non linear relation then it can be captures by a linear model after suitable transformation.
* *Validation of boundary conditions.* Some models are know to have different behavior on the boundary, for largest or lowest values. Random forest is known to shrink predictions towards the average, while support vector machines are known to have larger variance at edges. Contrastive comparisons may help to understand differences in boundary behavior.

Generic `plot{ingredients}` function handles multiple models as consecutive arguments. 

```{r, eval=FALSE}
library("ingredients")
plot(pdp_rf, pdp_glm, pdp_gbm, selected_variables = "Age", color = "_label_")
```

See an example in Figure \@ref{pdp_part_7}. Random forest is compared with gradient boosting model and generalized linear model (logistic regression). All three models agree when it comes to a general relation between Age and Survival. Logistic regression is of course the most smooth. Gradient boosting has on average higher predictions than random forest.

```{r pdp_part_7, warning=FALSE, message=FALSE, echo=FALSE, fig.width=5, fig.height=5, fig.cap="Comparison on three predictive models with different structures."}

cp_gbm <- ceteris_paribus(explainer_gbm, selected_passangers)
pdp_gbm <- aggregate_profiles(cp_gbm, selected_variables = "Age")

cp_glm <- ceteris_paribus(explainer_glm, selected_passangers)
pdp_glm <- aggregate_profiles(cp_glm, selected_variables = "Age")

cp_rf <- ceteris_paribus(explainer_rf, selected_passangers)
pdp_rf <- aggregate_profiles(cp_rf, selected_variables = "Age")

plot(pdp_rf, pdp_glm, pdp_gbm, selected_variables = "Age", color = "_label_", size = 2) +
  xlab("Passanger Age") + ylab("Survival probability") +
  ggtitle("Partial Dependency Profiles", "For random forest / gradient boosting / logistic model") +
  ggtitle("Ceteris Paribus profiles for three models") +
  scale_y_continuous(label = scales::percent, limits = c(0,1))
```


### Correlation between features

One of the largest advantages of the Partial Dependency Profiles is that they are easy to explain, as they are just an average across Ceteris Paribus profiles. But one of the largest disadvantages lies in assumptions of CPs. Profiles are created based on assumption that it makes sense to change variable $x^i$ independently from all other variables $x^{-i}$.

But this may not have sense at all. Features like $surface$ and $number.or.rooms$ are strongly correlated as apartments with larger number of rooms usually have larger surface. It may makes no sense to consider an apartment with 10 rooms and 20 square meters, so it may be misleading to change $x^{surface}$ independently from $x^{number.of.rooms}$.

There are several attempts to fix this problem. One of the most known are Accumulated Local Effects Plots (ALEPlots) [@R-ALEPlot] or Local Conditional Expectation Profiles (LCE) [TODO: referencja do pracy Rafala]?


## Merging Path Plots

[@demsar2018]

[@RJ2017016]
[@MAGIX]






[@R-factorMerger]


[@Strobl2007] 
[@Strobl2008] 
- variable importance

[@2018arXiv180101489F]

Beware Default Random Forest Importances

Terence Parr, Kerem Turgutlu, Christopher Csiszar, and Jeremy Howard
March 26, 2018.

http://explained.ai/rf-importance/index.html



[@R-factorMerger]

```{r, warning=FALSE, message=FALSE}
library(factorMerger)
```

# Other topics


[@R-randomForestExplainer]
[@R-ICEbox]
[@R-ALEPlot]

[@R-modelDown]

