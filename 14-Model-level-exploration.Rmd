# Model-level exploration {#modelLevelExploration}

In Part I, we focused on instance-level explainers, which help to understand how a model yields a prediction for a single observation (instance). 

In Part II, we concentrate on model-level explainers, which help to understand how model's predictions perform in general, for an entire set of observations. [TOMASZ: I FIND IT AWKWARD TO TALK ABOUT A "POPULATION" HERE. MODELS ARE USUALLY OBTAINED FOR SAMPLES, AND WE CAN ONLY ASSESS THE BEHAVIOR OF THE MODEL WRT THE SAMPLE.]

<!--- some population of interest. This is the main difference from the instance level explainers that were focused on a model behaviour around a single observation. Model level explainers work in the context of a population or subpopulation.
--->

We can think about the following situations as examples:
* We may want to learn which variables are ``important'' in the model. For instance, we may be interested in predicting the risk of heart attack. Explanatory variables are obtained based on results of some medical examinations. If some of the examinations do not influence model's predictions, we could simplify the model by removing the corresponding variables.
* We may want to understand how a selected variable influences model's predictions? For instance, we may be interested in predicting prices of apratments. Apartment's location is an important factor, but we may want to know which locations lead to higher prices? 
* We may want to discover whether there are any observations, for which the model does not seem to give correct predictions. For instance, for a model predicting the probability of survival after a risky treatment, we might know whether there are patients for whom the model predictions are extremely wrong. Identifying such a group of patients might point to, for instance, an incorrect form of or even a missed explanatory variable. 

<!--- The examples mentioned above are linked with either model diagnostic (checking if model behaves alog our expectations) or knowledge extraction (model was trained to extract some knowledge about the discipline). [TOMASZ: NOT SURE ABOUT THE PURPOSE OF THE REMARK.]
--->

Model-level explainers focus on four main aspects of a model. 

* Variable's importance: which explantory variables are ``important'', and which are not?
* Variable's effect: how does a variable influence model's predictions? 
* Model's performance: how ``good'' is the model? Is one model ``better'' than another?
* Model's fit: are there any observations for which model's predictions are clearly wrong? [TOMASZ: HOW IS IT DIFFERENT FROM MODEL'S PERFORMANCE?]

In all cases, measures capturing a particular aspect of the model have to be defined. We will discuss them in subsequent chapters. In particular, in Chapter \@ref{featureImportance}, we discuss methods for assessment of explanatory variable's importance. Chapter \@ref{featureEffects} is devoted to approaches that allow evaluation of a variable's effect on model's predictions. [TOMASZ: TO BE FINISHED.]

[TOMASZ: THIS CHAPTER IS A BIT THIN.]
<!--- [TOMASZ: I DO NOT FIND THE LAWS USEFUL AT THIS POINT]
## A bit of philosophy: Three Laws for Model Level Explanations

In the spirit of three laws introduces in the chapter \@ref(three-single-laws) here we propose three laws for model level explanations.


* **Variable importance.** For every model we shall be able to understand which variables are important and which are not.
* **Model audit.** For every model we shall be able to verify basic check like if residuals are correlated with variables and if there are unusual observations.
* **Second opinion.**  For every model we shall be able to compare it against other models to verify if they capture different stories about the data.
--->

