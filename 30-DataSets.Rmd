# Appendixes {-}

# Data Sets {#DataSets}

## Predict survival on the Titanic {#TitanicDataset}

Sinking of the RMS Titanic is one of the deadliest maritime disasters in history (during peacetime). Over 1500 people died as a consequence of collision with an iceberg. Thanks to projects like *Encyclopedia titanica* `https://www.encyclopedia-titanica.org/` we have a very rich and precise data about passagers. This dataset is avaliable in the `titanic` dataset.


```{r, warning=FALSE, message=FALSE}
library("titanic")
head(titanic_train)
```

Feature of interest is the binary variable `Survived`. Let's build some predictive models for this variable.

First we need to do some data preprocessing.

```{r, warning=FALSE, message=FALSE}
titanic_small <- titanic_train[,c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")]
titanic_small$Survived <- factor(titanic_small$Survived)
titanic_small$Sex <- factor(titanic_small$Sex)
titanic_small$Embarked <- factor(titanic_small$Embarked)
titanic_small <- na.omit(titanic_small)

titanic_train[760,]
titanic_small[760,]
```


### Random Forest

Now we can create models. Let's start with Random Forest

```{r, warning=FALSE, message=FALSE}
library("randomForest")
rf_model <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + 
                           Parch + Fare + Embarked, 
                           data = titanic_small)
rf_model
```

### Logistic regression

Logistic model is a generalized linear moedl with binomial family and logit link function.

```{r, warning=FALSE, message=FALSE}
glm_model <- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                         data = titanic_small, family = "binomial")
summary(glm_model)
```

### Splines with RMS


```{r, warning=FALSE, message=FALSE}
library("rms")
rms_model <- lrm(Survived == "1" ~ Pclass + Sex + rcs(Age) + SibSp +
                   Parch + Fare + Embarked, titanic_small)
rms_model
```

### Gradient boosting


```{r, warning=FALSE, message=FALSE}
library("gbm")
titanic_gbm <- gbm(Survived == "1" ~ Pclass + Sex + Age + SibSp +
                     Parch + Fare + Embarked, data = titanic_small, n.trees = 15000)
titanic_gbm
```




## Hire or Fire? HR in Call Center {#HRdataset}

In this chapter we present an artificial dataset from Human Resources department in a Call Center. 

The dataset is available in the `DALEX` package [@R-DALEX]. Each row corresponds to a single employee in a call center. Features like gender, age, average number of working hours per week, grade from the last evaluation and level of salary are used as predictive features.

The goal here is to first build a model, that will guess when to fire and when to promote an employer, so it's a classification problem with three classes. 

Why we need such model? We want to have objective decisions. That will not be subject to personal preferences of a manager. But is it possible to have an objective model? Would it be fair or it will just replicate some unfairness?

We will use this example to show how to use prediction level explainers to better understand how the model works for selected cases.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(HR)
```

In this book we are focused on model exploration rather than model building, thus for sake ok simplicity we will use two default models created with random forest [@R-randomForest] and generalized linear model [@R-nnet].

```{r, warning=FALSE, message=FALSE }
set.seed(59)
library("randomForest")
model_rf <- randomForest(status ~ gender + age + hours + evaluation + salary, data = HR)

library("nnet")
model_glm <- multinom(status ~ gender + age + hours + evaluation + salary, data = HR)
```

## How much does it cost? Price prediction for a square meter {#apartmentsDataset}

In this chapter we present an artificial dataset related to prediction of prices for appartments in Warsaw. This dataset wil be used to discuss  pros and cons for different techniques of model level explainers. 

The dataset is available in the `DALEX` package [@R-DALEX]. Each row corresponds to a single apartment. Features like surface, number of rooms, district or floor are used as predictive features.

The problem here is to predict price of a square meter for an appartment, so it's a regression problem with continouse outcome. 

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(apartments)
```

The goal here is to predict average price for square meter for an apartment. Let's build a random forest model with `randomForest` package  [@R-randomForest].

```{r, warning=FALSE, message=FALSE}
library("randomForest")
model_rf <- randomForest(m2.price ~ construction.year + surface + floor + no.rooms + district, data = apartments)
model_rf
```

And a linear model.

```{r, warning=FALSE, message=FALSE}
model_lm <- lm(m2.price ~ construction.year + surface + floor + no.rooms + district, data = apartments)
model_lm
```


