# Ceteris-paribus Profiles {#ceterisParibus}

## Introduction {#CPIntro}

Chapters \@ref(breakDown) -- \@ref(LIME) focused on the methods that quantified the importance of explanatory variables based on the decomposition of a single-instance prediction into components that could be attributed to particular variables. In this chapter, we focus on a method that analyses the effect of a selected variable in terms of changes of the model prediction induced by changes in the variable's values. The method is based on the *ceteris paribus* principle. *"Ceteris paribus"* is a Latin phrase meaning "other things held constant" or "all else unchanged." The method examines the influence of an explanatory variable by assuming that effects of all other variables are unchanged. The main goal is to understand how changes in the values of the variable affects model predictions. 

Explanation tools (explainers) presented in this chapter are linked to the second law introduced in Section \@ref(three-single-laws), i.e., the law of "Prediction's speculation." This is why the tools are also known as *What-If model analysis* or *Individual Conditional Expectations* [@ICEbox]. It appears that it is easier to understand how a black-box model is working if we can explore the model by investigating the influence of explanatory variables separately, changing one at a time. 

<!---
is the model response, or to what-if analysis, how model response would change if input is changes. It is important to remember that the what-if analysis is performed not in the sense of causal modeling, but in the sense of model exploration. We need causal model to do causal inference for the real-world phenomena. Here we focus on explanatory analysis of the model behaviour. To show the difference between these two things, think about a model for survival for lung-cancer patients based on some treatment parameters. We need causal model to say how the survival would change if the treatment is changed. Techniques presented in this chapter will explore how the model result will change if the treatment is changed.
--->

## Intuition {#CPIntuition}

Ceteris-paribus (CP) profiles show how the model prediction would change if the value of a single exploratory variable changed. In essence, a CP profile shows a conditional expectation of the dependent variable (response) for the particular explanatory variable. For example, panel A of Figure \@ref(fig:modelResponseCurveLine) presents response (prediction) surface for two explanatory variables, *age* and *class*, for the logistic-regression model `titanic_lmr_v6` (see Section \@ref(model-titanic-lmr)) for the Titanic dataset (see Section \@ref(TitanicDataset)). We are interested in the change of the model prediction for passenger Henry (see Section \@ref(predictions-titanic)) induced by each of the variables. Toward this end, we may want to explore the curvature of the response surface around a single point with *age* equal to 47 and *class* equal to "1st," indicated in the plot. CP profiles are one-dimensional profiles that examine the curvature across each dimension, i.e., for each variable. Panel B of Figure \@ref(fig:modelResponseCurveLine) presents CP profiles for *age* and *class*. Note that, in the CP profile for *age*, the point of interest is indicated by the dot. The plots for both variables suggest that the predicted probability of survival vary considerably for different ages and classes. 

```{r modelResponseCurveLine, echo=FALSE, fig.cap="Panel A) Model response (prediction) surface. Ceteris-paribus (CP) profiles marked with black curves help to understand the curvature of the surface while changing only a single explanatory variable. Panel B) CP profiles for individual variables, age (continuous) and class (categorical).", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/profile_age_class.png")
```

(ref:modelResponseCurveAnimationDesc) Animated model response for 2D surface as in \@ref(fig:modelResponseCurveLine).

```{r modelResponseCurveAnimation, echo=FALSE, fig.cap='(ref:modelResponseCurveAnimationDesc)', out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/profile_age_class.gif")
```



```{r, echo=FALSE, eval=FALSE}
# 3D animated map
age <- rep(0:100, each = 5)
class <- rep(c(3,2,1,4,5,6,7), each = 80)
ac <- expand.grid(age, class)
achenry <- henry[rep(1,nrow(ac)),]
achenry$age <- ac$Var1
achenry$class <- levels(titanic$class)[ac$Var2]

library("rms")
explain_lmr_v6 <- archivist::aread("pbiecek/models/2b9b6")

achenry$predict <- predict(explain_lmr_v6, achenry)

library(plotly)
acmat <- matrix(achenry$predict, ncol = length(class))
p <- plot_ly(z = ~acmat) %>% add_surface()
p


achenry$class <- reorder(achenry$class, achenry$predict, mean)

gghenry <- ggplot(achenry[,c(1,3,8)], aes(class, age)) +
 geom_raster(aes(fill = predict)) + coord_flip() +
  scale_fill_gradient2(midpoint = 0.5)

library(rayshader)
plot_gg(gghenry, width = 5, height = 5,  multicore = TRUE, scale = 250, 
        zoom = 0.7, 
        theta = 10, phi = 30, windowsize = c(800, 800))
```


<!---
CP belongs to the class of techniques that examine local curvature of the model response surface. Other very popular technique from this class called LIME is presented in Chapter \@ref(LIME).

The difference between these two methods lies in the fact that LIME approximates the model of interest locally with a simpler glass-box model. Usually, the LIME model is sparse, i.e., contains fewer explanatory variables. Thus, one needs to investigate a plot across a smaller number of dimensions. On the other hand, the CP profiles present conditional predictions for a single variable and, in most cases, are easier to interpret. More detailed comparison of these techniques is presented in the Chapter \@ref(summaryInstanceLevel).
--->

## Method {#CPMethod}

In this section, we introduce more formally one-dimensional CP profiles. 

Recall (see Section \@ref(notation)) that we use $\underline{x}_i$ to refer to the vector of values of explanatory variables corresponding to the $i$-th observation in a dataset. A vector with arbitrary values (not linked to any particular observation in the dataset) is denoted by $\underline{x}_*$. Let $x^{j}_{*}$ denote the $j$-th element of $\underline{x}_{*}$, i.e., the value of the $j$-th explanatory variable. We use $\underline{x}^{-j}_{*}$ to refer to a vector resulting from removing the $j$-th element from $\underline{x}_{*}$. By $\underline{x}^{j|=z}_{*}$, we denote a vector resulting from changing the value of the $j$-th element of $\underline{x}_{*}$ to (a scalar) $z$. 

We define a one-dimensional CP profile $h()$ for model $f()$, the $j$-th explanatory variable, and vector  $\underline{x}_*$ as follows:

\begin{equation}
h^{f,j}_{\underline{x}_*}(z) = f(\underline{x}_*^{j|=z}).
(\#eq:CPPdef)
\end{equation}

CP profile is a function that describes the dependence of the approximated expected value (prediction) of $Y$ on the value $z$ of the $j$-th explanatory variable. Note that, in practice, $z$ assumes values from the entire observed range for the variable, while values of all other explanatory variables are kept fixed at the values specified by $\underline{x}_*$. 

Note that in the situation when only a single model is considered, we will skip the model index and we will denote the CP profile for the $j$-th explanatory variable and the point of interest $\underline{x}_*$ by $h^{j}_{\underline{x}_*}(z)$.


## Example: Titanic data {#CPExample}

For continuous explanatory variables, a natural way to represent the CP function is to use a profile plot similar to one of those presented in Figure \@ref(fig:profileAgeRf). In the figure, the dot on the curves marks the instance prediction of interest, i.e., prediction $f(\underline{x}_*)$ for a single observation $\underline{x}_*$. The curve itself shows how the prediction would change if the value of a particular explanatory variable changed. 

Figure \@ref(fig:profileAgeRf) presents CP profiles for the *age* variable in the logistic-regression model `titanic_lmr_v6` and random-forest model `titanic_rf_v6` for the Titanic dataset (see Sections \@ref(model-titanic-lmr) and \@ref(model-titanic-rf), respectively). The instance of interest is passenger Henry, 
a 47-year-old man who travelled in the first class (see Section \@ref(predictions-titanic)). It is worth observing that the profile for the logistic-regression model is smooth, while the one for the random-forest model shows more variability. For this instance (observation), the prediction for the logistic-regression model would increase substantially if the value of *age* became lower than 20. For the random-forest model, a substantial increase would be obtained if *age* became lower than 13 or so.

(ref:profileAgeRfDesc) Ceteris-paribus profiles for variable `age` for the logistic-regression  (`titanic_lmr_v6`) and random-forest (`titanic_rf_v6` ) models that predict the probability of surviving based on the Titanic data. Dot indicates the value of the variable and of the prediction for passenger Henry. [TOMASZ: WHY PREDICTION .5 FOR RANDOM-FOREST?? IT WAS 0.25 IN CHAPTER 5.]

```{r profileAgeRf, echo=FALSE, fig.cap='(ref:profileAgeRfDesc)', out.width = '70%', fig.align='center'}
knitr::include_graphics("figure/profile_age_rf.png")
```

For a categorical explanatory variable, a natural way to represent the CP function is to use a barplot similar to one of those presented in Figure \@ref(fig:profileAgeRf2). Figure \@ref(fig:profileAgeRf) present CP profiles for the *class* variable in the logistic-regression and random-forest models for the Titanic dataset (see Sections \@ref(model-titanic-lmr) and \@ref(model-titanic-rf), respectively). For this instance (observation), passenger Henry, the predicted probability for the logistic-regression model would decrease substantially if the value of *class* changed to "2nd." On the other hand, for the random-forest model, the largest change would be marked if *class* changed to "restaurant staff." [TOMASZ: ANY CLUE WHY?]

(ref:profileAgeRf2Desc) Ceteris-paribus profiles for variable `class` for the logistic-regression (`titanic_lmr_v6`) and random-forest (`titanic_rf_v6` ) models that predict the probability of surviving of passenger Henry based on the Titanic data. [TOMASZ: WHY PREDICTION .3 FOR RANDOM-FOREST?? IT WAS 0.25 IN CHAPTER 5.]

```{r profileAgeRf2, echo=FALSE, fig.cap='(ref:profileAgeRf2Desc)', out.width = '70%', fig.align='center'}
knitr::include_graphics("figure/profile_class_rf.png")
```

Usually, black-box models contain a large number of explanatory variables. However, CP profiles are legible even for tiny subplots, created with techniques like sparklines or small multiples [@Tufte1986]. By using the techniques we can display a large number of profiles, while at the same time keeping profiles for consecutive variables in separate panels, as shown in Figure \@ref(fig:profileV4Rf) for the random-forest model for the Titanic dataset. It helps if the panels are ordered so that the most important profiles are listed first. A method to assess the importance of CP profiles is discussed in the next chapter. 

(ref:profileV4RfDesc) Ceteris-paribus profiles for all continuous explanatory variables for the random-forest model `titanic_rf_v6`  for the Titanic dataset and passenger Henry. Dot indicates the values of the variables and of the prediction for Henry. [TOMASZ: HERE PREDICTION .25 FOR RANDOM-FOREST, WHICH IS CORRECT.]

```{r profileV4Rf, echo=FALSE, fig.cap='(ref:profileV4RfDesc)', out.width = '70%', fig.align='center'}
knitr::include_graphics("figure/profile_v4_rf3.png")
```


## Pros and cons {#CPProsCons}

One-dimensional CP profiles, as presented in this chapter, offer a uniform, easy to communicate, and extendable approach to model exploration. Their graphical representation is easy to understand and explain. It is possible to show profiles for many variables or models in a single plot. CP profiles are easy to compare, thus we can overlay profiles for two or more models to better understand differences between the models. We can also compare two or more instances to better understand model-prediction's stability. CP profiles are also a useful tool for sensitivity analysis.

However, there are several issues related to the use of the CP profiles. If explanatory variables are correlated, then changing one variable implies a change in the other. In such case, the application of the *ceteris paribus* principle may lead to unrealistic settings and misleading results, as it is not possible to keep one variable fixed while varying the other one. For example, variables like surface and number of rooms, which can be used inprediction of an apartment's price,  are usually correlated. Thus, it is unrealistic to consider very small apartments with extremely large number of rooms. Special cases are interactions, which require the use of two-dimensional CP profiles that are more complex than one-dimensional ones. Also, in case of a model with hundreds or thousands of variables, the number of plots to inspect may be daunting. Finally, while barplots allow visualization of CP profiles for factors (categorical explanatory variables), their use becomes less trivial in case of factors with many nominal (unordered) categories (like, for example, a ZIP-code). 

## Code snippets for R {#CPR}

In this section, we present key features of the R package `DALEX` which is a part of `DrWhy.AI` universe and covers all methods presented in this chapter. Note that presented functions are, in fact, wrappers to package `ingredients` [@ingredientsRPackage].

Note that there are also other R packages that offer similar functionalities, like `condvis` [@condvisRPackage], `pdp` [@pdpRPackage], `ICEbox` [@ICEbox], `ALEPlot` [@ALEPlotRPackage], or `iml` [@imlRPackage].

For illustration, we use two classification models developed in Chapter \@ref(TitanicDataset), namely the logistic-regression model `titanic_lmr_v6` (Section \@ref(model-titanic-lmr)) and the random-forest model `titanic_rf_v6` (Section \@ref(model-titanic-rf)). They are developed to predict the probability of survival after sinking of Titanic. Instance-level explanations are calculated for Henry - a 47-year-old male passenger that travelled in the first class (see Section \@ref(predictions-titanic)).

`DALEX` explainers for both models and the `henry` data frame are retrieved via the `archivist` hooks, as listed in Section \@ref(ListOfModelsTitanic). 

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library("rms")
explain_lmr_v6 <- archivist::aread("pbiecek/models/34e19")

library("randomForest")
explain_rf_v6 <- archivist::aread("pbiecek/models/6ed54")

library("DALEX")
henry <- archivist::aread("pbiecek/models/a6538")
henry
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library("rms")
library("randomForest")
library("DALEX")
load("models/explain_lmr_v6.rda")
load("models/explain_rf_v6.rda")
load("models/henry.rda")
henry
```

### Basic use of the `individual_profile()` function

The easiest way to create and plot CP profiles is to use the `individual_profile()` function and then apply the generic `plot()` function to the resulting object. By default, profiles for all explanatory variables are calculated and plots for all numeric (continuous) variables are plotted. One can limit the number of variables for which calculations and/or plots are necessary by using the `variables` argument. 

To obtain CP profiles, the `individual_profile()` function requires arguments `explainer`, which specifies the name of the explainer-object, and `new_observation`, which specifies the name of the data frame for the instance for which prediction is of interest. As a result, the function returns an object of class `ceteris_paribus_explainer`. It is a data frame with model predictions.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
cp_titanic_rf <- individual_profile(explainer = explain_rf_v6, 
                                    new_observation = henry)
cp_titanic_rf
```

To obtain a graphical representation of CP profiles, the generic `plot()` function can be applied to the data frame returned by the `individual_profile()` function. It returns a `ggplot2` object that can be processed further if needed. In the examples below, we use the `ggplot2` functions, like `ggtitle()` or `ylim()`, to modify plot's title or the range of the y-axis.

The resulting plot can be enriched with additional data by applying functions `ingredients::show_rugs` (adds rugs for the selected points), `ingredients::show_observations` (adds dots that shows observations), or `ingredients::show_aggreagated_profiles`. All these functions can take additional arguments to modify size, color, or linetype.

Below we show R code that can be used to create plots similar to those presented in the upper part of Figure \@ref(fig:profileV4Rf). By default, the `plot()` function provides a graph with plots for all numerical variables. To limit the display to variables *age* and *fare*, the names of the variables are provided in the `variables` argument. The resulting plot is shown in Figure \@ref(fig:titanicCeterisProfile01). [TOMASZ: WHY ARE THE PLOTS SMOOTHER?] 

(ref:titanicCeterisProfile01Desc) Ceteris-paribus profiles for `age` and `fare` variables and the `titanic_rf_v6` random-forest model. Blue dots indicate the values of the variables and of the prediction for Henry.

```{r titanicCeterisProfile01, warning=FALSE, message=FALSE,  fig.width=7, fig.height=4, fig.cap='(ref:titanicCeterisProfile01Desc)', out.width = '70%', fig.align='center'}
library("ggplot2")
plot(cp_titanic_rf, variables = c("age", "fare")) +
  ggtitle("Ceteris-paribus Profile", 
            "For the random-forest model, Titanic data, and Henry")
```

To plot CP profiles for categorical variables, we have got to add the `variable_type = "categorical"` argument to the `plot()` function. In the code below, we use argument `variables` to indicate that we want to create plots for *class* and *embarked harbor* variables. The resulting plot is shown in Figure \@ref(fig:titanicCeterisProfile01). 

[TOMASZ: THE PLOT IS NOT SIMILAR TO the right-hand-side plot from Figure \@ref(fig:profileAgeRf2).]

(ref:titanicCeterisProfile01BDesc) Ceteris-paribus profiles for `class` and `embarked` variables and the `titanic_rf_v6` random-forest model. Blue dots indicate the values of the variables and of the prediction for Henry.

```{r titanicCeterisProfile01B, warning=FALSE, message=FALSE, fig.width=8.5, fig.height=4, fig.cap='(ref:titanicCeterisProfile01BDesc)', out.width = '80%', fig.align='center'}
plot(cp_titanic_rf, variables = c("class", "embarked"), 
     variable_type = "categorical") +
  ggtitle("Ceteris-paribus profile", 
            "For the random-forest model, Titanic data, and Henry")
```


### Advanced use of the `individual_profile()` function

The `individual_profile()` function is very flexible. To better understand how it can be used, we briefly review its arguments:

* `explainer`, `data`, `predict_function`, `label` - provide information about the model. If the object provided in the `explainer` argument has been created with the `DALEX::explain()` function, then other arguments are extracted from the object; this is how we use the function in this chapter. Otherwise, we have got to specify directly the model object, the data used for fitting the model, the  function that is used to compute predictions, and the model label.
* `new_observation` - a data frame dith data for instance(s), for which we want to calculate CP profiles, with same variables as in the data used to fit the model.  Note, however, that it is best when the dependent variable is not included in the data frame.
* `y` - observed value of the dependent variable for `new_observation`. The use of this argument is illustrated in Section  \@ref(cPLocDiagIntro). 
* `variables` - names of explanatory variables, for which CP profiles are to be calculated. By default, the profiles are constructed for all variables, which may be time consuming.
* `variable_splits` - a list of values for which CP profiles are to be calculated. By default, these are all values for categorical variables. For continuous variables, uniformly-placed values are selected; one can specify the number of the values with the `grid_points` argument (the default is 101).

The code below uses argument `variable_splits` to specify that CP profiles are to be calculated for *age* and *fare*, together with the list of values at which the profiles are to be evaluated. 

```{r, warning=FALSE, message=FALSE}
cp_titanic_rf <- individual_profile(explainer = explain_rf_v6, new_observation = henry,
              variable_splits = list(age = seq(0, 70, 0.1),
                                     fare = seq(0, 100, 0.1)))
```

Susbequently, to replicate the plots, presented in the upper part of Figure \@ref(fig:profileV4Rf), a call to function `plot()` can be used as below. The resulting plot is shown in Figure \@ref(fig:titanicCeterisProfile01).

(ref:titanicCeterisProfile01CDesc) Ceteris-paribus profiles for the `class` and `embarked` variables and the `titanic_rf_v6` random-forest model. Blue dots indicate the values of the variables and of the prediction for Henry.

```{r titanicCeterisProfile01C, warning=FALSE, message=FALSE, fig.width=7, fig.height=4, fig.cap='(ref:titanicCeterisProfile01CDesc)', out.width = '70%', fig.align='center'}
plot(cp_titanic_rf, variables = c("age", "fare")) + 
  ylim(0, 1) +
  ggtitle("Ceteris-paribus Profile", 
          "For the random-forest model, Titanic dataset, and Henry")
```

To enhance the plot, additional functions can be used. The generic `plot()` function creates a `ggplot2` object with a single `geom_line` layer. Function `show_observations` adds `geom_point` layer, `show_rugs` adds `geom_rugs`, while `show_profiles` adds another `geom_line`. All these functions take, as the first argument, an object created with the `ceteris_paribus()` function. They can be combined freely  to superimpose profiles for different models or observations.

In the example below, we present the code to create CP profiles for two passengers, Henry and Johny D (see Section \@ref(predictions-titanic)), for the random-forest model `titanic_rf_v6` (Section \@ref(model-titanic-rf)). Toward this end, we first retrieve the `johny_d` data frame via the `archivist` hook, as listed in Section \@ref(ListOfModelsTitanic). We then apply the `individual_profile()` function with the explainer-object `explain_rf_v6` as the `explainer` argument and the combined data frame for Henry and Johny D as the `new_observation` argument. We use argument `variable_splits` to specify that CP profiles are to be calculated for *age* and *fare*, together with the list of values at which the profiles are to be evaluated. 

<!---
{r, warning=FALSE, message=FALSE, eval=FALSE}
#johny_d <- archivist::aread("pbiecek/models/e3596")
#cp_titanic_rf2 <- variable_profile(explain_rf_v6, rbind(henry, johny_d))
#load("models/johny_d.rda")
--->

```{r, warning=FALSE, message=FALSE}
johny_d <- archivist::aread("pbiecek/models/e3596")

cp_titanic_rf2 <- individual_profile(explain_rf_v6, 
              rbind(henry, johny_d), 
              variable_splits = list(age = seq(0, 70, 0.1), fare = seq(0, 100, 0.1)))
```

To create the plots of CP profile, we apply the `plot()` function. We use the  `scale_color_manual` function to add names of passengers to the plot, and to control colors and positions.

(ref:titanicCeterisProfile01DDesc) Ceteris-paribus profiles for the `titanic_rf_v6` model. Profiles for different passengers are color-coded. Dots indicate the values of the variables and of the predictions for the passengers.

```{r titanicCeterisProfile01D, warning=FALSE, message=FALSE, fig.width=7, fig.height=4, fig.cap='(ref:titanicCeterisProfile01DDesc)', out.width = '70%', fig.align='center'}
library(ingredients)
plot(cp_titanic_rf2, color = "_ids_", variables = c("age", "fare")) + 
  scale_color_manual(name = "Passenger:", breaks = 1:2, 
            values = c("#4378bf", "#8bdcbe"), 
            labels = c("henry" , "johny_d")) + 
  ggtitle("Ceteris-paribus Profile", 
            "For the random-forest model, Titanic data, and Henry and Johny D")
```

The resulting graph, which includes CP profiles for Henry and Johny D, is presented in Figure \@ref(fig:titanicCeterisProfile01D). For Henry, the predicted probability of survival is larger than for Johny D, as seen from the location of the large dots on the profiles.

The profiles for *age* indicate a somewhat larger effect of the variable for Henry, as the predicted probability, in general, decreases from about 0.6 to 0.1 with increasing values of the variable. For Johny D, the probability changes from about 0.45 to about 0.05, with a bit less monotonic pattern. For *fare*, the effect is smaller for both passengers, as the probability changes within a smaller range of about 0.2. For Henry, the changes are approximately limited to the interval [0.1, 0.3], while for Johny D they are limited to the interval [0.4, 0.6]. 

### Comparison of models (challenger-champion analysis)

One of the most interesting uses of the explainers is the comparison of CP profiles for two or more of models.

To illustrate this possibility, first, we have go to construct profiles for the models. In our illustration, for the sake of clarity, we limit ourselves just to two models: the logistic-regression and random-forest models for the Titanic data. Moreover, we use Henry as the instance, for which predictions are of interest. We use the `individual_profile()` function to compute the CP profiles for the two models.

```{r, warning=FALSE, message=FALSE}
variable_splits = list(age = seq(0, 70, 0.1), fare = seq(0, 100, 0.1))
cp_titanic_rf <- individual_profile(explain_rf_v6, henry, variable_splits = variable_splits)
cp_titanic_lmr <- individual_profile(explain_lmr_v6, henry, variable_splits = variable_splits)
```

<!---
{r, warning=FALSE, message=FALSE, eval=FALSE}
cp_titanic_rf <- ceteris_paribus(explain_rf_v6, henry)
cp_titanic_lmr <- ceteris_paribus(explain_lmr_v6, henry)
--->

Subsequently, we construct the plot with the help of the `plot()` function. Note that, for the sake of brevity, we use the `variables` argument to limit the plot only to profiles for variables *age* and *class*. Every `plot` and `show_*` function can take a collection of explainers as arguments. Profiles for different models are included in a single plot. In the code presented below, the argument `color = "_label_"`, where `_label_` refers to the name of the column in the CP explainer that contains the model label, is used to specify that models are to be color-coded.

(ref:titanicCeterisProfile01EDesc) Comparison of the `titanic_lmr_v6` and `titanic_rf_v6` models for Henry. Profiles for different models are color-coded. Dots indicate the values of the variables and of the prediction for Henry.

```{r titanicCeterisProfile01E, warning=FALSE, message=FALSE, fig.width=7, fig.height=4, fig.cap='(ref:titanicCeterisProfile01EDesc)', out.width = '70%', fig.align='center'}
plot(cp_titanic_rf, cp_titanic_lmr, color = "_label_", 
     variables = c("age", "fare")) +
     ggtitle("Ceteris-paribus Profiles for Henry")
```

The result is shown in Figure \@ref(fig:titanicCeterisProfile01E). For Henry, the predicted probability of survival is higher for the logistic-regression model than for the random-forest model. CP profiles for *age* show a similar shape, however, and indicate decreasing probability with age. For *fare*, the profile for the logistic-regression model suggests a slightly increase of the probabilty, while for the random-forest a decreasing trend can be infered. The difference between the values of the CP profiles for *fare* increases with the increasing values of the variable. [TOMASZ: SO? ANY CONCLUSION WHERE DOES THIS MAY COME FROM?] Such analysis helps us to which degree different models agree on what-if scenarios.