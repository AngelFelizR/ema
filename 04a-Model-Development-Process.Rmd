# Model Development Process {#modelDevelopmentProcess}

## Introduction {#MDPIntro}

In this book we present tools that can be used for exploration of trained models. But before we can explore a model first we need to train one.

In this part of the book we overview the process of model development. It is needed to justify where we need additional tools for model exploration and diagnostic.

Predictive models are created for different purposes. Sometimes it is a team of data scientists that spend months on a single model that will be used for model scoring in a financial company. Every detail is important for models that operates on large scale and have for long-term effect. Another time it is an in-house model used for prediction of a demand for pizza. The model is developed by a single person in few hours. In most cases, for large and small models, similar steps are to be taken into account.

Several methologies are proposed in order to describe the process of model development. Their main goal is to standardize the process. And the standardisation is important because it helps to plan resources needed to develop and maintain the model and also to not miss any important phase.

The most known methodology for data science related projects is CRISP-DM [@crisp1999], [@crisp2019wiki] which is a tool agnostic procedue. The key component of CRISP is the break down of the whole process into six phases: business understanding, data understanding, data preparation, modeling, evaluation and deployment. CRISP is general, it was designed for any data science project. For predictive models some methodologies are introduced in [@r4ds2019] and [@misconceptions2019]. First is a very simple, focused on interations between three phases: data transformation, modeling and visualisation. 

In this book we will based on *Model Development Process* described in [@mdp2019]. It is motivated by Rational Unified Process for Software Development [@rup1998], [@usdp1999], [@spiral1988]. The process is shown in Figure \@ref(fig:mdpGeneral). Model building usually may be decomposed into four phases. First is the problem formulation followed by crisp modelling and find tuning of a model. Once the model is created it needs to be maintained and one day decommissioned. 

During each phase some tasks are to be done. Some are related to *Data preparation*. Accessing, cleaning and preparation of the data may be time consuming task. Other tasks are related to *Data understanding*. Visual model exploration and feature engineering is often needed in order to create a good model. During the *Model assembly* consecutive versions of a model are being created and compared. New models are benchmarked and validated during the *Model audit*. *Model delivery* are task needed to put the model into production.

```{r mdpGeneral, echo=FALSE, fig.cap="(fig:mdpGeneral) Overview of the Model Development Process. Horizontal axis show how time passes from the problem formulation to the model decommissioning. Vertical axis shows tasks are performed in a given phase. ", out.width = '99%', fig.align='center'}
knitr::include_graphics("figure/mdp_general.png")
```

Some versions of a model are created early and are being constantly refined. In this book we will introduce techniques that: 

* summarise how good is the current version of a model. Section \@ref(modelPerformance) overviews measures for model performance. These measures are usually used to trace the progress in model development.
* assess the feature importance. Section \@ref(featureImportance) shows how to assess influence of a single variable on model performance. Features that are not important are usually removed from a model during the model refinement. 
* shows how a single feature affects the model response. Sections \@ref(partialDependenceProfiles) -- \@ref(featureEffects) present Partial Dependency Profiles, Accumulated Local Effects and Marginal Profiles. All these techniques help to understand how model consumes particular features. 
* identifies potential problems with a model. Section \@ref(residualDiagnostic) shows techniques for exploration of model residuals. Looking closer on residuals often help to improve the model. This is possible with tools for local model exploration which are presented in the fist part of the book.
* performs sensitivity analysis for a model. Section \@ref(ceterisParibus) introduces Ceteris Paribus profiles that helps in a what-if analysis for a model.
* validated local fit for a model. Section \@ref(localDiagnostics) introduces techniques for assessment if for a single observation the model support its prediction
* decompose model predictions into pieces that can be attributed to particular variables.  Sections \@ref(breakDown) -- \@ref(LIME) show different techniques like SHAP, LIME or Break Down for local exploration of a model.


This techniques are useful in all phases of model development.





