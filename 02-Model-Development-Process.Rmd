# Model Development {#modelDevelopmentProcess}


## Notation - from introduction {#notation}

Methods described in this book were developed by different authors, who used different mathematical notations. 
We try to keep the mathematical notation consistent throughout the entre book. In some cases this may result in formulae with a fairly complex system of indices.

In this section, we provide a general overview of the notation we use. Whenever necessary, parts of the notation will be explained again in subsequent chapters.

We consider predictive models that operate on a $p$-dimensional input space $\mathcal X$. By $x \in \mathcal X$ we will refer to a single point in this input space.

In some cases models are described in context of a dataset with $n$ observations. By $x_i$ we refer to the $i$-th observation in this dataset. Of course, $x_i \in \mathcal X$.

Some explainers are constructed around an observation of interest which will be denoted by $x_{*}$. The observation may not necessarily belong to the analyzed dataset; hence, the use of the asterisk in the index. Of course, $x_* \in \mathcal X$.

Points in $\mathcal X$ are $p$ dimensional vectors. We will refer to the $j$-th coordinate by using $j$ in superscript. Thus, $x^j_i$ deontes the $j$-th coordinate of the $i$-th observation from the analyzed dataset. If $\mathcal J$ denotes a subset of indices, then $x^{\mathcal J}$ denotes the elements of vector $x$ corresponding to the indices included in $\mathcal J$. 

We will use the notation $x^{-j}$ to refer to a vector that results from removing the $j$-th coordinate from vector $x$. By $x^{j|=z}$, we denote a vector with the values at all coordinates equal to the values in $x$, except of the $j$-th coordinate, which is set equal to $z$. So, if $w=x^{j|=z}$, then $w^j = z$ and $\forall_{k\neq j} w^k = x^k$.

In this book, a model is a function $f:\mathcal X \rightarrow y$ that transforms a point from $\mathcal X$ into a real number. In most cases, the presented methods can be used directly for multi-variate dependent variables; however, we use examples with uni-variate responses to simplify the notation.

We will use $r_i = y_i - f(x_i)$ we refer to the model residual, i.e., the difference between the observed value of the dependent variable $Y$ for the $i$-th observation from a particular dataset and the model prediction for the observaton.



## Model visualization, exploration, and explanation - from introduction

In general, the lifecycle of a model can be divided, into three phases: development (or building),  deployment, and maintenance. 

Model development is the phase in which one is looking for the best available model. During this process, model exploration tools are useful. Exploration involves evaluation of the fit of the model, verification of the assumptions underlying the model (diagnostics), and assessment of the predictive performance of the model (validation). In our book we will focus on the visualization tools that can be useful in model exploration. We will not, however, discuss visualization methods for diagnostic purposes, as they are extensively discussed in many books devoted to statistical modelling. 

Model deployment is the phase in which a predictive model is adopted for use. In this phase, it is crucial that the users gain confidence in using the model. It is worth noting that the users might not have been involved in the model development. Moreover, they may only have access to the binary implementation of the model, may not provide any insight into the details of the model structure. In this situation, model explanation tools can help to understand the factors that influence model predictions and boost confidence in the model. The tools are one of the main focus points of our book.

Finally, a deployed model requires maintenance. In this phase, one monitors a model performance by, for instance, checking the validity of predictions for different datasets. If issues are detected, model explanation tools may be used to find the source of the problem and to suggest a modification of the structure of the model. 





## Introduction {#MDPIntro}

In this book we present methods that can be used for exploration of models. But before we can explore a model, first we need to train one.

In this part of the book we overview the process of model development and introduce steps that lead to a model creation. It is not a comprehensive manual ,,how to train a model in 5 steps''. The goal of this chapter is to show what needs to be performed before we can do any diagnostic or exploration of a trained model.

Predictive models are created for different purposes. Sometimes it is a team of data scientists that spend months on a single model that will be used for model scoring in a big financial company. Every detail is important for models that operate on large scale and have long-term consequences. Another time it is an in-house model trained for prediction of a demand for pizza. The model is developed by a single person in few hours. If model will not perform well it will be updated, replaced or removed.

Whatever it is a large model or small one, similar steps are to be taken during model development.

## The Process 

Several approaches are proposed in order to describe the process of model development. Their main goal is to standardize the process. And the standardisation is important because it helps to plan resources needed to develop and maintain the model and also to not miss any important phase.

The most known methodology for data science projects is CRISP-DM [@crisp1999], [@crisp2019wiki] which is a tool agnostic procedure. The key component of CRISP-DM is the break down of the whole process into six phases: business understanding, data understanding, data preparation, modeling, evaluation and deployment. CRISP-DM is general, it was designed for any data science project. For predictive models some methodologies are introduced in [@r4ds2019] and [@misconceptions2019]. First is a very simple, focused on interactions between three phases: data transformation, modeling and visualisation. 

In this book we use *Model Development Process* described in [@mdp2019]. It is motivated by Rational Unified Process for Software Development [@rup1998], [@usdp1999], [@spiral1988]. The process is shown in Figure \@ref(fig:mdpGeneral). Model building usually may be decomposed into four phases. First is the problem formulation followed by crisp modeling and find tuning of a model. Once the model is created it needs to be maintained and one day decommissioned. 

During each phase some tasks are to be done. Some are related to *Data preparation*. Accessing, cleaning and preparation of the data may be time consuming task. Other tasks are related to *Data understanding*. Visual model exploration and feature engineering is often needed in order to create a good model. During the *Model assembly* consecutive versions of a model are being created and compared. New models are benchmarked and validated during the *Model audit*. *Model delivery* are task needed to put the model into production.

```{r mdpGeneral, echo=FALSE, fig.cap="(fig:mdpGeneral) Overview of the Model Development Process. Horizontal axis show how time passes from the problem formulation to the model decommissioning. Vertical axis shows tasks are performed in a given phase. ", out.width = '99%', fig.align='center'}
knitr::include_graphics("figure/mdp_general.png")
```

## Data preparation 

In many cases the most time consuming phase of model development is the selection and aquisition of the right data. 

HERE: MORE DESCRIPTIONS AND REFERENCES ARE NEEDED. 

## Data exploration 

Before we start the modeling we need to understand the data.
Visual, tabular and statistical tools for data exploration are used depending on the character of variables.

The most know introduction to data exploration is the famous book by John Tukey [@tukey1977]. It introduced new tools for data exploration, like for example boxplots for continuous variables.

Availability of computational tools makes the process of data exploration easier and ore interactive. Find a good overview of techniques for data exploration in [@Nolan2015] or [@Wickham2017].


## Model assembly

Once the data is prepared we can start model assembly. 

One can try different algorithms for model training, validation strategies, tuning of hyperparameters. This process is usually iterative and computationally heavy.

Find a good overview of techniques for model development in [@Venables2010] or [@AppliedPredictiveModeling2013].


## Model understanding

Usually the model development starts with some crisp early versions that are refined in consecutive iterations. In order to train a final model we need to try numerous candidate models that will be explored, examined and diagnosed. In this book we will introduce techniques that: 

* summarise how good is the current version of a model. Section \@ref(modelPerformance) overviews measures for model performance. These measures are usually used to trace the progress in model development.
* assess the feature importance. Section \@ref(featureImportance) shows how to assess influence of a single variable on model performance. Features that are not important are usually removed from a model during the model refinement. 
* shows how a single feature affects the model response. Sections \@ref(partialDependenceProfiles) -- \@ref(featureEffects) present Partial Dependency Profiles, Accumulated Local Effects and Marginal Profiles. All these techniques help to understand how model consumes particular features. 
* identifies potential problems with a model. Section \@ref(residualDiagnostic) shows techniques for exploration of model residuals. Looking closer on residuals often help to improve the model. This is possible with tools for local model exploration which are presented in the fist part of the book.
* performs sensitivity analysis for a model. Section \@ref(ceterisParibus) introduces Ceteris Paribus profiles that helps in a what-if analysis for a model.
* validated local fit for a model. Section \@ref(localDiagnostics) introduces techniques for assessment if for a single observation the model support its prediction
* decompose model predictions into pieces that can be attributed to particular variables.  Sections \@ref(breakDown) -- \@ref(LIME) show different techniques like SHAP, LIME or Break Down for local exploration of a model.


