# Data Sets {#DataSetsIntro}

We illustrate techniques introduced in this book on three datasets: 

* *Sinking of the RMS Titanic* as an example of binary classification
* *Apartment prices* as an example of regression model
* *Hire or Fire* as an example of multi-class classification and interactions

## Sinking of the RMS Titanic {#TitanicDataset}

![Titanic sinking by Willy St√∂wer](figure/Titanic.jpg)

Sinking of the RMS Titanic is one of the deadliest maritime disasters in history (during peacetime). Over 1500 people died as a consequence of collision with an iceberg. Thanks to projects like *Encyclopedia titanica* `https://www.encyclopedia-titanica.org/` we have a very rich and precise data about passengers. This dataset is available in the `stablelearner` package. After some variable transformation it is also avaliable in the `DALEX` package.


```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(titanic, 2)
```

### Data cleaning

Feature of interest is the binary variable `survived`. Let's build some predictive models for this variable.

First we need to do some data preprocessing. Columns with missing data are filled up

```{r, warning=FALSE, message=FALSE}
# missing country is replaced by "X"
titanic$country[is.na(titanic$country)] = "X"
# missing age is replaced by average (30)
titanic$age[is.na(titanic$age)] = 30
# missing fare, sibsp, parch are replaced by 0
titanic$fare[is.na(titanic$fare)] = 0
titanic$sibsp[is.na(titanic$sibsp)] = 0
titanic$parch[is.na(titanic$parch)] = 0
```

### Data exploration

It is always a good idea to do data exploration before modelling. But since this book is focused on model exploration we will spend only a few lines on data exploration part. And we will limit ourselves to two-variable summaries for each variable.

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.width=6, fig.height=4}
library("ggmosaic")
ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, class), fill=survived)) +
   labs(x="Passenger class", y="Survived?", title='Survival for the titanic per class') + theme_drwhy() + theme(legend.position = "none") + coord_flip() + scale_fill_manual(values = theme_drwhy_colors(2))

ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, gender), fill=survived)) +
   labs(x="Gender", y="Survived?", title='Survival for the titanic per gender') + theme_drwhy() + theme(legend.position = "none") + coord_flip() + scale_fill_manual(values = theme_drwhy_colors(2))

ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, parch), fill=survived)) +
   labs(x="Number of Parents/Children Aboard", y="Survived?", title='Survival for the titanic per Parents/Children') + theme_drwhy() + theme(legend.position = "none") + coord_flip() + scale_fill_manual(values = theme_drwhy_colors(2))

ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, sibsp), fill=survived)) +
   labs(x="Number of Siblings/Spouses Aboard", y="Survived?", title='Survival for the titanic per Siblings/Spouses') + theme_drwhy() + theme(legend.position = "none") + coord_flip() + scale_fill_manual(values = theme_drwhy_colors(2))


ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, embarked), fill=survived)) +
   labs(x="Embarked", y="Survived?", title='Survival for the titanic per Embarked') + theme_drwhy() + theme(legend.position = "none") + coord_flip() + scale_fill_manual(values = theme_drwhy_colors(2))


ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, country), fill=survived)) +
   labs(x="Country", y="Survived?", title='Survival for the titanic per country') + theme_drwhy() + theme(legend.position = "none") + coord_flip() + scale_fill_manual(values = theme_drwhy_colors(2))

```

```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=2, echo=FALSE}
library("ggplot2")
ggplot(titanic, aes(survived, fare, color = survived)) + 
  geom_boxplot() + coord_flip() + scale_y_log10() +
  ggtitle("The more you pay for ticket, the more likely is your survival") +
  theme_drwhy()+ theme(legend.position = "none") + scale_color_manual(values = theme_drwhy_colors(2))
```

```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=4, echo=FALSE}
titanic$age_c <- cut(titanic$age,c(0,2,5,10,18,30,50,70,100))
ggplot(data = titanic) +
   geom_mosaic(aes(x = product(survived, age_c), fill=survived)) +
   labs(x="Age", y="Survived?", title='Survival for the titanic per age') + theme_drwhy() + theme(legend.position = "none") + coord_flip() + scale_fill_manual(values = theme_drwhy_colors(2))

```

### Logistic regression is always a good choice {#model_titanic_lmr}

The feature of interest `survival` is binary, thus a natural choice is a logistic regression. Most of predictive features are categorical except age. 

There is no reason to expect a linear relation between age and odds of survival, thus for age we will use linear tail-restricted cubic splines available in the `rcs()` function in the `rms` package [@rms].
 
```{r, warning=FALSE, message=FALSE}
library("rms")
titanic_lmr_v6 <- lrm(survived == "yes" ~ gender + rcs(age) + class + sibsp +
                   parch + fare + embarked, titanic)
titanic_lmr_v6
```


### Random Forest to the rescue  {#model_titanic_rf}

In addition to a logistic regression we will use a random forest model with default settings. Random forest is known for good performance, is able to grasp low-level variable interactions and is quite stable.

Here we are using the `randomForest` package [@randomForestRNews].

```{r, warning=FALSE, message=FALSE}
library("randomForest")
titanic_rf_v6 <- randomForest(survived ~ class + gender + age + sibsp + 
                           parch + fare + embarked, 
                           data = titanic)
titanic_rf_v6
```

And a smaller model

```{r, warning=FALSE, message=FALSE}
titanic_rf_v3 <- randomForest(survived ~ class + gender + age, 
                           data = titanic)
titanic_rf_v3
```

### Gradient boosting for interactions

Last model that we will train on this dataset is the gradient boosting model. This family of models is known for being able to grasp deep interactions between variables.

Here we are using the implementation from the `gbm` package [@gbm].

```{r, warning=FALSE, message=FALSE}
library("gbm")
titanic_gbm_v6 <- gbm(survived == "yes" ~ class + gender + age + sibsp +
                     parch + fare + embarked, data = titanic, n.trees = 15000)
titanic_gbm_v6
```

### Model predictions

Having all three models let's see what are odds of surviving for a 2-years old boy that travels in the 3rd class with 1 parent and 3 siblings.

```{r, warning=FALSE, message=FALSE}
henry <- data.frame(
            class = factor("2nd", levels = c("1st", "2nd", "3rd", "deck crew", "engineering crew", "restaurant staff", "victualling crew")),
            gender = factor("male", levels = c("female", "male")),
            age = 8,
            sibsp = 0,
            parch = 0,
            fare = 72,
            embarked = factor("Belfast", levels = c("Belfast","Cherbourg","Queenstown","Southampton"))
)
```

Logistic regression model says 47\% for survival.

```{r, warning=FALSE, message=FALSE}
predict(titanic_lmr_v6, henry, type = "fitted")
```

Random forest model says 39\% for survival.

```{r, warning=FALSE, message=FALSE}
predict(titanic_rf_v6, henry, type = "prob")
```

Gradient boosting model says 43.6\% for survival.

```{r, warning=FALSE, message=FALSE}
predict(titanic_gbm_v6, henry, type = "response", n.trees = 15000)
```

Three different opinions. Which one should we trust?
Tools introduced in following sections will help to understand how these models are different.

## Apartment Prices {#ApartmentDataset}

![Warsaw skyscrapers by Artur Malinowski Flicker](figure/am1974_flicker.jpg)

Predicting house prices is a common regression problem for machine learning. Various datasets for house prices are available at websites like Kaggle or UCI Machine Learning Repository. 

In this book we will work with a very interesting version of this problem. The `apartments` dataset is an artificial dataset created to match key characteristics of real apartments in Warsaw. But the dataset is created in a way that two very different models, namely linear regression and random forest, have almost exactly the same accuracy. 

Which one we should chose? Based on this dataset we show that visual explainers give a better understanding of key model characteristics and are very helpful in the model selection.

The dataset is available in the `DALEX` package [@R-DALEX]. Each row corresponds to a single apartment. Features like surface, number of rooms, district or floor are used as predictive features. The problem here is to predict price per a square meter for an apartment, so it's a regression problem with continuous target. 

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(apartments, 2)
head(apartments_test, 2)
```

### A tale of two models

Feature of interest is the variable `m2.price`, it's a price (EUR) for a square meter of an apartment. 

Let's build two predictive models for this variable.

For the champion we will use a linear model. Easy to train, easy to test, easy to understand.

```{r, warning=FALSE, message=FALSE}
apartments_lm_v5 <- lm(m2.price ~ ., data = apartments)
predicted_apartments_lm <- predict(apartments_lm_v5, apartments_test)
rmsd_lm <- sqrt(mean((predicted_apartments_lm - apartments_test$m2.price)^2))
rmsd_lm
```

The root mean square difference for linear model calculated on test data  is `r round(rmsd_lm, 1)`.

Now, let's train a challenger - a random forest model from `randomForest` package  [@R-randomForest]. Elastic, popular, able to handle non linear relations. 

```{r, warning=FALSE, message=FALSE}
library("randomForest")
set.seed(72)
apartments_rf_v5 <- randomForest(m2.price ~ ., data = apartments)
predicted_apartments_rf <- predict(apartments_rf_v5, apartments_test)
rmsd_rf <- sqrt(mean((predicted_apartments_rf - apartments_test$m2.price)^2))
rmsd_rf
```

The root mean square difference for random forest is `r round(rmsd_rf, 1)`.

The challenger is better in terms of RMSD, but the difference between accuracies is smaller than 10 cents. Shall we choose the model complex but elastic model or the linear model?


## Hire or Fire {#HFDataset}

In this chapter we present an artificial dataset from Human Resources department in a Call Center. 

The dataset is available in the `DALEX` package [@R-DALEX]. Each row corresponds to a single employee in a call center. Features like gender, age, average number of working hours per week, grade from the last evaluation and level of salary are used as predictive features.

The goal here is to first build a model, that will guess when to fire and when to promote an employer, so it's a classification problem with three classes. 

Why we need such model? We want to have objective decisions. That will not be subject to personal preferences of a manager. But is it possible to have an objective model? Would it be fair or it will just replicate some unfairness?

We will use this example to show how to use prediction level explainers to better understand how the model works for selected cases.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(HR)
```

In this book we are focused on model exploration rather than model building, thus for sake ok simplicity we will use two default models created with random forest [@R-randomForest] and generalized linear model [@R-nnet].

```{r, warning=FALSE, message=FALSE }
set.seed(59)
library("randomForest")
HR_rf_v5 <- randomForest(status ~ gender + age + hours + evaluation + salary, data = HR)

library("nnet")
HR_glm_v5 <- multinom(status ~ gender + age + hours + evaluation + salary, data = HR)
```


```{r save_models, warning=FALSE, message=FALSE, echo=FALSE}
link_to_data_models <- "models/titanic_lmr_v6.rda"

if (!file.exists(link_to_data_models)) {
  save(titanic_lmr_v6, file = "models/titanic_lmr_v6.rda")
  save(titanic_rf_v6, file = "models/titanic_rf_v6.rda")
  save(titanic_rf_v3, file = "models/titanic_rf_v3.rda")
  save(titanic_gbm_v6, file = "models/titanic_gbm_v6.rda")
  
  save(apartments_lm_v5, file = "models/apartments_lm_v5.rda")
  save(apartments_rf_v5, file = "models/apartments_rf_v5.rda")
  
  save(HR_rf_v5, file = "models/HR_rf_v5.rda")
  save(HR_glm_v5, file = "models/HR_glm_v5.rda")
}
```

## List of Models {#ListOfModels}

In previous sections we build a collection of predictive models for dataset about Sinking of RMS Titanic, apartment prices and Human Resources data. These models will be used in next chapters. 

The table below summarizes all these models. You can download the binary object with the attached archivist hooks [@archivist].

| Model name   | Model generator | Dataset  | Variables  | Link to the model |
|--------------|-----------------|----------|------------|-------|
| `titanic_lmr_v6`  | `rms:: lmr`  | `DALEX:: titanic`  | gender, age, class, sibsp, parch, fare, embarked |  `archivist:: aread("pbiecek/models/f285c")` |
| `titanic_rf_v6`  | `randomForest:: randomForest`  | `DALEX:: titanic`  | gender, age, class, sibsp, parch, fare, embarked |  `archivist:: aread("pbiecek/models/92753")` |
| `titanic_rf_v3`  | `randomForest:: randomForest`  | `DALEX:: titanic`  | gender, age, class  |  `archivist:: aread("pbiecek/models/bcd20")` |
| `titanic_gbm_v6`  | `gbm:: gbm`  | `DALEX:: titanic`  | gender, age, class, sibsp, parch, fare, embarked |  `archivist:: aread("pbiecek/models/2bdad")` |
| `apartments_lm_v5`  | `stats:: lm`  |  `DALEX:: apartments` | construction .year, surface, floor, no.rooms, district  |  `archivist:: aread("pbiecek/models/55f19")` |
|  `apartments_rf_v5` | `randomForest:: randomForest`  | `DALEX:: apartments`  | construction .year, surface, floor, no.rooms, district  | `archivist:: aread("pbiecek/models/fe7a5")`  |
|  `HR_rf_v5` | `randomForest:: randomForest`  | `DALEX:: HR`  | gender, age, hours, evaluation, salary  | `archivist:: aread("pbiecek/models/1ecfd")`  |
|  `HR_glm_v5` | `stats:: glm`  | `DALEX:: HR`  | gender, age, hours, evaluation, salary  | `archivist:: aread("pbiecek/models/f0244")`  |


