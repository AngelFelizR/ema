# Data Sets {#DataSetsIntro}

We illustrate techniques introduced in this book on three datasets: 

* *Sinking of the RMS Titanic* as an example of binary classification
* *Apartment prices* as an example of regression model
* *Hire or Fire* as an example of multi-class classification and interactions

## Sinking of the RMS Titanic {#TitanicDataset}

![Titanic sinking by Willy St√∂wer](figure/Titanic.jpg)

Sinking of the RMS Titanic is one of the deadliest maritime disasters in history (during peacetime). Over 1500 people died as a consequence of collision with an iceberg. Thanks to projects like *Encyclopedia titanica* `https://www.encyclopedia-titanica.org/` we have a very rich and precise data about passengers. This dataset is available in the `titanic` dataset.


```{r, warning=FALSE, message=FALSE}
library("titanic")
head(titanic_train, 2)
```

### Data cleaning

Feature of interest is the binary variable `Survived`. Let's build some predictive models for this variable.

First we need to do some data preprocessing. Columns with characters are converted to factors and rows with missing data are removed.

```{r, warning=FALSE, message=FALSE}
titanic_small <- titanic_train[,c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")]
titanic_small$Survived <- factor(titanic_small$Survived)
titanic_small$Sex <- factor(titanic_small$Sex)
titanic_small$Embarked <- factor(titanic_small$Embarked)
titanic_small <- na.omit(titanic_small)
head(titanic_small)
```

### Data exploration

It is always a good idea to do data exploration before modelling. But since this book is focused on model exploration we will spend only a few lines on data exploration part. And we will limit ourselves to two-variable summaries for each variable.

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.width=4, fig.height=4}
mosaicplot(table(titanic_small$Pclass, titanic_small$Survived), color = c("red4","green4"), border = "white", off = 0, main = "First class have the highest survival", ylab = "Survived", xlab = "Passenger class")

mosaicplot(table(titanic_small$Sex, titanic_small$Survived), color = c("red4","green4"), border = "white", off = 0, main = "Female are more likely to survive", ylab = "Survived", xlab = "Sex")

mosaicplot(table(titanic_small$Parch, titanic_small$Survived), color = c("red4","green4"), border = "white", off = 0, main = "It's better to have a parent on board", ylab = "Survived", xlab = "Number of Parents/Children Aboard")

mosaicplot(table(titanic_small$SibSp, titanic_small$Survived), color = c("red4","green4"), border = "white", off = 0, main = "It's better to have one sibling on board", ylab = "Survived", xlab = "Number of Siblings/Spouses Aboard")

mosaicplot(table(titanic_small$Embarked, titanic_small$Survived), color = c("red4","green4"), border = "white", off = 0, main = "", ylab = "Survived", xlab = "Embarked")
```

```{r, warning=FALSE, message=FALSE, fig.width=7, fig.height=2, echo=FALSE}
library("ggplot2")
ggplot(titanic_small, aes(Survived, Fare, color = Survived)) + 
  geom_boxplot() + coord_flip() + scale_y_log10() +
  ggtitle("The more you pay for ticket, the more likely is your survival") +
  theme_minimal() + scale_color_manual(values = c("red4", "green4"))
```

```{r, warning=FALSE, message=FALSE, fig.width=7, fig.height=5, echo=FALSE}
ggplot(titanic_small, aes(fill = Survived, x=cut(Age,c(0,2,5,10,18,30,50,70,100)))) + 
  geom_bar(position = "fill") + xlab("Age") + ylab("Fraction of survivors") +
  ggtitle("If you are below 5 your survival is more likely")+
  theme_minimal() + scale_fill_manual(values = c("red4", "green4"))
```

### Logistic regression is always a good choice {#model_titanic_lmr}

The feature of interest `survival` is binary, thus a natural choice is a logistic regression. Most of predictive features are categorical except age. 

There is no reason to expect a linear relation between age and odds of survival, thus for age we will use linear tail-restricted cubic splines available in the `rcs()` function in the `rms` package [@rms].
 
```{r, warning=FALSE, message=FALSE}
library("rms")
model_titanic_lmr <- lrm(Survived == "1" ~ Pclass + Sex + rcs(Age) + SibSp +
                   Parch + Fare + Embarked, titanic_small)
model_titanic_lmr
```


### Random Forest to the rescue  {#model_titanic_rf}

In addition to a logistic regression we will use a random forest model with default settings. Random forest is known for good performance, is able to grasp low-level variable interactions and is quite stable.

Here we are using the `randomForest` package [@randomForestRNews].

```{r, warning=FALSE, message=FALSE}
library("randomForest")
model_titanic_rf <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + 
                           Parch + Fare + Embarked, 
                           data = titanic_small)
model_titanic_rf
```

### Gradient boosting for interactions

Last model that we will train on this dataset is the gradient boosting model. This family of models is known for being able to grasp deep interactions between variables.

Here we are using the implementation from the `gbm` package [@gbm].

```{r, warning=FALSE, message=FALSE}
library("gbm")
gbm_model <- gbm(Survived == "1" ~ Pclass + Sex + Age + SibSp +
                     Parch + Fare + Embarked, data = titanic_small, n.trees = 15000)
gbm_model
```

### Model predictions

Having all three models let's see what are odds of surviving for a 2-years old boy that travels in the 3rd class with 1 parent and 3 siblings.

```{r, warning=FALSE, message=FALSE}
henry <- data.frame(
            Pclass = 1,
            Sex = factor("male", levels = c("female", "male")),
            Age = 8,
            SibSp = 0,
            Parch = 0,
            Fare = 72,
            Embarked = factor("C", levels = c("","C","Q","S"))
)
```

Logistic regression model says 88.3\% for survival.

```{r, warning=FALSE, message=FALSE}
predict(model_titanic_lmr, henry, type = "fitted")
```

Random forest model says 53.2\% for survival.

```{r, warning=FALSE, message=FALSE}
predict(model_titanic_rf, henry, type = "prob")
```

Gradient boosting model says 53.2\% for survival.

```{r, warning=FALSE, message=FALSE}
predict(gbm_model, henry, type = "response", n.trees = 15000)
```

Three different opinions. Which one should we trust?
Tools introduced in following sections will help to understand how these models are different.

## Apartment Prices {#ApartmentDataset}

![Warsaw skyscrapers by Artur Malinowski Flicker](figure/am1974_flicker.jpg)

Predicting house prices is a common regression problem for machine learning. Various datasets for house prices are available at websites like Kaggle or UCI Machine Learning Repository. 

In this book we will work with a very interesting version of this problem. The `apartments` dataset is an artificial dataset created to match key characteristics of real apartments in Warsaw. But the dataset is created in a way that two very different models, namely linear regression and random forest, have almost exactly the same accuracy. 

Which one we should chose? Based on this dataset we show that visual explainers give a better understanding of key model characteristics and are very helpful in the model selection.

The dataset is available in the `DALEX` package [@R-DALEX]. Each row corresponds to a single apartment. Features like surface, number of rooms, district or floor are used as predictive features. The problem here is to predict price per a square meter for an apartment, so it's a regression problem with continuous target. 

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(apartments, 2)
head(apartments_test, 2)
```

### A tale of two models

Feature of interest is the variable `m2.price`, it's a price (EUR) for a square meter of an apartment. 

Let's build two predictive models for this variable.

For the champion we will use a linear model. Easy to train, easy to test, easy to understand.

```{r, warning=FALSE, message=FALSE}
model_apartments_lm <- lm(m2.price ~ ., data = apartments)
predicted_apartments_lm <- predict(model_apartments_lm, apartments_test)
rmsd_lm <- sqrt(mean((predicted_apartments_lm - apartments_test$m2.price)^2))
rmsd_lm
```

The root mean square difference for linear model calculated on test data  is `r round(rmsd_lm, 1)`.

Now, let's train a challenger - a random forest model from `randomForest` package  [@R-randomForest]. Elastic, popular, able to handle non linear relations. 

```{r, warning=FALSE, message=FALSE}
library("randomForest")
set.seed(72)
model_apartments_rf <- randomForest(m2.price ~ ., data = apartments)
predicted_apartments_rf <- predict(model_apartments_rf, apartments_test)
rmsd_rf <- sqrt(mean((predicted_apartments_rf - apartments_test$m2.price)^2))
rmsd_rf
```

The root mean square difference for random forest is `r round(rmsd_rf, 1)`.

The challenger is better in terms of RMSD, but the difference between accuracies is smaller than 10 cents. Shall we choose the model complex but elastic model or the linear model?


## Hire or Fire {#HFDataset}

In this chapter we present an artificial dataset from Human Resources department in a Call Center. 

The dataset is available in the `DALEX` package [@R-DALEX]. Each row corresponds to a single employee in a call center. Features like gender, age, average number of working hours per week, grade from the last evaluation and level of salary are used as predictive features.

The goal here is to first build a model, that will guess when to fire and when to promote an employer, so it's a classification problem with three classes. 

Why we need such model? We want to have objective decisions. That will not be subject to personal preferences of a manager. But is it possible to have an objective model? Would it be fair or it will just replicate some unfairness?

We will use this example to show how to use prediction level explainers to better understand how the model works for selected cases.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(HR)
```

In this book we are focused on model exploration rather than model building, thus for sake ok simplicity we will use two default models created with random forest [@R-randomForest] and generalized linear model [@R-nnet].

```{r, warning=FALSE, message=FALSE }
set.seed(59)
library("randomForest")
model_HR_rf <- randomForest(status ~ gender + age + hours + evaluation + salary, data = HR)

library("nnet")
model_HR_glm <- multinom(status ~ gender + age + hours + evaluation + salary, data = HR)
```
