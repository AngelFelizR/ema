# Concept Drift

Machine learning models are often fitted and validated on historical data under silent assumption that data are stationary. The most popular techniques for validation (k-fold cross-validation, repeated cross-validation, and so on) test models on data with the same distribution as training data.

Yet, in many practical applications, deployed models are working in a changing environment. After some time, due to changes in the environment, model performance may degenerate, as model may be less reliable.

Concept drift refers to the change in the data distribution or in the relationships between variables over time. Think about model for energy consumption for a school, over time the school may be equipped with larger number of devices of with more power-efficient devices that may affect the model performance.

In this chapter we define basic ideas behind concept drift and propose some solutions.

## Introduction

In general, concept drift means that some statistical properties of variables used in the model change over time. This may result in degenerated performance. Thus the early dectection of concept drift is very important as it is needed to adapt quickly to these changes. 

The term `concept` usually refers to target variable, but generally, it can also refer to model input of relations between variables.

The most general formulation of a concept drift refers to chnges in joint distribution of $p(X, y)$. It is usefull to define also following measires.

* Conditional Covariate Drift as change in $p(X | y)$
* Conditional Class Drift as change in $p(y | X)$
* Covariate Drift or Concept Shift as changes in $p(X)$

Once the drift is detected one may re-fit the model on newer data or update the model.

## Covariate Drift

Covariate Drift is a change in distribution of input, change in the distribution of $p(X)$. The input is a $p$-dimensional vector with variables of possible mixed types and distributions. 

Here we propose a simple one-dimensional method, that can be applied to each variable separately despite of it's type. We do not rely on any formal statistical test, as the power of the test depends on sample size and for large samples the test will detect even small differences.

We also consider an use-case for two samples. One sample gathers historical ,,old'' data, this may be data available during the model development (part of it may be used as training and part as test data). Second sample is the current ,,new'' data, and we want to know is the distribution of $X_{old}$ differs from the distribution of $X_{new}$. 

There is a lot of distances between probability measures that can be used here (as for example Wasserstein, Total Variation and so on). We are using the Non-Intersection Distance due to its easy interpretation. 

For categorical variables $P$ and $Q$ non-intersection distance is defined as
$$
d(P,Q) = 1 - \sum_{i\in \mathcal X} \min(p_i, q_i)
$$
where $\mathcal X$ is a set of all possible values while $p_i$ and $q_i$ are probabilities for these values in distribution $P$ and $Q$ respectively. An intuition behind this distance is that it's amount of the distribution $P$ that is not shared with $Q$ (it's symmetric). The smaller the value the closes are these distributions. 

For continuous variables we discretize their distribution in the spirit of $\chi^2$ test.



## Code snippets

Here we are going to use the `drifter` package that implements some tools for concept drift detection. 

As an illustration we use two datasets from the `DALEX2` package, namely `apartments` (here we do not have drift) and `dragons` (here we do have drift).

```{r, warning=FALSE, message=FALSE}
library("DALEX2")
library("drifter")

# here we do not have any drift
head(apartments, 2)
d <- calculate_covariate_drift(apartments, apartments_test)
d
# here we do have drift
head(dragons, 2)
d <- calculate_covariate_drift(dragons, dragons_test)
d
```




Two sample 
- train two elastic moels and compare them pdp by pdp

- drift in residuals


