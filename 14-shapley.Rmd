# Average variable attributions {#shapley}

In the Section \@ref(modelAgnosticAttribution) we show the problem related to the ordering of variables. In the Section \@ref(breakDown) we show an approach in which the ordering was determined based on single step assessment of variable importance.

In this section we introduce other, very popular approach for additive variable attribution. The problem of contributions that depends on the variable ordering is solved by averaging over all possible orderings.

This method is motivated with results in cooperative game theory and was first introduced in [@Strumbelj2014]. Wide adoption of this method comes with a NIPS 2017 paper [@SHAP] and python library SHAP `https://github.com/slundberg/shap`. Authors of the SHAP method introduced also efficient algorithm for tree-based models, see [@TreeSHAP].

## Intuition

Since in sequential attribution effect depends on the ordering. Here the idea is to average across all possible orderings.

TODO:  a nice example

## Method

The name *Shapley Values* comes from the solution in cooperative game theory attributed to Lloyd Shapley. The original problem was to assess how important is each player to the overall cooperation, and what payoff can he or she reasonably expect from the coalition? [@shapleybook1952]

In the context of model interpretability the payoff is the average model response while the players are the variables in the conditioning. Then Formula for variable contributions is following.


$$
v(f, x^*, i) = \frac 1{|P|}\sum_{S \subseteq P\setminus \{i\}}  {{|P|-1}\choose{|S|}}^{-1} \left(E [f(X) | X_{S \cup \{i\}} = x^*_{S \cup \{i\}}] - E [f(X) | X_{S} = x^*_{S}]\right)
$$
where $P = \{1, \ldots, p\}$ is the set of all variables.
The intuition beyond this contribution is following. We consider all possible orderings of variables (yes, there is $2^p$ of them) and calculate the contribution of variable $i$ as an average from contributions calculated in particular orderings.

The part $E[f(X) | X_{S \cup \{i\}} = x^*_{S \cup \{i\}}] - E [f(X) | X_{S} = x^*_{S}]$ is the contribution of variable $i$ which is introduces after variables from $S$.

Time complexity of this method is $O(2^p)$ where $p$ stands for the number of variables. Such complexity makes this method impractical for most cases. Fortunately it is enough to assess this value. [@Strumbelj2014] proposed to use sampling. [@TreeSHAP] proposed fast implementations for tree based ensembles.

## Example: Hire or Fire?


## Pros and cons

Shapley Values give a uniform approach to decompose model prediction into parts that can be attributed additively to variables. Below we summarize key strengths and weaknesses of this approach. 

**Pros**

- There is a nice theory based on cooperative games.
- [@SHAP] shows that this method unifies different approaches to additive features attribution.
- There is efficient implementation available for Python.
- [@SHAP] shows more desired properties of this method, like symmetry or additivity.

**Cons**

- The exact calculation of Shapley values is time consuming.
- If the model is not additive, then the Shaply scores may be misleading. And there is no way to determine if model is far from additiveness.

Note that fully additive model solutions presented in sections \@ref(modelAgnosticAttribution), \@ref(breakDown) and \@ref(shapley) lead to same variable contributions.


## Code snippets for R

In this section we will present an example based on the `HR` dataset and Random Forest model [@R-randomForest]. See the Section \@ref(HRdataset) for more details.

```{r, message=FALSE, warning=FALSE}
library("DALEX")
library("randomForest")
model_rf <- randomForest(status ~ gender + age + hours + evaluation + salary, data = HR)
```

Here we use the `iml` package, see more examples in [@imlPackage].


```{r, message=FALSE, warning=FALSE}
library("iml")
explainer_rf = Predictor$new(model_rf, data = HR, type="prob")
```

Explanations for a new observation.

```{r, message=FALSE, warning=FALSE}
new_observation <- data.frame(gender = factor("male", levels = c("male", "female")),
                      age = 57.7,
                      hours = 42.3,
                      evaluation = 2,
                      salary = 2,
                      status = factor("fired"))

shapley = Shapley$new(explainer_rf, x.interest = new_observation)
shapley
```

And the plot with Shapley attributions.

```{r, message=FALSE, warning=FALSE}
plot(shapley)
```

See more examples for `iml` package in the [@molnar] book.
