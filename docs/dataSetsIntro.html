<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Datasets and models | Explanatory Model Analysis</title>
  <meta name="description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Datasets and models | Explanatory Model Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="github-repo" content="pbiecek/ema" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Datasets and models | Explanatory Model Analysis" />
  
  <meta name="twitter:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  

<meta name="author" content="Przemyslaw Biecek and Tomasz Burzykowski" />


<meta name="date" content="2020-07-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="doItYourselfWithPython.html"/>
<link rel="next" href="InstanceLevelExploration.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-5650686-15', 'https://pbiecek.github.io/ema/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>
<style>
.figure {
   padding:40px 0px;
}
</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><span style="font-size: large">Explanatory Model Analysis</span><br/>Explore, Explain and Examine<br/>Predictive Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#notes-to-readers"><i class="fa fa-check"></i><b>1.1</b> Notes to readers</a></li>
<li class="chapter" data-level="1.2" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#the-aim-of-the-book"><i class="fa fa-check"></i><b>1.2</b> The aim of the book</a></li>
<li class="chapter" data-level="1.3" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#three-single-laws"><i class="fa fa-check"></i><b>1.3</b> A bit of philosophy: three laws of model explanation</a></li>
<li class="chapter" data-level="1.4" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#teminology"><i class="fa fa-check"></i><b>1.4</b> Terminology</a></li>
<li class="chapter" data-level="1.5" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#glassblack"><i class="fa fa-check"></i><b>1.5</b> Black-box models and glass-box models</a></li>
<li class="chapter" data-level="1.6" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#agnosticspecific"><i class="fa fa-check"></i><b>1.6</b> Model-agnostic and model-specific approach</a></li>
<li class="chapter" data-level="1.7" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#bookstructure"><i class="fa fa-check"></i><b>1.7</b> The structure of the book</a></li>
<li class="chapter" data-level="1.8" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#whatisinthebook"><i class="fa fa-check"></i><b>1.8</b> What is included in this book and what is not</a></li>
<li class="chapter" data-level="1.9" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#thanksto"><i class="fa fa-check"></i><b>1.9</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html"><i class="fa fa-check"></i><b>2</b> Model Development</a>
<ul>
<li class="chapter" data-level="2.1" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPIntro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPprocess"><i class="fa fa-check"></i><b>2.2</b> Model-development process</a></li>
<li class="chapter" data-level="2.3" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#notation"><i class="fa fa-check"></i><b>2.3</b> Notation</a></li>
<li class="chapter" data-level="2.4" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#dataunderstanding"><i class="fa fa-check"></i><b>2.4</b> Data understanding</a></li>
<li class="chapter" data-level="2.5" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#fitting"><i class="fa fa-check"></i><b>2.5</b> Model assembly (fitting)</a></li>
<li class="chapter" data-level="2.6" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#validation"><i class="fa fa-check"></i><b>2.6</b> Model audit</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html"><i class="fa fa-check"></i><b>3</b> Do-it-yourself with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#what-to-install"><i class="fa fa-check"></i><b>3.1</b> What to install?</a></li>
<li class="chapter" data-level="3.2" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#infoDALEX"><i class="fa fa-check"></i><b>3.2</b> How to work with <code>DALEX</code>?</a></li>
<li class="chapter" data-level="3.3" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#how-to-work-with-archivist"><i class="fa fa-check"></i><b>3.3</b> How to work with <code>archivist</code>?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html"><i class="fa fa-check"></i><b>4</b> Do-it-yourself with Python</a>
<ul>
<li class="chapter" data-level="4.1" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#what-to-install"><i class="fa fa-check"></i><b>4.1</b> What to install?</a></li>
<li class="chapter" data-level="4.2" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html#infoDALEXpy"><i class="fa fa-check"></i><b>4.2</b> How to work with <code>dalex</code>?</a></li>
<li class="chapter" data-level="4.3" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html#code-snippets-for-python"><i class="fa fa-check"></i><b>4.3</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html"><i class="fa fa-check"></i><b>5</b> Datasets and models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#TitanicDataset"><i class="fa fa-check"></i><b>5.1</b> Sinking of the RMS Titanic</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-titanic"><i class="fa fa-check"></i><b>5.1.1</b> Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#r-classification-models-for-titanic"><i class="fa fa-check"></i><b>5.2</b> R classification models for Titanic</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-lmr"><i class="fa fa-check"></i><b>5.2.1</b> Logistic-regression model</a></li>
<li class="chapter" data-level="5.2.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-rf"><i class="fa fa-check"></i><b>5.2.2</b> Random-forest model</a></li>
<li class="chapter" data-level="5.2.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-gbm"><i class="fa fa-check"></i><b>5.2.3</b> Gradient-boosting model</a></li>
<li class="chapter" data-level="5.2.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-svm"><i class="fa fa-check"></i><b>5.2.4</b> Support Vector Machine model for Classification</a></li>
<li class="chapter" data-level="5.2.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic"><i class="fa fa-check"></i><b>5.2.5</b> Models’ predictions</a></li>
<li class="chapter" data-level="5.2.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicRCode"><i class="fa fa-check"></i><b>5.2.6</b> Models’ explainers</a></li>
<li class="chapter" data-level="5.2.7" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsTitanic"><i class="fa fa-check"></i><b>5.2.7</b> List of objects for the Titanic example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#python-classification-models-for-titanic"><i class="fa fa-check"></i><b>5.3</b> Python classification models for Titanic</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-lr"><i class="fa fa-check"></i><b>5.3.1</b> Logistic-regression model</a></li>
<li class="chapter" data-level="5.3.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-rf"><i class="fa fa-check"></i><b>5.3.2</b> Random-forest model</a></li>
<li class="chapter" data-level="5.3.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-gbm"><i class="fa fa-check"></i><b>5.3.3</b> Gradient-boosting model</a></li>
<li class="chapter" data-level="5.3.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-svm"><i class="fa fa-check"></i><b>5.3.4</b> Support Vector Machine model for Classification</a></li>
<li class="chapter" data-level="5.3.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic-python"><i class="fa fa-check"></i><b>5.3.5</b> Models’ predictions</a></li>
<li class="chapter" data-level="5.3.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicPythonCode"><i class="fa fa-check"></i><b>5.3.6</b> Models’ explainers</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ApartmentDataset"><i class="fa fa-check"></i><b>5.4</b> Apartment prices</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-apartments"><i class="fa fa-check"></i><b>5.4.1</b> Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#r-regression-model-for-apartment-prices"><i class="fa fa-check"></i><b>5.5</b> R regression model for Apartment prices</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-lr"><i class="fa fa-check"></i><b>5.5.1</b> Linear-regression model</a></li>
<li class="chapter" data-level="5.5.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-rf"><i class="fa fa-check"></i><b>5.5.2</b> Random-forest model</a></li>
<li class="chapter" data-level="5.5.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-svm"><i class="fa fa-check"></i><b>5.5.3</b> Support Vector Machine model for Regression</a></li>
<li class="chapter" data-level="5.5.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictionsApartments"><i class="fa fa-check"></i><b>5.5.4</b> Models’ predictions</a></li>
<li class="chapter" data-level="5.5.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsRCode"><i class="fa fa-check"></i><b>5.5.5</b> Models’ explainers</a></li>
<li class="chapter" data-level="5.5.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsApartments"><i class="fa fa-check"></i><b>5.5.6</b> List of objects for the Apartment prices example</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#python-regression-models-for-apartment-prices"><i class="fa fa-check"></i><b>5.6</b> Python regression models for Apartment prices</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-lr"><i class="fa fa-check"></i><b>5.6.1</b> Linear-regression model</a></li>
<li class="chapter" data-level="5.6.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-rf"><i class="fa fa-check"></i><b>5.6.2</b> Random-forest model</a></li>
<li class="chapter" data-level="5.6.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-svm"><i class="fa fa-check"></i><b>5.6.3</b> Support Vector Machine model for Regression</a></li>
<li class="chapter" data-level="5.6.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-apartments-python"><i class="fa fa-check"></i><b>5.6.4</b> Models’ predictions</a></li>
<li class="chapter" data-level="5.6.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsPythonCode"><i class="fa fa-check"></i><b>5.6.5</b> Models’ explainers</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Instance Level</b></span></li>
<li class="chapter" data-level="6" data-path="InstanceLevelExploration.html"><a href="InstanceLevelExploration.html"><i class="fa fa-check"></i><b>6</b> Introduction to Instance-level Exploration</a></li>
<li class="chapter" data-level="7" data-path="breakDown.html"><a href="breakDown.html"><i class="fa fa-check"></i><b>7</b> Break-down Plots for Additive Attributions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="breakDown.html"><a href="breakDown.html#BDIntroduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="breakDown.html"><a href="breakDown.html#BDIntuition"><i class="fa fa-check"></i><b>7.2</b> Intuition</a></li>
<li class="chapter" data-level="7.3" data-path="breakDown.html"><a href="breakDown.html#BDMethod"><i class="fa fa-check"></i><b>7.3</b> Method</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="breakDown.html"><a href="breakDown.html#BDMethodLin"><i class="fa fa-check"></i><b>7.3.1</b> Break-down for linear models</a></li>
<li class="chapter" data-level="7.3.2" data-path="breakDown.html"><a href="breakDown.html#BDMethodGen"><i class="fa fa-check"></i><b>7.3.2</b> Break-down for a general case</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="breakDown.html"><a href="breakDown.html#BDExample"><i class="fa fa-check"></i><b>7.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="7.5" data-path="breakDown.html"><a href="breakDown.html#BDProsCons"><i class="fa fa-check"></i><b>7.5</b> Pros and cons</a></li>
<li class="chapter" data-level="7.6" data-path="breakDown.html"><a href="breakDown.html#BDR"><i class="fa fa-check"></i><b>7.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="breakDown.html"><a href="breakDown.html#basic-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>7.6.1</b> Basic use of the <code>predict_parts()</code> function</a></li>
<li class="chapter" data-level="7.6.2" data-path="breakDown.html"><a href="breakDown.html#advanced-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>7.6.2</b> Advanced use of the <code>predict_parts()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="breakDown.html"><a href="breakDown.html#BDPython"><i class="fa fa-check"></i><b>7.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="iBreakDown.html"><a href="iBreakDown.html"><i class="fa fa-check"></i><b>8</b> Break-down Plots for Interactions (iBreak-down Plots)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDIntuition"><i class="fa fa-check"></i><b>8.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDMethod"><i class="fa fa-check"></i><b>8.2</b> Method</a></li>
<li class="chapter" data-level="8.3" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDExample"><i class="fa fa-check"></i><b>8.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="8.4" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDProsCons"><i class="fa fa-check"></i><b>8.4</b> Pros and cons</a></li>
<li class="chapter" data-level="8.5" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDRcode"><i class="fa fa-check"></i><b>8.5</b> Code snippets for R</a></li>
<li class="chapter" data-level="8.6" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDPythonCode"><i class="fa fa-check"></i><b>8.6</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>9</b> Shapley Additive Explanations (SHAP) and Average Variable Attributions</a>
<ul>
<li class="chapter" data-level="9.1" data-path="shapley.html"><a href="shapley.html#SHAPIntuition"><i class="fa fa-check"></i><b>9.1</b> Intuition</a></li>
<li class="chapter" data-level="9.2" data-path="shapley.html"><a href="shapley.html#SHAPMethod"><i class="fa fa-check"></i><b>9.2</b> Method</a></li>
<li class="chapter" data-level="9.3" data-path="shapley.html"><a href="shapley.html#SHAPExample"><i class="fa fa-check"></i><b>9.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="9.4" data-path="shapley.html"><a href="shapley.html#SHAProsCons"><i class="fa fa-check"></i><b>9.4</b> Pros and cons</a></li>
<li class="chapter" data-level="9.5" data-path="shapley.html"><a href="shapley.html#SHAPRcode"><i class="fa fa-check"></i><b>9.5</b> Code snippets for R</a></li>
<li class="chapter" data-level="9.6" data-path="shapley.html"><a href="shapley.html#SHAPPythonCode"><i class="fa fa-check"></i><b>9.6</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="LIME.html"><a href="LIME.html"><i class="fa fa-check"></i><b>10</b> Local Interpretable Model-agnostic Explanations (LIME)</a>
<ul>
<li class="chapter" data-level="10.1" data-path="LIME.html"><a href="LIME.html#LIMEIntroduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="LIME.html"><a href="LIME.html#LIMEIntuition"><i class="fa fa-check"></i><b>10.2</b> Intuition</a></li>
<li class="chapter" data-level="10.3" data-path="LIME.html"><a href="LIME.html#LIMEMethod"><i class="fa fa-check"></i><b>10.3</b> Method</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="LIME.html"><a href="LIME.html#LIMErepr"><i class="fa fa-check"></i><b>10.3.1</b> Interpretable data representation</a></li>
<li class="chapter" data-level="10.3.2" data-path="LIME.html"><a href="LIME.html#LIMEsample"><i class="fa fa-check"></i><b>10.3.2</b> Sampling around the instance of interest</a></li>
<li class="chapter" data-level="10.3.3" data-path="LIME.html"><a href="LIME.html#LIMEglas"><i class="fa fa-check"></i><b>10.3.3</b> Fitting the glass-box model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="LIME.html"><a href="LIME.html#LIMEExample"><i class="fa fa-check"></i><b>10.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="10.5" data-path="LIME.html"><a href="LIME.html#LIMEProsCons"><i class="fa fa-check"></i><b>10.5</b> Pros and cons</a></li>
<li class="chapter" data-level="10.6" data-path="LIME.html"><a href="LIME.html#LIMERcode"><i class="fa fa-check"></i><b>10.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="LIME.html"><a href="LIME.html#the-lime-package"><i class="fa fa-check"></i><b>10.6.1</b> The <code>lime</code> package</a></li>
<li class="chapter" data-level="10.6.2" data-path="LIME.html"><a href="LIME.html#the-localmodel-package"><i class="fa fa-check"></i><b>10.6.2</b> The <code>localModel</code> package</a></li>
<li class="chapter" data-level="10.6.3" data-path="LIME.html"><a href="LIME.html#the-iml-package"><i class="fa fa-check"></i><b>10.6.3</b> The <code>iml</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ceterisParibus.html"><a href="ceterisParibus.html"><i class="fa fa-check"></i><b>11</b> Ceteris-paribus Profiles</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntro"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntuition"><i class="fa fa-check"></i><b>11.2</b> Intuition</a></li>
<li class="chapter" data-level="11.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPMethod"><i class="fa fa-check"></i><b>11.3</b> Method</a></li>
<li class="chapter" data-level="11.4" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPExample"><i class="fa fa-check"></i><b>11.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="11.5" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPProsCons"><i class="fa fa-check"></i><b>11.5</b> Pros and cons</a></li>
<li class="chapter" data-level="11.6" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPR"><i class="fa fa-check"></i><b>11.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#basic-use-of-the-predict_profile-function"><i class="fa fa-check"></i><b>11.6.1</b> Basic use of the <code>predict_profile()</code> function</a></li>
<li class="chapter" data-level="11.6.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#advanced-use-of-the-predict_profile-function"><i class="fa fa-check"></i><b>11.6.2</b> Advanced use of the <code>predict_profile()</code> function</a></li>
<li class="chapter" data-level="11.6.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#comparison-of-models-challenger-champion-analysis"><i class="fa fa-check"></i><b>11.6.3</b> Comparison of models (challenger-champion analysis)</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPPython"><i class="fa fa-check"></i><b>11.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html"><i class="fa fa-check"></i><b>12</b> Ceteris-paribus Oscillations</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntuition"><i class="fa fa-check"></i><b>12.2</b> Intuition</a></li>
<li class="chapter" data-level="12.3" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscMethod"><i class="fa fa-check"></i><b>12.3</b> Method</a></li>
<li class="chapter" data-level="12.4" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscExample"><i class="fa fa-check"></i><b>12.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="12.5" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscProsCons"><i class="fa fa-check"></i><b>12.5</b> Pros and cons</a></li>
<li class="chapter" data-level="12.6" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscR"><i class="fa fa-check"></i><b>12.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="breakDown.html"><a href="breakDown.html#basic-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>12.6.1</b> Basic use of the <code>predict_parts()</code> function</a></li>
<li class="chapter" data-level="12.6.2" data-path="breakDown.html"><a href="breakDown.html#advanced-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>12.6.2</b> Advanced use of the <code>predict_parts()</code> function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="localDiagnostics.html"><a href="localDiagnostics.html"><i class="fa fa-check"></i><b>13</b> Local-diagnostics Plots</a>
<ul>
<li class="chapter" data-level="13.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntro"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntuition"><i class="fa fa-check"></i><b>13.2</b> Intuition</a></li>
<li class="chapter" data-level="13.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagMethod"><i class="fa fa-check"></i><b>13.3</b> Method</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagNeighbors"><i class="fa fa-check"></i><b>13.3.1</b> Nearest neighbors</a></li>
<li class="chapter" data-level="13.3.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagLFplot"><i class="fa fa-check"></i><b>13.3.2</b> Local-fidelity plot</a></li>
<li class="chapter" data-level="13.3.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProfiles"><i class="fa fa-check"></i><b>13.3.3</b> Local-stability plot</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagExample"><i class="fa fa-check"></i><b>13.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="13.5" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProsCons"><i class="fa fa-check"></i><b>13.5</b> Pros and cons</a></li>
<li class="chapter" data-level="13.6" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagR"><i class="fa fa-check"></i><b>13.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html"><i class="fa fa-check"></i><b>14</b> Summary of Instance-level Exploration</a>
<ul>
<li class="chapter" data-level="14.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#summaryInstanceLevelIntro"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#number-of-explanatory-variables-in-the-model"><i class="fa fa-check"></i><b>14.2</b> Number of explanatory variables in the model</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#low-to-medium-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.2.1</b> Low to medium number of explanatory variables</a></li>
<li class="chapter" data-level="14.2.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#medium-to-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.2.2</b> Medium to large number of explanatory variables</a></li>
<li class="chapter" data-level="14.2.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#very-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.2.3</b> Very large number of explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#correlated-explanatory-variables"><i class="fa fa-check"></i><b>14.3</b> Correlated explanatory variables</a></li>
<li class="chapter" data-level="14.4" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#models-with-interactions"><i class="fa fa-check"></i><b>14.4</b> Models with interactions</a></li>
<li class="chapter" data-level="14.5" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#sparse-explanations"><i class="fa fa-check"></i><b>14.5</b> Sparse explanations</a></li>
<li class="chapter" data-level="14.6" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#additional-uses-of-model-exploration-and-explanation"><i class="fa fa-check"></i><b>14.6</b> Additional uses of model exploration and explanation</a></li>
<li class="chapter" data-level="14.7" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#comparison-of-models-champion-challenger-analysis"><i class="fa fa-check"></i><b>14.7</b> Comparison of models (champion-challenger analysis)</a></li>
</ul></li>
<li class="part"><span><b>III Dataset Level</b></span></li>
<li class="chapter" data-level="15" data-path="modelLevelExploration.html"><a href="modelLevelExploration.html"><i class="fa fa-check"></i><b>15</b> Introduction to Dataset-level Exploration</a></li>
<li class="chapter" data-level="16" data-path="modelPerformance.html"><a href="modelPerformance.html"><i class="fa fa-check"></i><b>16</b> Model-performance Measures</a>
<ul>
<li class="chapter" data-level="16.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntro"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntuition"><i class="fa fa-check"></i><b>16.2</b> Intuition</a></li>
<li class="chapter" data-level="16.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethod"><i class="fa fa-check"></i><b>16.3</b> Method</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCont"><i class="fa fa-check"></i><b>16.3.1</b> Continuous dependent variable</a></li>
<li class="chapter" data-level="16.3.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodBin"><i class="fa fa-check"></i><b>16.3.2</b> Binary dependent variable</a></li>
<li class="chapter" data-level="16.3.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCateg"><i class="fa fa-check"></i><b>16.3.3</b> Categorical dependent variable</a></li>
<li class="chapter" data-level="16.3.4" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCount"><i class="fa fa-check"></i><b>16.3.4</b> Count dependent variable</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="modelPerformance.html"><a href="modelPerformance.html#example"><i class="fa fa-check"></i><b>16.4</b> Example</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceApartments"><i class="fa fa-check"></i><b>16.4.1</b> Apartment prices</a></li>
<li class="chapter" data-level="16.4.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceTitanic"><i class="fa fa-check"></i><b>16.4.2</b> Titanic data</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceProsCons"><i class="fa fa-check"></i><b>16.5</b> Pros and cons</a></li>
<li class="chapter" data-level="16.6" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceR"><i class="fa fa-check"></i><b>16.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="16.7" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformancePython"><i class="fa fa-check"></i><b>16.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="featureImportance.html"><a href="featureImportance.html"><i class="fa fa-check"></i><b>17</b> Variable-importance Measures</a>
<ul>
<li class="chapter" data-level="17.1" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntro"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntuition"><i class="fa fa-check"></i><b>17.2</b> Intuition</a></li>
<li class="chapter" data-level="17.3" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceMethod"><i class="fa fa-check"></i><b>17.3</b> Method</a></li>
<li class="chapter" data-level="17.4" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceTitanic"><i class="fa fa-check"></i><b>17.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="17.5" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceProsCons"><i class="fa fa-check"></i><b>17.5</b> Pros and cons</a></li>
<li class="chapter" data-level="17.6" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceR"><i class="fa fa-check"></i><b>17.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="17.7" data-path="featureImportance.html"><a href="featureImportance.html#featureImportancePython"><i class="fa fa-check"></i><b>17.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html"><i class="fa fa-check"></i><b>18</b> Partial-dependence Profiles</a>
<ul>
<li class="chapter" data-level="18.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntro"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntuition"><i class="fa fa-check"></i><b>18.2</b> Intuition</a></li>
<li class="chapter" data-level="18.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPMethod"><i class="fa fa-check"></i><b>18.3</b> Method</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPs"><i class="fa fa-check"></i><b>18.3.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="18.3.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clusteredPDPs"><i class="fa fa-check"></i><b>18.3.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="18.3.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#groupedPDPs"><i class="fa fa-check"></i><b>18.3.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="18.3.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastivePDPs"><i class="fa fa-check"></i><b>18.3.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPExample"><i class="fa fa-check"></i><b>18.4</b> Example: apartment-prices data</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="18.4.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="18.4.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="18.4.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPProsCons"><i class="fa fa-check"></i><b>18.5</b> Pros and cons</a></li>
<li class="chapter" data-level="18.6" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPR"><i class="fa fa-check"></i><b>18.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="18.6.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="18.6.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="18.6.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="featureImportance.html"><a href="featureImportance.html#featureImportancePython"><i class="fa fa-check"></i><b>18.7</b> Code snippets for Python</a>
<ul>
<li class="chapter" data-level="18.7.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles-2"><i class="fa fa-check"></i><b>18.7.1</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="18.7.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles-2"><i class="fa fa-check"></i><b>18.7.2</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html"><i class="fa fa-check"></i><b>19</b> Local-dependence and Accumulated Local Profiles</a>
<ul>
<li class="chapter" data-level="19.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntro"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntuition"><i class="fa fa-check"></i><b>19.2</b> Intuition</a></li>
<li class="chapter" data-level="19.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPMethod"><i class="fa fa-check"></i><b>19.3</b> Method</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#local-dependence-profile"><i class="fa fa-check"></i><b>19.3.1</b> Local-dependence profile</a></li>
<li class="chapter" data-level="19.3.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#accumulated-local-profile"><i class="fa fa-check"></i><b>19.3.2</b> Accumulated local profile</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#CDPExample"><i class="fa fa-check"></i><b>19.4</b> Example: apartment-prices data</a></li>
<li class="chapter" data-level="19.5" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPProsCons"><i class="fa fa-check"></i><b>19.5</b> Pros and cons</a></li>
<li class="chapter" data-level="19.6" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPR"><i class="fa fa-check"></i><b>19.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html"><i class="fa fa-check"></i><b>20</b> Residual Diagnostics</a>
<ul>
<li class="chapter" data-level="20.1" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntroResidualDiagnostic"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
<li class="chapter" data-level="20.2" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntuitionResidualDiagnostic"><i class="fa fa-check"></i><b>20.2</b> Intuition</a></li>
<li class="chapter" data-level="20.3" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#MethodResidualDiagnostic"><i class="fa fa-check"></i><b>20.3</b> Method</a></li>
<li class="chapter" data-level="20.4" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ExampleResidualDiagnostic"><i class="fa fa-check"></i><b>20.4</b> Example: apartment-prices data</a></li>
<li class="chapter" data-level="20.5" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ProsConsResidualDiagnostic"><i class="fa fa-check"></i><b>20.5</b> Pros and cons</a></li>
<li class="chapter" data-level="20.6" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#RcodeResidualDiagnostic"><i class="fa fa-check"></i><b>20.6</b> Code snippets for R</a></li>
</ul></li>
<li class="part"><span><b>IV Use-cases</b></span></li>
<li class="chapter" data-level="21" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html"><i class="fa fa-check"></i><b>21</b> FIFA 19</a>
<ul>
<li class="chapter" data-level="21.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAintro"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAdataprep"><i class="fa fa-check"></i><b>21.2</b> Data preparation</a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r"><i class="fa fa-check"></i><b>21.2.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.2.2" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html#code-snippets-for-python"><i class="fa fa-check"></i><b>21.2.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAdataunderst"><i class="fa fa-check"></i><b>21.3</b> Data understanding</a></li>
<li class="chapter" data-level="21.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelassembly"><i class="fa fa-check"></i><b>21.4</b> Model assembly</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-1"><i class="fa fa-check"></i><b>21.4.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.4.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-1"><i class="fa fa-check"></i><b>21.4.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelaudit"><i class="fa fa-check"></i><b>21.5</b> Model audit</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-2"><i class="fa fa-check"></i><b>21.5.1</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelunderst"><i class="fa fa-check"></i><b>21.6</b> Model understanding (dataset-level explanation)</a>
<ul>
<li class="chapter" data-level="21.6.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-3"><i class="fa fa-check"></i><b>21.6.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.6.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-2"><i class="fa fa-check"></i><b>21.6.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.7" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAinstanceunderst"><i class="fa fa-check"></i><b>21.7</b> Instance-level explanation</a>
<ul>
<li class="chapter" data-level="21.7.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFALewy"><i class="fa fa-check"></i><b>21.7.1</b> Robert Lewandowski</a></li>
<li class="chapter" data-level="21.7.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-4"><i class="fa fa-check"></i><b>21.7.2</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.7.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-3"><i class="fa fa-check"></i><b>21.7.3</b> Code snippets for Python</a></li>
<li class="chapter" data-level="21.7.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFACR7"><i class="fa fa-check"></i><b>21.7.4</b> CR7</a></li>
<li class="chapter" data-level="21.7.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFASzczesny"><i class="fa fa-check"></i><b>21.7.5</b> Wojciech Szczęsny</a></li>
<li class="chapter" data-level="21.7.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAMessi"><i class="fa fa-check"></i><b>21.7.6</b> Lionel Messi</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="session-info.html"><a href="session-info.html"><i class="fa fa-check"></i>Session Info</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/ModelOriented/DALEX" target="blank">DALEX website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Explanatory Model Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dataSetsIntro" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Datasets and models</h1>
<p>We will illustrate the methods presented in this book by using three datasets related to:</p>
<ul>
<li>predicting probability of survival for passengers of the <em>RMS Titanic</em>;</li>
<li>predicting prices of <em>apartments in Warsaw</em>;</li>
<li>estimating the value of the football players based on <em>FIFA</em> dataset.</li>
</ul>
<p>The first dataset will be used to illustrate the application of the techniques in the case of classification predictive models for a binary dependent variable. This dataset is mainly used in the examples in the part II of this book.
The second one will offer an example for regression models for a continuous variable. This dataset is mainly used in the examples in the part III of this book.
The third dataset will be introduced in Chapter <a href="UseCaseFIFA.html#UseCaseFIFA">21</a> to summarise all techniques introduced in the book.</p>
<p>In this chapter, we provide a short description of first two datasets, together with results of exploratory analyses. We also introduce models that will be used for illustration purposes in subsequent chapters.</p>
<div id="TitanicDataset" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Sinking of the RMS Titanic</h2>
<div class="figure">
<img src="figure/Titanic.jpg" alt="" />
<p class="caption">Titanic sinking by Willy Stöwer</p>
</div>
<p>Sinking of the RMS Titanic is one of the deadliest maritime disasters in history (during peacetime). Over 1500 people died as a consequence of collision with an iceberg. Projects like <em>Encyclopedia titanica</em> (<a href="https://www.encyclopedia-titanica.org/" class="uri">https://www.encyclopedia-titanica.org/</a>) are a source of rich and precise data about Titanic’s passengers.
The <code>stablelearner</code> package in R includes a data frame with information about passengers’ characteristics. The dataset, after some data cleaning and variable transformations, is also available in the <code>DALEX</code> package for R and <code>dalex</code> library for Python. In particular, the <code>titanic</code> data frame contains 2207 observations (for 1317 passengers and 890 crew members) and nine variables:</p>
<ul>
<li><em>gender</em>, person’s (passenger’s or crew member’s) gender, a factor (categorical variable) with two levels (categories): “male” (78%) and “female” (22%);</li>
<li><em>age</em>, person’s age in years, a numerical variable; the age is given in (integer) years, in the range of 0–74 years;</li>
<li><em>class</em>, the class in which the passenger travelled, or the duty class of a crew member; a factor with seven levels: “1st” (14.7%), “2nd” (12.9%), “3rd” (32.1%), “deck crew” (3%), “engineering crew” (14.7%), “restaurant staff” (3.1%), and “victualling crew” (19.5%);</li>
<li><em>embarked</em>, the harbor in which the person embarked on the ship, a factor with four levels: “Belfast” (8.9%), “Cherbourg” (12.3%), “Queenstown” (5.6%), and “Southampton” (73.2%);</li>
<li><em>country</em>, person’s home country, a factor with 48 levels; the most common levels are “England” (51%), “United States” (12%), “Ireland” (6.2%), and “Sweden” (4.8%);</li>
<li><em>fare</em>, the price of the ticket (only available for passengers; 0 for crew members), a numerical variable in the range of 0–512;</li>
<li><em>sibsp</em>, the number of siblings/spouses aboard the ship, a numerical variable in the range of 0–8;</li>
<li><em>parch</em>, the number of parents/children aboard the ship, a numerical variable in the range of 0–9;</li>
<li><em>survived</em>, a factor with two levels: “yes” (67.8%) and “no” (32.2%) indicating whether the person survived or not.</li>
</ul>
<p>See an first six rows of this dataset in the Table below.</p>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
gender
</th>
<th style="text-align:right;">
age
</th>
<th style="text-align:left;">
class
</th>
<th style="text-align:left;">
embarked
</th>
<th style="text-align:left;">
country
</th>
<th style="text-align:right;">
fare
</th>
<th style="text-align:right;">
sibsp
</th>
<th style="text-align:right;">
parch
</th>
<th style="text-align:left;">
survived
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
male
</td>
<td style="text-align:right;">
42
</td>
<td style="text-align:left;">
3rd
</td>
<td style="text-align:left;">
Southampton
</td>
<td style="text-align:left;">
United States
</td>
<td style="text-align:right;">
7.11
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
no
</td>
</tr>
<tr>
<td style="text-align:left;">
male
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:left;">
3rd
</td>
<td style="text-align:left;">
Southampton
</td>
<td style="text-align:left;">
United States
</td>
<td style="text-align:right;">
20.05
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
no
</td>
</tr>
<tr>
<td style="text-align:left;">
male
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:left;">
3rd
</td>
<td style="text-align:left;">
Southampton
</td>
<td style="text-align:left;">
United States
</td>
<td style="text-align:right;">
20.05
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
no
</td>
</tr>
<tr>
<td style="text-align:left;">
female
</td>
<td style="text-align:right;">
39
</td>
<td style="text-align:left;">
3rd
</td>
<td style="text-align:left;">
Southampton
</td>
<td style="text-align:left;">
England
</td>
<td style="text-align:right;">
20.05
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
yes
</td>
</tr>
<tr>
<td style="text-align:left;">
female
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:left;">
3rd
</td>
<td style="text-align:left;">
Southampton
</td>
<td style="text-align:left;">
Norway
</td>
<td style="text-align:right;">
7.13
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
yes
</td>
</tr>
<tr>
<td style="text-align:left;">
male
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:left;">
3rd
</td>
<td style="text-align:left;">
Southampton
</td>
<td style="text-align:left;">
United States
</td>
<td style="text-align:right;">
7.13
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
yes
</td>
</tr>
</tbody>
</table>
<p>Models considered for this dataset will use <em>survived</em> as the (binary) dependent variable.</p>
<div id="exploration-titanic" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Data exploration</h3>
<p>As discussed in Chapter <a href="modelDevelopmentProcess.html#modelDevelopmentProcess">2</a>, it is always advisable to explore data before modelling. However, as this book is focused on model exploration, we will limit the data exploration part.</p>
<p>Before exploring the data, we first conduct some pre-processing. In particular, the value of variables <em>age</em>, <em>country</em>, <em>sibsp</em>, <em>parch</em>, and <em>fare</em> is missing for a limited number of observations (2, 81, 10, 10, and 26, respectively). Analyzing data with missing values is a topic on its own <span class="citation">(Schafer <a href="#ref-Schafer1997" role="doc-biblioref">1997</a>; Little and Rubin <a href="#ref-LittleRubin2002" role="doc-biblioref">2002</a>; Molenberghs and Kenward <a href="#ref-MolKen2007" role="doc-biblioref">2007</a>)</span>. An often-used approach is to impute the missing values. Toward this end, multiple imputation should be considered <span class="citation">(Schafer <a href="#ref-Schafer1997" role="doc-biblioref">1997</a>; Molenberghs and Kenward <a href="#ref-MolKen2007" role="doc-biblioref">2007</a>; Buuren <a href="#ref-vanBuuren2012" role="doc-biblioref">2012</a>)</span>. However, given the limited number of missing values and the intended illustrative use of the dataset, we will limit ourselves to, admittedly inferior, single imputation. In particular, we replace the missing <em>age</em> values by the mean of the observed ones, i.e., 30. Missing <em>country</em> is encoded by <code>"X"</code>. For <em>sibsp</em> and <em>parch</em>, we replace the missing values by the most frequently observed value, i.e., 0. Finally, for <em>fare</em>, we use the mean fare for a given <em>class</em>, i.e., 0 pounds for crew, 89 pounds for the 1st, 22 pounds for the 2nd, and 13 pounds for the 3rd class.</p>
<p>After imputing the missing values, we investigate the association between survival status and other variables. Most variables in the Titanic dataset are categorical, except <em>age</em> and <em>fare</em>. In order to keep the exploration uniform, we first transformed them into categorical variables. Figure <a href="dataSetsIntro.html#fig:titanicExplorationHistograms">5.1</a> shows histograms for both variables. <em>Age</em> is discretized into five categories resulting from cutoffs equal to 5, 10, 20, and 30, while <em>fare</em> is discretized by applying cutoffs equal to 1, 10, 25, and 50.</p>

<div class="figure" style="text-align: center"><span id="fig:titanicExplorationHistograms"></span>
<img src="ema_files/figure-html/titanicExplorationHistograms-1.png" alt="Histograms for variables age and fare for the Titanic data." width="100%" />
<p class="caption">
Figure 5.1: Histograms for variables <em>age</em> and <em>fare</em> for the Titanic data.
</p>
</div>
<p>Figures <a href="dataSetsIntro.html#fig:titanicExplorationGenderAge">5.2</a>–<a href="dataSetsIntro.html#fig:titanicExplorationCountryHarbor">5.5</a> present graphically the proportion of non- and survivors for different levels of the other variables with the use of mosaic plots. The width of the bars (on the x-axis) reflects the marginal distribution (proportions) of the observed levels of the variable. On the other hand, the height of the bars (on the y-axis) provides the information about the proportion of non- and survivors. The graphs for <em>age</em> and <em>fare</em> were constructed by using the categorized versions of the variables.</p>
<p>Figure <a href="dataSetsIntro.html#fig:titanicExplorationGenderAge">5.2</a> indicates that the proportion of survivors was larger for females and children below 5 years of age. This is most likely the result of the “women and children first” principle that is often evoked in situations that require evacuation of persons whose life is in danger.</p>

<div class="figure" style="text-align: center"><span id="fig:titanicExplorationGenderAge"></span>
<img src="ema_files/figure-html/titanicExplorationGenderAge-1.png" alt="Survival according to gender and age category in the Titanic data." width="100%" />
<p class="caption">
Figure 5.2: Survival according to gender and age category in the Titanic data.
</p>
</div>
<p>The principle can, perhaps, partially explain the trend seen in Figure <a href="dataSetsIntro.html#fig:titanicExplorationParch">5.3</a>, i.e., a higher proportion of survivors among those with 1-2 parents/children and 1-2 siblings/spouses aboard.</p>

<div class="figure" style="text-align: center"><span id="fig:titanicExplorationParch"></span>
<img src="ema_files/figure-html/titanicExplorationParch-1.png" alt="Survival according to the number of parents/children and siblings/spouses in the Titanic data." width="100%" />
<p class="caption">
Figure 5.3: Survival according to the number of parents/children and siblings/spouses in the Titanic data.
</p>
</div>
<p>Figure <a href="dataSetsIntro.html#fig:titanicExplorationClassFare">5.4</a> indicates that passengers travelling in the first and second class had a higher chance of survival, perhaps due to the proximity of the location of their cabins to the deck. Interestingly, the proportion of survivors among crew deck was similar to the proportion of the first-class passengers. The figure also shows that the proportion of survivors increased with the fare, which is consistent with the fact that the proportion was higher for passengers travelling in the first and second class.</p>

<div class="figure" style="text-align: center"><span id="fig:titanicExplorationClassFare"></span>
<img src="ema_files/figure-html/titanicExplorationClassFare-1.png" alt="Survival according to travel-class and ticket-fare in the Titanic data." width="100%" />
<p class="caption">
Figure 5.4: Survival according to travel-class and ticket-fare in the Titanic data.
</p>
</div>
<p>Finally, Figure <a href="dataSetsIntro.html#fig:titanicExplorationCountryHarbor">5.5</a> does not suggest any noteworthy trends.</p>

<div class="figure" style="text-align: center"><span id="fig:titanicExplorationCountryHarbor"></span>
<img src="ema_files/figure-html/titanicExplorationCountryHarbor-1.png" alt="Survival according to the embarked harbor and country in the Titanic data." width="100%" />
<p class="caption">
Figure 5.5: Survival according to the embarked harbor and country in the Titanic data.
</p>
</div>
</div>
</div>
<div id="r-classification-models-for-titanic" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> R classification models for Titanic</h2>
<div id="model-titanic-lmr" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Logistic-regression model</h3>
<p>The dependent variable of interest, <em>survived</em>, is binary. Thus, a natural choice is to start the predictive modelling with a logistic-regression model. As there is no reason to expect a linear relationship between age and odds of survival, we use linear tail-restricted cubic splines, available in the <code>rcs()</code> function of the <code>rms</code> package <span class="citation">(Harrell Jr <a href="#ref-rms" role="doc-biblioref">2018</a>)</span>, to model the effect of age. We also do not expect linear relation for the <em>fare</em> variable, but because of its skewness (see Figure <a href="dataSetsIntro.html#fig:titanicExplorationHistograms">5.1</a>), we do not use splines for this variable. The results of the model are stored in model-object <code>titanic_lmr</code>, which will be used in subsequent chapters.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="dataSetsIntro.html#cb6-1"></a><span class="kw">library</span>(<span class="st">&quot;rms&quot;</span>)</span>
<span id="cb6-2"><a href="dataSetsIntro.html#cb6-2"></a>titanic_lmr &lt;-<span class="st"> </span><span class="kw">lrm</span>(survived <span class="op">==</span><span class="st"> &quot;yes&quot;</span> <span class="op">~</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span><span class="kw">rcs</span>(age) <span class="op">+</span><span class="st"> </span>class <span class="op">+</span></span>
<span id="cb6-3"><a href="dataSetsIntro.html#cb6-3"></a><span class="st">         </span>sibsp <span class="op">+</span><span class="st"> </span>parch <span class="op">+</span><span class="st"> </span>fare <span class="op">+</span><span class="st"> </span>embarked, titanic)</span>
<span id="cb6-4"><a href="dataSetsIntro.html#cb6-4"></a>titanic_lmr</span></code></pre></div>
<pre><code>## Logistic Regression Model
##  
##  lrm(formula = survived == &quot;yes&quot; ~ gender + rcs(age) + class + 
##      sibsp + parch + fare + embarked, data = titanic)
##  
##                          Model Likelihood    Discrimination    Rank Discrim.    
##                                Ratio Test           Indexes          Indexes    
##  Obs           2207    LR chi2     752.06    R2       0.404    C       0.817    
##   FALSE        1496    d.f.            17    g        1.647    Dxy     0.635    
##   TRUE          711    Pr(&gt; chi2) &lt;0.0001    gr       5.191    gamma   0.636    
##  max |deriv| 0.0001                          gp       0.282    tau-a   0.277    
##                                              Brier    0.146                     
##  
##                         Coef    S.E.   Wald Z Pr(&gt;|Z|)
##  Intercept               4.5746 0.5480   8.35 &lt;0.0001 
##  gender=male            -2.7687 0.1586 -17.45 &lt;0.0001 
##  age                    -0.1180 0.0221  -5.35 &lt;0.0001 
##  age&#39;                    0.6313 0.1628   3.88 0.0001  
##  age&#39;&#39;                  -2.6583 0.7840  -3.39 0.0007  
##  age&#39;&#39;&#39;                  2.8977 1.0130   2.86 0.0042  
##  class=2nd              -1.1390 0.2501  -4.56 &lt;0.0001 
##  class=3rd              -2.0627 0.2490  -8.28 &lt;0.0001 
##  class=deck crew         1.0672 0.3498   3.05 0.0023  
##  class=engineering crew -0.9702 0.2648  -3.66 0.0002  
##  class=restaurant staff -3.1712 0.6583  -4.82 &lt;0.0001 
##  class=victualling crew -1.0877 0.2596  -4.19 &lt;0.0001 
##  sibsp                  -0.4504 0.1006  -4.48 &lt;0.0001 
##  parch                  -0.0871 0.0987  -0.88 0.3776  
##  fare                    0.0014 0.0020   0.70 0.4842  
##  embarked=Cherbourg      0.7881 0.2836   2.78 0.0055  
##  embarked=Queenstown     0.2745 0.3409   0.80 0.4208  
##  embarked=Southampton    0.2343 0.2119   1.11 0.2689  
## </code></pre>
<p>Note that we are not very much interested in the assessment of the model’s predictive performance, but rather on understanding how does the model yield its predictions. This is why we do not split the data into the training and testing subsets. Instead, the model is fitted to the entire dataset and will be examined on the same dataset.</p>
</div>
<div id="model-titanic-rf" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Random-forest model</h3>
<p>As an alternative to the logistic-regression model we consider a random-forest model. Random-forest modelling is known for good predictive performance, ability to grasp low-order variable interactions, and stability <span class="citation">(Leo Breiman <a href="#ref-randomForestBreiman" role="doc-biblioref">2001</a><a href="#ref-randomForestBreiman" role="doc-biblioref">a</a>)</span>. To fit the model, we apply the <code>randomForest()</code> function, with default settings, from the package with the same name <span class="citation">(Liaw and Wiener <a href="#ref-randomForest" role="doc-biblioref">2002</a>)</span>.</p>
<p>In the first instance, we fit a model with the same set of explanatory variables as the logistic-regression model (see Section <a href="dataSetsIntro.html#model-titanic-lmr">5.2.1</a>). The results of the random-forest model are stored in model-object <code>titanic_rf</code>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="dataSetsIntro.html#cb8-1"></a><span class="kw">library</span>(<span class="st">&quot;randomForest&quot;</span>)</span>
<span id="cb8-2"><a href="dataSetsIntro.html#cb8-2"></a><span class="kw">set.seed</span>(<span class="dv">1313</span>)</span>
<span id="cb8-3"><a href="dataSetsIntro.html#cb8-3"></a>titanic_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(survived <span class="op">~</span><span class="st"> </span>class <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>sibsp <span class="op">+</span><span class="st"> </span>parch <span class="op">+</span><span class="st"> </span>fare <span class="op">+</span><span class="st"> </span>embarked,</span>
<span id="cb8-4"><a href="dataSetsIntro.html#cb8-4"></a>   <span class="dt">data =</span> titanic)</span>
<span id="cb8-5"><a href="dataSetsIntro.html#cb8-5"></a>titanic_rf</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = survived ~ class + gender + age + sibsp +      parch + fare + embarked, data = titanic) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 18.62%
## Confusion matrix:
##       no yes class.error
## no  1393 103  0.06885027
## yes  308 403  0.43319269</code></pre>
<!-- For comparison purposes, we also consider a model with only three explanatory variables: *class*, *gender*, and *age*. The results of the model are stored in model-object `titanic_rf_v3`. -->
</div>
<div id="model-titanic-gbm" class="section level3" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> Gradient-boosting model</h3>
<p>Additionally, we consider the gradient-boosting model <span class="citation">(Friedman <a href="#ref-Friedman00greedyfunction" role="doc-biblioref">2000</a>)</span>. Tree-based-boosting models are known for being able to accommodate higher-order interactions between variables. We use the same set of six explanatory variables as for the logistic-regression model (see Section <a href="dataSetsIntro.html#model-titanic-lmr">5.2.1</a>). To fit the gradient-boosting model, we use function <code>gbm()</code> from the <code>gbm</code> package <span class="citation">(Ridgeway <a href="#ref-gbm" role="doc-biblioref">2017</a>)</span>. The results of the model are stored in model-object <code>titanic_gbm</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="dataSetsIntro.html#cb10-1"></a><span class="kw">library</span>(<span class="st">&quot;gbm&quot;</span>)</span>
<span id="cb10-2"><a href="dataSetsIntro.html#cb10-2"></a><span class="kw">set.seed</span>(<span class="dv">1313</span>)</span>
<span id="cb10-3"><a href="dataSetsIntro.html#cb10-3"></a>titanic_gbm &lt;-<span class="st"> </span><span class="kw">gbm</span>(survived <span class="op">==</span><span class="st"> &quot;yes&quot;</span> <span class="op">~</span><span class="st"> </span>class <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>sibsp <span class="op">+</span><span class="st"> </span></span>
<span id="cb10-4"><a href="dataSetsIntro.html#cb10-4"></a><span class="st">         </span>parch <span class="op">+</span><span class="st"> </span>fare <span class="op">+</span><span class="st"> </span>embarked, <span class="dt">data =</span> titanic, <span class="dt">n.trees =</span> <span class="dv">15000</span>, </span>
<span id="cb10-5"><a href="dataSetsIntro.html#cb10-5"></a>         <span class="dt">distribution =</span> <span class="st">&quot;bernoulli&quot;</span>)</span>
<span id="cb10-6"><a href="dataSetsIntro.html#cb10-6"></a>titanic_gbm</span></code></pre></div>
<pre><code>## gbm(formula = survived == &quot;yes&quot; ~ class + gender + age + sibsp + 
##     parch + fare + embarked, distribution = &quot;bernoulli&quot;, data = titanic, 
##     n.trees = 15000)
## A gradient boosted model with bernoulli loss function.
## 15000 iterations were performed.
## There were 7 predictors of which 7 had non-zero influence.</code></pre>
</div>
<div id="model-titanic-svm" class="section level3" number="5.2.4">
<h3><span class="header-section-number">5.2.4</span> Support Vector Machine model for Classification</h3>
<p>Finally, we also consider a support vector machine model <span class="citation">(Cortes and Vapnik <a href="#ref-svm95vapnik" role="doc-biblioref">1995</a>)</span>. We use the C-classification mode. Again, we fit a model with the same set of explanatory variables as in the logistic-regression model (see Section <a href="dataSetsIntro.html#model-titanic-lmr">5.2.1</a>) To fit the model, we use function <code>svm()</code> from the <code>e1071</code> package <span class="citation">(Meyer et al. <a href="#ref-e1071" role="doc-biblioref">2019</a>)</span>. The results of the model are stored in model-object <code>titanic_svm</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="dataSetsIntro.html#cb12-1"></a><span class="kw">library</span>(<span class="st">&quot;e1071&quot;</span>)</span>
<span id="cb12-2"><a href="dataSetsIntro.html#cb12-2"></a>titanic_svm &lt;-<span class="st"> </span><span class="kw">svm</span>(survived <span class="op">==</span><span class="st"> &quot;yes&quot;</span> <span class="op">~</span><span class="st"> </span>class <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>sibsp <span class="op">+</span></span>
<span id="cb12-3"><a href="dataSetsIntro.html#cb12-3"></a><span class="st">            </span>parch <span class="op">+</span><span class="st"> </span>fare <span class="op">+</span><span class="st"> </span>embarked, <span class="dt">data =</span> titanic, </span>
<span id="cb12-4"><a href="dataSetsIntro.html#cb12-4"></a>            <span class="dt">type =</span> <span class="st">&quot;C-classification&quot;</span>, <span class="dt">probability =</span> <span class="ot">TRUE</span>)</span>
<span id="cb12-5"><a href="dataSetsIntro.html#cb12-5"></a>titanic_svm</span></code></pre></div>
<pre><code>## 
## Call:
## svm(formula = survived == &quot;yes&quot; ~ class + gender + age + sibsp + 
##     parch + fare + embarked, data = titanic, type = &quot;C-classification&quot;, 
##     probability = TRUE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
## 
## Number of Support Vectors:  1030</code></pre>
</div>
<div id="predictions-titanic" class="section level3" number="5.2.5">
<h3><span class="header-section-number">5.2.5</span> Models’ predictions</h3>
<p>Let us now compare predictions that are obtained from the different models. In particular, we compute the predicted probability of survival for Johnny D, an 8-year-old boy who embarked in Southampton and travelled in the first class with no parents nor siblings, and with a ticket costing 72 pounds.</p>
<p>First, we create a dataframe <code>johnny_d</code> that contains the data describing the passenger.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="dataSetsIntro.html#cb14-1"></a>johnny_d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb14-2"><a href="dataSetsIntro.html#cb14-2"></a>            <span class="dt">class =</span> <span class="kw">factor</span>(<span class="st">&quot;1st&quot;</span>, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;1st&quot;</span>, <span class="st">&quot;2nd&quot;</span>, <span class="st">&quot;3rd&quot;</span>, <span class="st">&quot;deck crew&quot;</span>,</span>
<span id="cb14-3"><a href="dataSetsIntro.html#cb14-3"></a>                        <span class="st">&quot;engineering crew&quot;</span>, <span class="st">&quot;restaurant staff&quot;</span>, <span class="st">&quot;victualling crew&quot;</span>)),</span>
<span id="cb14-4"><a href="dataSetsIntro.html#cb14-4"></a>            <span class="dt">gender =</span> <span class="kw">factor</span>(<span class="st">&quot;male&quot;</span>, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;female&quot;</span>, <span class="st">&quot;male&quot;</span>)),</span>
<span id="cb14-5"><a href="dataSetsIntro.html#cb14-5"></a>            <span class="dt">age =</span> <span class="dv">8</span>,</span>
<span id="cb14-6"><a href="dataSetsIntro.html#cb14-6"></a>            <span class="dt">sibsp =</span> <span class="dv">0</span>,</span>
<span id="cb14-7"><a href="dataSetsIntro.html#cb14-7"></a>            <span class="dt">parch =</span> <span class="dv">0</span>,</span>
<span id="cb14-8"><a href="dataSetsIntro.html#cb14-8"></a>            <span class="dt">fare =</span> <span class="dv">72</span>,</span>
<span id="cb14-9"><a href="dataSetsIntro.html#cb14-9"></a>            <span class="dt">embarked =</span> <span class="kw">factor</span>(<span class="st">&quot;Southampton&quot;</span>, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;Belfast&quot;</span>,</span>
<span id="cb14-10"><a href="dataSetsIntro.html#cb14-10"></a>                        <span class="st">&quot;Cherbourg&quot;</span>,<span class="st">&quot;Queenstown&quot;</span>,<span class="st">&quot;Southampton&quot;</span>))</span>
<span id="cb14-11"><a href="dataSetsIntro.html#cb14-11"></a>)</span></code></pre></div>
<p>Subsequently, we use the generic function <code>predict()</code> to obtain the predicted probability of survival for the logistic-regression model.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="dataSetsIntro.html#cb15-1"></a>(pred_lmr &lt;-<span class="st"> </span><span class="kw">predict</span>(titanic_lmr, johnny_d, <span class="dt">type =</span> <span class="st">&quot;fitted&quot;</span>))</span></code></pre></div>
<pre><code>##         1 
## 0.7677036</code></pre>
<p>The predicted probability is equal to 0.77.</p>
<p>We do the same for the remaining three models.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="dataSetsIntro.html#cb17-1"></a>(pred_rf &lt;-<span class="st"> </span><span class="kw">predict</span>(titanic_rf, johnny_d, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))</span></code></pre></div>
<pre><code>##      no   yes
## 1 0.578 0.422
## attr(,&quot;class&quot;)
## [1] &quot;matrix&quot; &quot;array&quot;  &quot;votes&quot;</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="dataSetsIntro.html#cb19-1"></a>(pred_gbm &lt;-<span class="st"> </span><span class="kw">predict</span>(titanic_gbm, johnny_d, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">n.trees =</span> <span class="dv">15000</span>))</span></code></pre></div>
<pre><code>## [1] 0.6632574</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="dataSetsIntro.html#cb21-1"></a>(pred_svm &lt;-<span class="st"> </span><span class="kw">predict</span>(titanic_svm, johnny_d, <span class="dt">probability =</span> <span class="ot">TRUE</span>))</span></code></pre></div>
<pre><code>##     1 
## FALSE 
## attr(,&quot;probabilities&quot;)
##       FALSE      TRUE
## 1 0.7799685 0.2200315
## Levels: FALSE TRUE</code></pre>
<p>As a result, we obtain the predicted probabilities of 0.42, 0.66, and 0.22 for the random-forest, gradient-boosting, and support vector machine models, respectively. The models lead to different probabilities. Thus, it might be of interest to understand the reason for the differences, as it could help us to decide which of the predictions we might want to trust. We will investigate this issue in the subsequent chapters.</p>
<p>Note that, for some examples later in the book, we will use another observation (instance) with lower chances of survival. We will call this passenger Henry.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="dataSetsIntro.html#cb23-1"></a>henry &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb23-2"><a href="dataSetsIntro.html#cb23-2"></a>         <span class="dt">class =</span> <span class="kw">factor</span>(<span class="st">&quot;1st&quot;</span>, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;1st&quot;</span>, <span class="st">&quot;2nd&quot;</span>, <span class="st">&quot;3rd&quot;</span>, <span class="st">&quot;deck crew&quot;</span>, </span>
<span id="cb23-3"><a href="dataSetsIntro.html#cb23-3"></a>                     <span class="st">&quot;engineering crew&quot;</span>, <span class="st">&quot;restaurant staff&quot;</span>, <span class="st">&quot;victualling crew&quot;</span>)),</span>
<span id="cb23-4"><a href="dataSetsIntro.html#cb23-4"></a>         <span class="dt">gender =</span> <span class="kw">factor</span>(<span class="st">&quot;male&quot;</span>, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;female&quot;</span>, <span class="st">&quot;male&quot;</span>)),</span>
<span id="cb23-5"><a href="dataSetsIntro.html#cb23-5"></a>         <span class="dt">age =</span> <span class="dv">47</span>,</span>
<span id="cb23-6"><a href="dataSetsIntro.html#cb23-6"></a>         <span class="dt">sibsp =</span> <span class="dv">0</span>,</span>
<span id="cb23-7"><a href="dataSetsIntro.html#cb23-7"></a>         <span class="dt">parch =</span> <span class="dv">0</span>,</span>
<span id="cb23-8"><a href="dataSetsIntro.html#cb23-8"></a>         <span class="dt">fare =</span> <span class="dv">25</span>,</span>
<span id="cb23-9"><a href="dataSetsIntro.html#cb23-9"></a>         <span class="dt">embarked =</span> <span class="kw">factor</span>(<span class="st">&quot;Cherbourg&quot;</span>, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;Belfast&quot;</span>,</span>
<span id="cb23-10"><a href="dataSetsIntro.html#cb23-10"></a>                           <span class="st">&quot;Cherbourg&quot;</span>,<span class="st">&quot;Queenstown&quot;</span>,<span class="st">&quot;Southampton&quot;</span>))</span>
<span id="cb23-11"><a href="dataSetsIntro.html#cb23-11"></a>)</span>
<span id="cb23-12"><a href="dataSetsIntro.html#cb23-12"></a><span class="kw">predict</span>(titanic_lmr, henry, <span class="dt">type =</span> <span class="st">&quot;fitted&quot;</span>)</span></code></pre></div>
<pre><code>##         1 
## 0.4318245</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="dataSetsIntro.html#cb25-1"></a><span class="kw">predict</span>(titanic_rf, henry, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)[,<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## [1] 0.246</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="dataSetsIntro.html#cb27-1"></a><span class="kw">predict</span>(titanic_gbm, henry, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">n.trees =</span> <span class="dv">15000</span>)</span></code></pre></div>
<pre><code>## [1] 0.3073358</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="dataSetsIntro.html#cb29-1"></a><span class="kw">attr</span>(<span class="kw">predict</span>(titanic_svm, henry, <span class="dt">probability =</span> <span class="ot">TRUE</span>),<span class="st">&quot;probabilities&quot;</span>)[,<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## [1] 0.1767995</code></pre>
</div>
<div id="ExplainersTitanicRCode" class="section level3" number="5.2.6">
<h3><span class="header-section-number">5.2.6</span> Models’ explainers</h3>
<p>Model-objects created with different libraries may have different internal structures. Thus, first, we have got to create an “explainer,” i.e., an object that provides an uniform interface for different models. Toward this end, we use the <code>explain()</code> function from the <code>DALEX</code> package <span class="citation">(Biecek <a href="#ref-DALEX" role="doc-biblioref">2018</a>)</span>. As it was mentioned in Section <a href="doItYourselfWithR.html#infoDALEX">3.2</a>, there is only one argument that is required by the function, i.e., <code>model</code>. The argument is used to specify the model-object with the fitted form of the model. However, the function allows additional arguments that extend its functionalities. In particular, the list of arguments includes the following:</p>
<ul>
<li><code>data</code>, a data frame or matrix providing data to which the model is to be applied; if not provided (<code>data = NULL</code> by default), the data are extracted from the model-object. Note that the data object should not in principle, contain the dependent variable.</li>
<li><code>y</code>, observed values of the dependent variable corresponding to the data given in the <code>data</code> object; if not provided (<code>y = NULL</code> by default), the values are extracted from the model-object;</li>
<li><code>predict_function</code>, a function that returns prediction scores; if not specified (<code>predict_function = NULL</code> by default), then a default <code>predict()</code> function is used (note that this may lead to errors);</li>
<li><code>residual_function</code>, a function that returns model residuals; if not specified (<code>residual_function = NULL</code> by default), then model residuals defined in equation <a href="modelDevelopmentProcess.html#eq:modelResiduals">(2.1)</a> are calculated;</li>
<li><code>verbose</code>, a logical argument (<code>verbose = TRUE</code> by default) indicating whether diagnostic messages are to be printed;</li>
<li><code>precalculate</code>, a logical argument (<code>precalculate = TRUE</code> by default) indicating whether predicted values and residuals are to be calculated when the explainer is created. Note that this will also happen if <code>verbose = TRUE</code>. To skip the calculations, both <code>verbose</code> and <code>precalculate</code> should be set to FALSE .</li>
<li><code>model_info</code>, a named list (with components “package,” “version,” and “type”) providing information about the model; if not specified (<code>model_info = NULL</code> by default), <code>DALEX</code> seeks for information on its own;</li>
<li><code>type</code>, information about the type of the model, either “classification” (for a binary dependent variable) or “regression” (for a continuous depenent variable); if not specified (<code>type = NULL</code> by default), then the value of the argument is extracted from <code>model_info</code>;</li>
<li><code>label</code>, a unique name of the model; if not specified (<code>label = NULL</code> by default), then it is extracted from <code>class(model)</code>.</li>
</ul>
<p>Application of function <code>explain()</code> provides an object of class <code>explainer</code>. It is a list with many components that include:</p>
<ul>
<li><code>model</code>, the explained model;</li>
<li><code>data</code>, the data to which the model is applied;</li>
<li><code>y</code>, observed values of the dependet variable corresponding to <code>data</code>;</li>
<li><code>y_hat</code>, predictions obtained by applying <code>model</code> to <code>data</code>;</li>
<li><code>residuals</code>, residuals computed based on <code>y</code> and <code>y_hat</code>;</li>
<li><code>predict_function</code>, the function used to obtain the model’s predictions;</li>
<li><code>residual_function</code>, the function used to obtain residuals;</li>
<li><code>class</code>, class/classes of the model;</li>
<li><code>label</code>, label of the model/explainer;</li>
<li><code>model_info</code>, a named list (with components <code>package</code>, <code>version</code>, and <code>type</code>) providing information about the model.</li>
</ul>
<p>Thus, each explainer-object contains all elements needed to create a model explanation. The code below creates explainers for the models (see Sections <a href="dataSetsIntro.html#model-titanic-lmr">5.2.1</a>–<a href="dataSetsIntro.html#model-titanic-svm">5.2.4</a>) fitted to the Titanic data. Note that, in the <code>data</code> argument, we indicate the <code>titanic</code> data frame without the ninth column, i.e., without the <em>survived</em> variable. The variable is used in the <code>y</code> argument to explicitly define the binary dependent variable equal to 1 for survivors and 0 for passengers who did not survive.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="dataSetsIntro.html#cb31-1"></a>titanic_lmr_exp &lt;-<span class="st"> </span><span class="kw">explain</span>(<span class="dt">model =</span> titanic_lmr, </span>
<span id="cb31-2"><a href="dataSetsIntro.html#cb31-2"></a>                           <span class="dt">data =</span> titanic[, <span class="dv">-9</span>],</span>
<span id="cb31-3"><a href="dataSetsIntro.html#cb31-3"></a>                           <span class="dt">y =</span> titanic<span class="op">$</span>survived <span class="op">==</span><span class="st"> &quot;yes&quot;</span>, </span>
<span id="cb31-4"><a href="dataSetsIntro.html#cb31-4"></a>                           <span class="dt">label =</span> <span class="st">&quot;Logistic Regression&quot;</span>)</span>
<span id="cb31-5"><a href="dataSetsIntro.html#cb31-5"></a>titanic_lmr_exp<span class="op">$</span>model_info<span class="op">$</span>type =<span class="st"> &quot;classification&quot;</span></span>
<span id="cb31-6"><a href="dataSetsIntro.html#cb31-6"></a></span>
<span id="cb31-7"><a href="dataSetsIntro.html#cb31-7"></a>titanic_rf_exp &lt;-<span class="st"> </span><span class="kw">explain</span>(<span class="dt">model =</span> titanic_rf, </span>
<span id="cb31-8"><a href="dataSetsIntro.html#cb31-8"></a>                           <span class="dt">data =</span> titanic[, <span class="dv">-9</span>],</span>
<span id="cb31-9"><a href="dataSetsIntro.html#cb31-9"></a>                           <span class="dt">y =</span> titanic<span class="op">$</span>survived <span class="op">==</span><span class="st"> &quot;yes&quot;</span>, </span>
<span id="cb31-10"><a href="dataSetsIntro.html#cb31-10"></a>                           <span class="dt">label =</span> <span class="st">&quot;Random Forest&quot;</span>)</span>
<span id="cb31-11"><a href="dataSetsIntro.html#cb31-11"></a></span>
<span id="cb31-12"><a href="dataSetsIntro.html#cb31-12"></a>titanic_gbm_exp &lt;-<span class="st"> </span><span class="kw">explain</span>(<span class="dt">model =</span> titanic_gbm, </span>
<span id="cb31-13"><a href="dataSetsIntro.html#cb31-13"></a>                           <span class="dt">data =</span> titanic[, <span class="dv">-9</span>],</span>
<span id="cb31-14"><a href="dataSetsIntro.html#cb31-14"></a>                           <span class="dt">y =</span> titanic<span class="op">$</span>survived <span class="op">==</span><span class="st"> &quot;yes&quot;</span>, </span>
<span id="cb31-15"><a href="dataSetsIntro.html#cb31-15"></a>                           <span class="dt">label =</span> <span class="st">&quot;Generalized Boosted Regression&quot;</span>)</span>
<span id="cb31-16"><a href="dataSetsIntro.html#cb31-16"></a></span>
<span id="cb31-17"><a href="dataSetsIntro.html#cb31-17"></a>titanic_svm_exp &lt;-<span class="st"> </span><span class="kw">explain</span>(<span class="dt">model =</span> titanic_svm, </span>
<span id="cb31-18"><a href="dataSetsIntro.html#cb31-18"></a>                           <span class="dt">data =</span> titanic[, <span class="dv">-9</span>],</span>
<span id="cb31-19"><a href="dataSetsIntro.html#cb31-19"></a>                           <span class="dt">y =</span> titanic<span class="op">$</span>survived <span class="op">==</span><span class="st"> &quot;yes&quot;</span>, </span>
<span id="cb31-20"><a href="dataSetsIntro.html#cb31-20"></a>                           <span class="dt">label =</span> <span class="st">&quot;Support Vector Machine&quot;</span>)</span></code></pre></div>
</div>
<div id="ListOfModelsTitanic" class="section level3" number="5.2.7">
<h3><span class="header-section-number">5.2.7</span> List of objects for the Titanic example</h3>
<p>In the previous sections, we have built four predictive models for the Titanic dataset. The models will be used in the rest of the book to illustrate model-explanation methods and tools.</p>
<p>For the ease of reference, we summarize the models in Table <a href="dataSetsIntro.html#tab:archivistHooksOfModelsTitanic">5.1</a>. The binary model-objects can be downloaded by using the indicated <code>archivist</code> hooks <span class="citation">(Biecek and Kosinski <a href="#ref-archivist" role="doc-biblioref">2017</a>)</span>. By calling a function specified in the last column of the table, one can restore a selected model in its local R environment.</p>
<table style="width:100%;">
<caption><span id="tab:archivistHooksOfModelsTitanic">Table 5.1: </span> Predictive models created for the <code>titanic</code> dataset.</caption>
<colgroup>
<col width="23%" />
<col width="28%" />
<col width="20%" />
<col width="28%" />
</colgroup>
<thead>
<tr class="header">
<th>Model name</th>
<th>Model generator</th>
<th>Variables</th>
<th>Archivist hooks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>titanic_lmr</code></td>
<td><code>rms:: lmr</code> v.5.1.3</td>
<td>gender, age, class, sibsp, parch, fare, embarked</td>
<td>Get the model: <code>archivist:: aread("pbiecek/models/58b24")</code>.</td>
</tr>
<tr class="even">
<td><code>titanic_rf</code></td>
<td><code>randomForest:: randomForest</code> v.4.6.14</td>
<td>gender, age, class, sibsp, parch, fare, embarked</td>
<td>Get the model: <code>archivist:: aread("pbiecek/models/4e0fc")</code>.</td>
</tr>
<tr class="odd">
<td><code>titanic_gbm</code></td>
<td><code>gbm:: gbm</code> v.2.1.5</td>
<td>gender, age, class, sibsp, parch, fare, embarked</td>
<td>Get the model: <code>archivist:: aread("pbiecek/models/b7078")</code>.</td>
</tr>
<tr class="even">
<td><code>titanic_svm</code></td>
<td><code>e1071:: svm</code> v.1.7.3</td>
<td>gender, age, class, sibsp, parch, fare, embarked</td>
<td>Get the model: <code>archivist:: aread("pbiecek/models/9c27f")</code>.</td>
</tr>
</tbody>
</table>
<!-- 
Get the explainer: `archivist:: aread("pbiecek/models/ff1cd")` 
Get the explainer: `archivist:: aread("pbiecek/models/6ed54")`
Get the explainer: `archivist:: aread("pbiecek/models/87271")`
Get the explainer: `archivist:: aread("pbiecek/models/21966")`

| `titanic_rf_v3`  | `randomForest:: randomForest`  v.4.6.14 | gender, age, class  | Get the model:  `archivist:: aread("pbiecek/models/293e8")`. Get the explainer: `archivist:: aread("pbiecek/models/5b32a")` |
-->
<p>Table <a href="dataSetsIntro.html#tab:archivistHooksOfDataFramesTitanic">5.2</a> summarizes the data frames that will be used in examples in the subsequent chapters.</p>
<table>
<caption><span id="tab:archivistHooksOfDataFramesTitanic">Table 5.2: </span> Data frames created for the Titanic use-case.</caption>
<colgroup>
<col width="24%" />
<col width="17%" />
<col width="21%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th>Description</th>
<th>No. rows</th>
<th>Variables</th>
<th>Link to this object</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>titanic</code> dataset with imputed missing values</td>
<td>2207</td>
<td>gender, age, class, embarked, country, fare, sibsp, parch, survived</td>
<td><code>archivist:: aread("pbiecek/models/27e5c")</code></td>
</tr>
<tr class="even">
<td><code>johnny_d</code> 8-year-old boy from the 1st class without parents, paid 72 pounds, embarked in Southampton</td>
<td>1</td>
<td>class, gender, age, sibsp, parch, fare, embarked</td>
<td><code>archivist:: aread("pbiecek/models/e3596")</code></td>
</tr>
<tr class="odd">
<td><code>henry</code> 47-year-old male from the 1st class, travelled alone, paid 25 pounds, embarked in Cherbourg</td>
<td>1</td>
<td>class, gender, age, sibsp, parch, fare, embarked</td>
<td><code>archivist:: aread("pbiecek/models/a6538")</code></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="python-classification-models-for-titanic" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Python classification models for Titanic</h2>
<p>Examples in Python are based on the <code>titanic</code> data set, which is available in the <code>dalex</code> library. The survived column is a target variable, the remaining columns will be used to construct the classifier.</p>
<p>The following instructions load the titanic dataset and split it into the target variable <code>y</code> and the predictive variables <code>X</code>. For the purpose of this example we do not divide the data into training and testing data. Instructions on how to deal with the situation when you want to analyze the model on data other than training data will be presented in the next chapter.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="dataSetsIntro.html#cb32-1"></a><span class="im">import</span> dalex <span class="im">as</span> dx</span>
<span id="cb32-2"><a href="dataSetsIntro.html#cb32-2"></a>titanic <span class="op">=</span> dx.datasets.load_titanic()</span>
<span id="cb32-3"><a href="dataSetsIntro.html#cb32-3"></a>X <span class="op">=</span> titanic.drop(columns<span class="op">=</span><span class="st">&#39;survived&#39;</span>)</span>
<span id="cb32-4"><a href="dataSetsIntro.html#cb32-4"></a>y <span class="op">=</span> titanic.survived</span></code></pre></div>
<p>Data <code>X</code> contains numeric variables with different ranges (e.g. age and fare) and categorical variables. Machine Learning algorithms in scikitlearn require the data to be preprocessed into numeric form. Therefore, before modeling, we prepared a pipeline that performs data preprocessing. That is scaling for continuous variables (<em>age</em>, <em>fare</em>, <em>parch</em>, <em>sibsp</em>) and one-hot-encoding for qualitative variables (<em>gender</em>, <em>class</em>, <em>embarked</em>).</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="dataSetsIntro.html#cb33-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, OneHotEncoder</span>
<span id="cb33-2"><a href="dataSetsIntro.html#cb33-2"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> make_column_transformer</span>
<span id="cb33-3"><a href="dataSetsIntro.html#cb33-3"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb33-4"><a href="dataSetsIntro.html#cb33-4"></a></span>
<span id="cb33-5"><a href="dataSetsIntro.html#cb33-5"></a>preprocess <span class="op">=</span> make_column_transformer(</span>
<span id="cb33-6"><a href="dataSetsIntro.html#cb33-6"></a>    (StandardScaler(), [<span class="st">&#39;age&#39;</span>, <span class="st">&#39;fare&#39;</span>, <span class="st">&#39;parch&#39;</span>, <span class="st">&#39;sibsp&#39;</span>]),</span>
<span id="cb33-7"><a href="dataSetsIntro.html#cb33-7"></a>    (OneHotEncoder(), [<span class="st">&#39;gender&#39;</span>, <span class="st">&#39;class&#39;</span>, <span class="st">&#39;embarked&#39;</span>]))</span></code></pre></div>
<div id="model-titanic-python-lr" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Logistic-regression model</h3>
<p>The dependent variable of interest, <em>survived</em>, is binary. Thus, a natural choice is to start the predictive modelling with a logistic-regression model. Here we use the <code>LogisticRegression</code> algorithm from <code>sklearn</code> library. By default the <code>sklearn</code> implementation use ridge regression with <span class="math inline">\(L_2\)</span> penalty for model coefficients. This is whay it was impotant to scale numerical valriables like <code>age</code> and <code>fare</code>.</p>
<p>The result is a model stored in object <code>titanic_lr</code>, which will be used in subsequent chapters.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="dataSetsIntro.html#cb34-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb34-2"><a href="dataSetsIntro.html#cb34-2"></a></span>
<span id="cb34-3"><a href="dataSetsIntro.html#cb34-3"></a>titanic_lr <span class="op">=</span> make_pipeline(</span>
<span id="cb34-4"><a href="dataSetsIntro.html#cb34-4"></a>    preprocess,</span>
<span id="cb34-5"><a href="dataSetsIntro.html#cb34-5"></a>    LogisticRegression(penalty<span class="op">=</span><span class="st">&#39;l2&#39;</span>))</span>
<span id="cb34-6"><a href="dataSetsIntro.html#cb34-6"></a>    </span>
<span id="cb34-7"><a href="dataSetsIntro.html#cb34-7"></a>titanic_lr.fit(X, y)</span></code></pre></div>
<p>Note that we are not very much interested in the assessment of the model’s predictive performance, but rather on understanding how does the model yield its predictions. This is why we do not split the data into the training and testing subsets. Instead, the model is fitted to the entire dataset and will be examined on the same dataset.</p>
</div>
<div id="model-titanic-python-rf" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Random-forest model</h3>
<p>As an alternative to the logistic-regression model we consider a random-forest model. Random-forest modelling is known for good predictive performance, ability to grasp low-order variable interactions, and stability <span class="citation">(Leo Breiman <a href="#ref-randomForestBreiman" role="doc-biblioref">2001</a><a href="#ref-randomForestBreiman" role="doc-biblioref">a</a>)</span>. To fit the model, we use <code>RandomForestClassifier</code> algorithm from <code>sklearn</code> library. We use the default settings with tree not deeper than 3 levels, and the number of trees in the random forest is set to 500.</p>
<p>The result is a model stored in object <code>model_rf</code>, which will be used in subsequent chapters.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="dataSetsIntro.html#cb35-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb35-2"><a href="dataSetsIntro.html#cb35-2"></a></span>
<span id="cb35-3"><a href="dataSetsIntro.html#cb35-3"></a>titanic_rf <span class="op">=</span> make_pipeline(</span>
<span id="cb35-4"><a href="dataSetsIntro.html#cb35-4"></a>    preprocess,</span>
<span id="cb35-5"><a href="dataSetsIntro.html#cb35-5"></a>    RandomForestClassifier(max_depth<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">0</span>, n_estimators<span class="op">=</span><span class="dv">500</span>))</span>
<span id="cb35-6"><a href="dataSetsIntro.html#cb35-6"></a>    </span>
<span id="cb35-7"><a href="dataSetsIntro.html#cb35-7"></a>titanic_rf.fit(X, y)</span></code></pre></div>
</div>
<div id="model-titanic-python-gbm" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Gradient-boosting model</h3>
<p>Additionally, we consider the gradient-boosting model <span class="citation">(Friedman <a href="#ref-Friedman00greedyfunction" role="doc-biblioref">2000</a>)</span>. Tree-based-boosting models are known for being able to accommodate higher-order interactions between variables.
To fit the model, we use <code>GradientBoostingClassifier</code> algorithm from <code>sklearn</code> library. We use the default settings and the number of trees in the ensemble is set to 100.</p>
<p>The result is a model stored in object <code>model_gbc</code>, which will be used in subsequent chapters.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="dataSetsIntro.html#cb36-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb36-2"><a href="dataSetsIntro.html#cb36-2"></a></span>
<span id="cb36-3"><a href="dataSetsIntro.html#cb36-3"></a>titanic_gbc <span class="op">=</span> make_pipeline(</span>
<span id="cb36-4"><a href="dataSetsIntro.html#cb36-4"></a>    preprocess,</span>
<span id="cb36-5"><a href="dataSetsIntro.html#cb36-5"></a>    GradientBoostingClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>))</span>
<span id="cb36-6"><a href="dataSetsIntro.html#cb36-6"></a></span>
<span id="cb36-7"><a href="dataSetsIntro.html#cb36-7"></a>titanic_gbc.fit(X, y)</span></code></pre></div>
</div>
<div id="model-titanic-python-svm" class="section level3" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> Support Vector Machine model for Classification</h3>
<p>Finally, we also consider a support vector machine model <span class="citation">(Cortes and Vapnik <a href="#ref-svm95vapnik" role="doc-biblioref">1995</a>)</span>. We use the C-Support Vector Classification mode.
To fit the model, we use <code>SVC</code> algorithm from <code>sklearn</code> library based on <code>libsvm</code>.</p>
<p>The result is a model stored in object <code>model_svm</code>, which will be used in subsequent chapters.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="dataSetsIntro.html#cb37-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb37-2"><a href="dataSetsIntro.html#cb37-2"></a></span>
<span id="cb37-3"><a href="dataSetsIntro.html#cb37-3"></a>titanic_svm <span class="op">=</span> make_pipeline(</span>
<span id="cb37-4"><a href="dataSetsIntro.html#cb37-4"></a>    preprocess,</span>
<span id="cb37-5"><a href="dataSetsIntro.html#cb37-5"></a>    SVC(probability<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb37-6"><a href="dataSetsIntro.html#cb37-6"></a>    </span>
<span id="cb37-7"><a href="dataSetsIntro.html#cb37-7"></a>titanic_svm.fit(X, y)</span></code></pre></div>
</div>
<div id="predictions-titanic-python" class="section level3" number="5.3.5">
<h3><span class="header-section-number">5.3.5</span> Models’ predictions</h3>
<p>Let us now compare predictions that are obtained from the different models. In particular, we compute the predicted probability of survival for Johnny D, an 8-year-old boy who embarked in Southampton and travelled in the first class with no parents nor siblings, and with a ticket costing 72 pounds.</p>
<p>First, we create a dataframe <code>johnny_d</code> that contains the data describing the passenger.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="dataSetsIntro.html#cb38-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb38-2"><a href="dataSetsIntro.html#cb38-2"></a></span>
<span id="cb38-3"><a href="dataSetsIntro.html#cb38-3"></a>johnny_d <span class="op">=</span> pd.DataFrame({<span class="st">&#39;gender&#39;</span>: [<span class="st">&#39;male&#39;</span>],</span>
<span id="cb38-4"><a href="dataSetsIntro.html#cb38-4"></a>                       <span class="st">&#39;age&#39;</span>: [<span class="dv">8</span>],</span>
<span id="cb38-5"><a href="dataSetsIntro.html#cb38-5"></a>                       <span class="st">&#39;class&#39;</span>: [<span class="st">&#39;1st&#39;</span>],</span>
<span id="cb38-6"><a href="dataSetsIntro.html#cb38-6"></a>                       <span class="st">&#39;embarked&#39;</span>: [<span class="st">&#39;Southampton&#39;</span>],</span>
<span id="cb38-7"><a href="dataSetsIntro.html#cb38-7"></a>                       <span class="st">&#39;fare&#39;</span>: [<span class="dv">72</span>],</span>
<span id="cb38-8"><a href="dataSetsIntro.html#cb38-8"></a>                       <span class="st">&#39;sibsp&#39;</span>: [<span class="dv">0</span>],</span>
<span id="cb38-9"><a href="dataSetsIntro.html#cb38-9"></a>                       <span class="st">&#39;parch&#39;</span>: [<span class="dv">0</span>]},</span>
<span id="cb38-10"><a href="dataSetsIntro.html#cb38-10"></a>                      index <span class="op">=</span> [<span class="st">&#39;JohnnyD&#39;</span>])</span></code></pre></div>
<p>Subsequently, we use the method <code>predict_proba()</code> to obtain the predicted probability of survival for the logistic-regression model.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="dataSetsIntro.html#cb39-1"></a>titanic_lr.predict_proba(johnny_d)</span>
<span id="cb39-2"><a href="dataSetsIntro.html#cb39-2"></a><span class="co"># array([[0.35884528, 0.64115472]])</span></span></code></pre></div>
<p>We do the same for the remaining three models.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="dataSetsIntro.html#cb40-1"></a>titanic_rf.predict_proba(johnny_d)</span>
<span id="cb40-2"><a href="dataSetsIntro.html#cb40-2"></a><span class="co"># array([[0.63028556, 0.36971444]])</span></span>
<span id="cb40-3"><a href="dataSetsIntro.html#cb40-3"></a></span>
<span id="cb40-4"><a href="dataSetsIntro.html#cb40-4"></a>titanic_gbc.predict_proba(johnny_d)</span>
<span id="cb40-5"><a href="dataSetsIntro.html#cb40-5"></a><span class="co"># array([[0.1567194, 0.8432806]])</span></span>
<span id="cb40-6"><a href="dataSetsIntro.html#cb40-6"></a></span>
<span id="cb40-7"><a href="dataSetsIntro.html#cb40-7"></a>titanic_svm.predict_proba(johnny_d)</span>
<span id="cb40-8"><a href="dataSetsIntro.html#cb40-8"></a><span class="co"># array([[0.78308146, 0.21691854]])</span></span></code></pre></div>
<p>Note that, for some examples later in the book, we will use another observation (instance) with lower chances of survival. We will call this passenger Henry.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="dataSetsIntro.html#cb41-1"></a>henry <span class="op">=</span> pd.DataFrame({<span class="st">&#39;gender&#39;</span>: [<span class="st">&#39;male&#39;</span>],</span>
<span id="cb41-2"><a href="dataSetsIntro.html#cb41-2"></a>                       <span class="st">&#39;age&#39;</span>: [<span class="dv">47</span>],</span>
<span id="cb41-3"><a href="dataSetsIntro.html#cb41-3"></a>                       <span class="st">&#39;class&#39;</span>: [<span class="st">&#39;1st&#39;</span>],</span>
<span id="cb41-4"><a href="dataSetsIntro.html#cb41-4"></a>                       <span class="st">&#39;embarked&#39;</span>: [<span class="st">&#39;Southampton&#39;</span>],</span>
<span id="cb41-5"><a href="dataSetsIntro.html#cb41-5"></a>                       <span class="st">&#39;fare&#39;</span>: [<span class="dv">25</span>],</span>
<span id="cb41-6"><a href="dataSetsIntro.html#cb41-6"></a>                       <span class="st">&#39;sibsp&#39;</span>: [<span class="dv">0</span>],</span>
<span id="cb41-7"><a href="dataSetsIntro.html#cb41-7"></a>                       <span class="st">&#39;parch&#39;</span>: [<span class="dv">0</span>]},</span>
<span id="cb41-8"><a href="dataSetsIntro.html#cb41-8"></a>                      index <span class="op">=</span> [<span class="st">&#39;Henry&#39;</span>])</span>
<span id="cb41-9"><a href="dataSetsIntro.html#cb41-9"></a></span>
<span id="cb41-10"><a href="dataSetsIntro.html#cb41-10"></a>titanic_lr.predict_proba(henry)</span>
<span id="cb41-11"><a href="dataSetsIntro.html#cb41-11"></a><span class="co"># array([[0.69547744, 0.30452256]])</span></span>
<span id="cb41-12"><a href="dataSetsIntro.html#cb41-12"></a></span>
<span id="cb41-13"><a href="dataSetsIntro.html#cb41-13"></a>titanic_rf.predict_proba(henry)</span>
<span id="cb41-14"><a href="dataSetsIntro.html#cb41-14"></a><span class="co"># array([[0.73060059, 0.26939941]])</span></span>
<span id="cb41-15"><a href="dataSetsIntro.html#cb41-15"></a></span>
<span id="cb41-16"><a href="dataSetsIntro.html#cb41-16"></a>titanic_gbc.predict_proba(henry)</span>
<span id="cb41-17"><a href="dataSetsIntro.html#cb41-17"></a><span class="co"># array([[0.1567194, 0.8432806]])</span></span>
<span id="cb41-18"><a href="dataSetsIntro.html#cb41-18"></a></span>
<span id="cb41-19"><a href="dataSetsIntro.html#cb41-19"></a>titanic_svm.predict(henry)</span>
<span id="cb41-20"><a href="dataSetsIntro.html#cb41-20"></a><span class="co"># array([[0.82429369, 0.17570631]])</span></span></code></pre></div>
</div>
<div id="ExplainersTitanicPythonCode" class="section level3" number="5.3.6">
<h3><span class="header-section-number">5.3.6</span> Models’ explainers</h3>
<p>The examples that we show in this chapter are based on the sklearn library, which makes it possible to work with models in a uniform way. But often we also want to work with models built in other libraries. To make it easier to work models with different structures, the <code>dalex</code> library wraps models in the objects of the class <code>Explainer</code>, that have all the necessary functions of the model available in a uniform way.</p>
<p>There is only one argument that is required by the function, i.e., <code>model</code>. However, the function allows additional arguments that extend its functionalities. In particular, the list of arguments includes the following:</p>
<ul>
<li><code>data</code>, a data frame or matrix providing data to which the model is to be applied.</li>
<li><code>y</code>, observed values of the dependent variable corresponding to the data given in the <code>data</code> object;</li>
<li><code>predict_function</code>, a function that returns prediction scores; if not specified, then <code>dalex</code> will make a guess which function shall be used, <code>predict()</code>, <code>predict_proba()</code> or something else;</li>
<li><code>residual_function</code>, a function that returns model residuals;</li>
<li><code>label</code>, a unique name of the model;</li>
<li><code>model_class</code>, class of actual model;</li>
<li><code>verbose</code>, a logical argument (<code>verbose = TRUE</code> by default) indicating whether diagnostic messages are to be printed;</li>
<li><code>model_type</code>, information about the type of the model, either “classification” (for a binary dependent variable) or “regression” (for a continuous dependent variable);</li>
<li><code>model_info</code>, a dictionary with additional information about the model.</li>
</ul>
<p>Application of constructor <code>Explainer()</code> provides an object of class “Explainer”. It is an object with many components that include:</p>
<ul>
<li><code>model</code>, the explained model;</li>
<li><code>data</code>, the data to which the model is applied;</li>
<li><code>y</code>, observed values of the dependet variable corresponding to <code>data</code>;</li>
<li><code>y_hat</code>, predictions obtained by applying <code>model</code> to ’data`;</li>
<li><code>residuals</code>, residuals computed based on <code>y</code> and <code>y_hat</code>;</li>
<li><code>predict_function</code>, the function used to obtain the model’s predictions;</li>
<li><code>residual_function</code>, the function used to obtain residuals;</li>
<li><code>class</code>, class/classes of the model;</li>
<li><code>label</code>, label of the model/Explainer;</li>
<li><code>model_info</code>, a dictionary (with components “package,” “version,” and “type”) providing information about the model.</li>
</ul>
<p>Thus, each Explainer-object contains all elements needed to create a model explanation. The code below creates explainers for the models (see Sections <a href="dataSetsIntro.html#model-titanic-python-lr">5.3.1</a>–<a href="dataSetsIntro.html#model-titanic-python-svm">5.3.4</a>) fitted to the Titanic data.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="dataSetsIntro.html#cb42-1"></a>titanic_rf_exp <span class="op">=</span> dx.Explainer(titanic_rf, X, y, label <span class="op">=</span> <span class="st">&quot;Titanic RF Pipeline&quot;</span>)</span>
<span id="cb42-2"><a href="dataSetsIntro.html#cb42-2"></a></span>
<span id="cb42-3"><a href="dataSetsIntro.html#cb42-3"></a>titanic_lr_exp <span class="op">=</span> dx.Explainer(titanic_lr, X, y, label <span class="op">=</span> <span class="st">&quot;Titanic LR Pipeline&quot;</span>)</span>
<span id="cb42-4"><a href="dataSetsIntro.html#cb42-4"></a></span>
<span id="cb42-5"><a href="dataSetsIntro.html#cb42-5"></a>titanic_gbc_exp <span class="op">=</span> dx.Explainer(titanic_gbc, X, y, label <span class="op">=</span> <span class="st">&quot;Titanic XGB Pipeline&quot;</span>)</span>
<span id="cb42-6"><a href="dataSetsIntro.html#cb42-6"></a></span>
<span id="cb42-7"><a href="dataSetsIntro.html#cb42-7"></a>titanic_svm_exp <span class="op">=</span> dx.Explainer(titanic_svm, X, y, label <span class="op">=</span> <span class="st">&quot;Titanic SVM Pipeline&quot;</span>)</span></code></pre></div>
</div>
</div>
<div id="ApartmentDataset" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Apartment prices</h2>
<div class="figure">
<img src="figure/am1974_flicker.jpg" alt="" />
<p class="caption">Warsaw skyscrapers by Artur Malinowski Flicker</p>
</div>
<p>Predicting house prices is a common exercise used in machine-learning courses. Various datasets for house prices are available at websites like Kaggle (<a href="https://www.kaggle.com" class="uri">https://www.kaggle.com</a>) or UCI Machine Learning Repository (<a href="https://archive.ics.uci.edu" class="uri">https://archive.ics.uci.edu</a>).</p>
<p>In this book, we will work with an interesting variant of this problem. The <code>apartments</code> dataset is an artificial dataset created to match key characteristics of real apartments in Warsaw, the capital of Poland. However, the dataset is created in a way that two very different models, namely linear regression and random forest, have almost exactly the same accuracy. The natural question is then: which model should we choose? We will show that the model-explanation tools provide important insight into the key model characteristics and are helpful in model selection.</p>
<p>The dataset is available in the <code>DALEX</code> package for R and <code>dalex</code> library for Python. It contains 1000 observations (apartments) and six variables:</p>
<ul>
<li><em>m2.price</em>, apartment’s price per square meter (in EUR), a numerical variable in the range of 1607–6595;</li>
<li><em>construction.year</em>, the year of construction of the block of flats in which the apartment is located, a numerical variable in the range of 1920–2010;</li>
<li><em>surface</em>, apartment’s total surface in square meters, a numerical variable in the range of 20–150;</li>
<li><em>floor</em>, the floor at which the apartment is located (ground floor taken to be the first floor), a numerical integer variable with values ranging from 1 to 10;</li>
<li><em>no.rooms</em>, the total number of rooms, a numerical integer variable with values ranging from 1 to 6;</li>
<li><em>district</em>, a factor with 10 levels indicating the district of Warsaw where the apartment is located.</li>
</ul>
<p>See an first six rows of this dataset in the Table below.</p>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
m2.price
</th>
<th style="text-align:right;">
construction.year
</th>
<th style="text-align:right;">
surface
</th>
<th style="text-align:right;">
floor
</th>
<th style="text-align:right;">
no.rooms
</th>
<th style="text-align:left;">
district
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
5897
</td>
<td style="text-align:right;">
1953
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Srodmiescie
</td>
</tr>
<tr>
<td style="text-align:right;">
1818
</td>
<td style="text-align:right;">
1992
</td>
<td style="text-align:right;">
143
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Bielany
</td>
</tr>
<tr>
<td style="text-align:right;">
3643
</td>
<td style="text-align:right;">
1937
</td>
<td style="text-align:right;">
56
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Praga
</td>
</tr>
<tr>
<td style="text-align:right;">
3517
</td>
<td style="text-align:right;">
1995
</td>
<td style="text-align:right;">
93
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Ochota
</td>
</tr>
<tr>
<td style="text-align:right;">
3013
</td>
<td style="text-align:right;">
1992
</td>
<td style="text-align:right;">
144
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Mokotow
</td>
</tr>
<tr>
<td style="text-align:right;">
5795
</td>
<td style="text-align:right;">
1926
</td>
<td style="text-align:right;">
61
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Srodmiescie
</td>
</tr>
</tbody>
</table>
<p>Models considered for this dataset will use <em>m2.price</em> as the (continuous) dependent variable. Models’ predictions will be validated on a set of 9000 apartments included in data frame <code>apartments_test</code>.</p>
<p>Note that usually, the testing data is smaller than the training data. In this example we deliberately use a small training set so that model selection is not too easy. After all, it’s just an use-case.</p>
<div id="exploration-apartments" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Data exploration</h3>
<p>Note that <code>apartments</code> is an artificial dataset created to illustrate and explain differences between random forest and linear regression. Hence, the structure of the data, the form and strength of association between variables, plausibility of distributional assumptions, etc., is better than in a real-life dataset. In fact, all these characteristics of the data are known. Nevertheless, we present some data exploration below to illustrate the important aspects of the data.</p>
<p>The variable of interest is <em>m2.price</em>, the price per square meter. The histogram presented in Figure <a href="dataSetsIntro.html#fig:apartmentsExplorationMi2">5.6</a> indicates that the distribution of the variable is slightly skewed to the right.</p>

<div class="figure" style="text-align: center"><span id="fig:apartmentsExplorationMi2"></span>
<img src="ema_files/figure-html/apartmentsExplorationMi2-1.png" alt="Distribution of the price per square meter in the apartment-prices data." width="400" />
<p class="caption">
Figure 5.6: Distribution of the price per square meter in the apartment-prices data.
</p>
</div>
<p>Figure <a href="dataSetsIntro.html#fig:apartmentsMi2Construction">5.7</a> suggests (possibly) a non-linear relationship between <em>construction.year</em> and <em>m2.price</em> and a linear relation between <em>surface</em> and <em>m2.price</em>.</p>

<div class="figure" style="text-align: center"><span id="fig:apartmentsMi2Construction"></span>
<img src="ema_files/figure-html/apartmentsMi2Construction-1.png" alt="Apartment-prices data. Price per square meter vs. year of construction (left-hand-side panel) and vs. surface (right-hand-side panel)." width="100%" />
<p class="caption">
Figure 5.7: Apartment-prices data. Price per square meter vs. year of construction (left-hand-side panel) and vs. surface (right-hand-side panel).
</p>
</div>
<p>Figure <a href="dataSetsIntro.html#fig:apartmentsMi2Floor">5.8</a> indicates that the relationship between <em>floor</em> and <em>m2.price</em> is also close to linear, as well as is the association between <em>no.rooms</em> and <em>m2.price</em> .</p>

<div class="figure" style="text-align: center"><span id="fig:apartmentsMi2Floor"></span>
<img src="ema_files/figure-html/apartmentsMi2Floor-1.png" alt="Apartment-prices data. Price per square meter vs. floor (left-hand-side panel) and vs. number of rooms (right-hand-side panel)." width="100%" />
<p class="caption">
Figure 5.8: Apartment-prices data. Price per square meter vs. floor (left-hand-side panel) and vs. number of rooms (right-hand-side panel).
</p>
</div>
<p>Figure <a href="dataSetsIntro.html#fig:apartmentsSurfaceNorooms">5.9</a> shows that <em>surface</em> and <em>number of rooms</em> are positively associatied and that prices depend on district. In particular, box plots in Figure <a href="dataSetsIntro.html#fig:apartmentsSurfaceNorooms">5.9</a> indicate that the highest prices per square meter are observed in Srodmiescie (Downtown).</p>

<div class="figure" style="text-align: center"><span id="fig:apartmentsSurfaceNorooms"></span>
<img src="ema_files/figure-html/apartmentsSurfaceNorooms-1.png" alt="Apartment-prices data. Surface vs. number of rooms (left-hand-side panel) and price per square meter for different districts (right-hand-side panel)." width="100%" />
<p class="caption">
Figure 5.9: Apartment-prices data. Surface vs. number of rooms (left-hand-side panel) and price per square meter for different districts (right-hand-side panel).
</p>
</div>
</div>
</div>
<div id="r-regression-model-for-apartment-prices" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> R regression model for Apartment prices</h2>
<div id="model-Apartments-lr" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Linear-regression model</h3>
<p>The dependent variable of interest, <em>m2.price</em>, is continuous. Thus, a natural choice to build a predictive model is linear regression. We treat all the other variables in the <code>apartments</code> data frame as explanatory and include them in the model. To fit the model, we apply the <code>lm()</code> function. The results of the model are stored in model-object <code>apartments_lm</code>.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="dataSetsIntro.html#cb43-1"></a>apartments_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(m2.price <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> apartments)</span>
<span id="cb43-2"><a href="dataSetsIntro.html#cb43-2"></a><span class="kw">anova</span>(apartments_lm)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: m2.price
##                    Df    Sum Sq   Mean Sq  F value    Pr(&gt;F)    
## construction.year   1   2629802   2629802   33.233 1.093e-08 ***
## surface             1 207840733 207840733 2626.541 &lt; 2.2e-16 ***
## floor               1  79823027  79823027 1008.746 &lt; 2.2e-16 ***
## no.rooms            1    956996    956996   12.094  0.000528 ***
## district            9 451993980  50221553  634.664 &lt; 2.2e-16 ***
## Residuals         986  78023123     79131                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="model-Apartments-rf" class="section level3" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Random-forest model</h3>
<p>As an alternative to linear regression, we consider a random-forest model. Again, we treat all the variables in the <code>apartments</code> data frame other than <em>m2.price</em> as explanatory and include them in the model. To fit the model, we apply the <code>randomForest()</code> function, with default settings, from the package with the same name <span class="citation">(Liaw and Wiener <a href="#ref-randomForest" role="doc-biblioref">2002</a>)</span>. The results of the model are stored in model-object <code>apartments_rf</code>.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="dataSetsIntro.html#cb45-1"></a><span class="kw">library</span>(<span class="st">&quot;randomForest&quot;</span>)</span>
<span id="cb45-2"><a href="dataSetsIntro.html#cb45-2"></a><span class="kw">set.seed</span>(<span class="dv">72</span>)</span>
<span id="cb45-3"><a href="dataSetsIntro.html#cb45-3"></a>apartments_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(m2.price <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> apartments)</span>
<span id="cb45-4"><a href="dataSetsIntro.html#cb45-4"></a>apartments_rf</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = m2.price ~ ., data = apartments) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 1
## 
##           Mean of squared residuals: 79789.39
##                     % Var explained: 90.28</code></pre>
</div>
<div id="model-Apartments-svm" class="section level3" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> Support Vector Machine model for Regression</h3>
<p>Finally, we consider a support vector machine model, with all the variables in the <code>apartments</code> data frame other than <em>m2.price</em> treated as explanatory. To fit the model, we use the <code>svm()</code> function, with default settings, from package <code>e1071</code> <span class="citation">(Meyer et al. <a href="#ref-R-e1071" role="doc-biblioref">2017</a>)</span>. The results of the model are stored in model-object <code>apartments_svm</code>.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="dataSetsIntro.html#cb47-1"></a><span class="kw">library</span>(<span class="st">&quot;e1071&quot;</span>)</span>
<span id="cb47-2"><a href="dataSetsIntro.html#cb47-2"></a>apartments_svm &lt;-<span class="st"> </span><span class="kw">svm</span>(m2.price <span class="op">~</span><span class="st"> </span>construction.year <span class="op">+</span><span class="st"> </span>surface <span class="op">+</span><span class="st"> </span>floor <span class="op">+</span><span class="st"> </span></span>
<span id="cb47-3"><a href="dataSetsIntro.html#cb47-3"></a><span class="st">         </span>no.rooms <span class="op">+</span><span class="st"> </span>district, <span class="dt">data =</span> apartments)</span>
<span id="cb47-4"><a href="dataSetsIntro.html#cb47-4"></a>apartments_svm</span></code></pre></div>
<pre><code>## 
## Call:
## svm(formula = m2.price ~ construction.year + surface + floor + no.rooms + 
##     district, data = apartments)
## 
## 
## Parameters:
##    SVM-Type:  eps-regression 
##  SVM-Kernel:  radial 
##        cost:  1 
##       gamma:  0.07142857 
##     epsilon:  0.1 
## 
## 
## Number of Support Vectors:  536</code></pre>
</div>
<div id="predictionsApartments" class="section level3" number="5.5.4">
<h3><span class="header-section-number">5.5.4</span> Models’ predictions</h3>
<p>The <code>predict()</code> function calculates predictions for a specific model. In the example below, we use model-objects <code>apartments_lm</code>, <code>apartments_rf</code>, and <code>apartments_svm</code>, to calculate predictions for prices of the apartments from the <code>apartments_test</code> data frame. Note that, for brevity sake, we compute the predictions only for the first six observations from the data frame.</p>
<p>Actual prices for the six observations from <code>apartments_test</code>.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="dataSetsIntro.html#cb49-1"></a>apartments_test<span class="op">$</span>m2.price[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>]</span></code></pre></div>
<pre><code>## [1] 4644 3082 2498 2735 2781 2936</code></pre>
<p>Predictions with linear regression.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="dataSetsIntro.html#cb51-1"></a><span class="kw">predict</span>(apartments_lm, apartments_test[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,])</span></code></pre></div>
<pre><code>##     1001     1002     1003     1004     1005     1006 
## 4820.009 3292.678 2717.910 2922.751 2974.086 2527.043</code></pre>
<p>Predictions with random-forest model.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="dataSetsIntro.html#cb53-1"></a><span class="kw">predict</span>(apartments_rf, apartments_test[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,])</span></code></pre></div>
<pre><code>##     1001     1002     1003     1004     1005     1006 
## 4214.084 3178.061 2695.787 2744.775 2951.069 2999.450</code></pre>
<p>Predictions with support vector model.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="dataSetsIntro.html#cb55-1"></a><span class="kw">predict</span>(apartments_svm, apartments_test[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>,])</span></code></pre></div>
<pre><code>##     1001     1002     1003     1004     1005     1006 
## 4590.076 3012.044 2369.748 2712.456 2681.777 2750.904</code></pre>
<p>By using the code below, we summarize the predictive performance of the linear-regression and random-forest models by computing the square root of the mean-squared-error (RMSE). For a “perfect” predictive model, which would predict all observations exactly, RMSE should be equal to 0. More information about RMSE can be found in Section <a href="modelPerformance.html#modelPerformanceMethodCont">16.3.1</a>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="dataSetsIntro.html#cb57-1"></a>predicted_apartments_lm &lt;-<span class="st"> </span><span class="kw">predict</span>(apartments_lm, apartments_test)</span>
<span id="cb57-2"><a href="dataSetsIntro.html#cb57-2"></a>(rmsd_lm &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((predicted_apartments_lm <span class="op">-</span><span class="st"> </span>apartments_test<span class="op">$</span>m2.price)<span class="op">^</span><span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## [1] 283.0865</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="dataSetsIntro.html#cb59-1"></a>predicted_apartments_rf &lt;-<span class="st"> </span><span class="kw">predict</span>(apartments_rf, apartments_test)</span>
<span id="cb59-2"><a href="dataSetsIntro.html#cb59-2"></a>(rmsd_rf &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((predicted_apartments_rf <span class="op">-</span><span class="st"> </span>apartments_test<span class="op">$</span>m2.price)<span class="op">^</span><span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## [1] 282.9519</code></pre>
<p>For the random-forest model, RMSE is equal to 283. It is almost identical to the RMSE for the linear-regression model, which is equal to 283.1. Thus, the question we may face is: should we choose the more complex, but flexible random-forest model, or the simpler and easier to interpret linear-regression model? In the subsequent chapters we will try to provide an answer to this question. In particular, we will show that a proper model exploration may help to discover weak and strong sides of any of the models and, in consequence, allow creation of a new model, with a better performance than either of the two.</p>
</div>
<div id="ExplainersApartmentsRCode" class="section level3" number="5.5.5">
<h3><span class="header-section-number">5.5.5</span> Models’ explainers</h3>
<p>The code below creates explainers for the models (see Sections <a href="dataSetsIntro.html#model-Apartments-lr">5.5.1</a>–<a href="dataSetsIntro.html#model-Apartments-svm">5.5.3</a>) fitted to the apartment-prices data. Note that we use the <code>apartments_test</code> data frame without the first column, i.e., the <em>m2.price</em> variable, in the <code>data</code> argument. This will be the dataset to which the model will be applied (see Section <a href="dataSetsIntro.html#ExplainersTitanicRCode">5.2.6</a>). The <em>m2.price</em> variable is explicitly specified as the dependent variable in the <code>y</code> argument (see Section <a href="dataSetsIntro.html#ExplainersTitanicRCode">5.2.6</a>).</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="dataSetsIntro.html#cb61-1"></a>apartments_lm_exp &lt;-<span class="st"> </span><span class="kw">explain</span>(<span class="dt">model =</span> apartments_lm, </span>
<span id="cb61-2"><a href="dataSetsIntro.html#cb61-2"></a>                             <span class="dt">data =</span> apartments_test[,<span class="op">-</span><span class="dv">1</span>], </span>
<span id="cb61-3"><a href="dataSetsIntro.html#cb61-3"></a>                             <span class="dt">y =</span> apartments_test<span class="op">$</span>m2.price, </span>
<span id="cb61-4"><a href="dataSetsIntro.html#cb61-4"></a>                             <span class="dt">label =</span> <span class="st">&quot;Linear Regression&quot;</span>)</span>
<span id="cb61-5"><a href="dataSetsIntro.html#cb61-5"></a></span>
<span id="cb61-6"><a href="dataSetsIntro.html#cb61-6"></a>apartments_rf_exp &lt;-<span class="st"> </span><span class="kw">explain</span>(<span class="dt">model =</span> apartments_rf, </span>
<span id="cb61-7"><a href="dataSetsIntro.html#cb61-7"></a>                             <span class="dt">data =</span> apartments_test[,<span class="op">-</span><span class="dv">1</span>], </span>
<span id="cb61-8"><a href="dataSetsIntro.html#cb61-8"></a>                             <span class="dt">y =</span> apartments_test<span class="op">$</span>m2.price, </span>
<span id="cb61-9"><a href="dataSetsIntro.html#cb61-9"></a>                             <span class="dt">label =</span> <span class="st">&quot;Random Forest&quot;</span>)</span>
<span id="cb61-10"><a href="dataSetsIntro.html#cb61-10"></a></span>
<span id="cb61-11"><a href="dataSetsIntro.html#cb61-11"></a>apartments_svm_exp &lt;-<span class="st"> </span><span class="kw">explain</span>(<span class="dt">model =</span> apartments_svm, </span>
<span id="cb61-12"><a href="dataSetsIntro.html#cb61-12"></a>                              <span class="dt">data =</span> apartments_test[,<span class="op">-</span><span class="dv">1</span>], </span>
<span id="cb61-13"><a href="dataSetsIntro.html#cb61-13"></a>                              <span class="dt">y =</span> apartments_test<span class="op">$</span>m2.price, </span>
<span id="cb61-14"><a href="dataSetsIntro.html#cb61-14"></a>                              <span class="dt">label =</span> <span class="st">&quot;Support Vector Machine&quot;</span>)</span></code></pre></div>
</div>
<div id="ListOfModelsApartments" class="section level3" number="5.5.6">
<h3><span class="header-section-number">5.5.6</span> List of objects for the Apartment prices example</h3>
<p>In Sections <a href="dataSetsIntro.html#model-Apartments-lr">5.5.1</a>–<a href="dataSetsIntro.html#model-Apartments-svm">5.5.3</a>, we have built three predictive models for the <code>apartments</code> dataset. The models will be used in the rest of the book to illustrate the model-explanation methods and tools.</p>
<p>For the ease of reference, we summarize the models in Table <a href="dataSetsIntro.html#tab:archivistHooksOfModelsApartments">5.3</a>. The binary model-objects can be downloaded by using the indicated <code>archivist</code> hooks <span class="citation">(Biecek and Kosinski <a href="#ref-archivist" role="doc-biblioref">2017</a>)</span>. By calling a function specified in the last column of the table, one can restore a selected model in a local R environment.</p>
<table style="width:100%;">
<caption><span id="tab:archivistHooksOfModelsApartments">Table 5.3: </span> Predictive models created for the dataset Apartment prices.</caption>
<colgroup>
<col width="23%" />
<col width="28%" />
<col width="20%" />
<col width="28%" />
</colgroup>
<thead>
<tr class="header">
<th>Model name</th>
<th>Model generator</th>
<th>Variables</th>
<th>Archivist hooks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>apartments_lm</code></td>
<td><code>stats:: lm</code> v.3.5.3</td>
<td>construction .year, surface, floor, no.rooms, district</td>
<td>Get the model: <code>archivist:: aread("pbiecek/models/55f19")</code>.</td>
</tr>
<tr class="even">
<td><code>apartments_rf</code></td>
<td><code>randomForest:: randomForest</code> v.4.6.14</td>
<td>construction .year, surface, floor, no.rooms, district</td>
<td>Get the model: <code>archivist:: aread("pbiecek/models/fe7a5")</code>.</td>
</tr>
<tr class="odd">
<td><code>apartments_svm</code></td>
<td><code>e1071:: svm</code> v.1.7.3</td>
<td>construction .year, surface, floor, no.rooms, district</td>
<td>Get the model: <code>archivist:: aread("pbiecek/models/d2ca0")</code>.</td>
</tr>
</tbody>
</table>
<!--
Get the explainer: `archivist:: aread("pbiecek/models/78d4e")`
Get the explainer: `archivist:: aread("pbiecek/models/b1739")`
Get the explainer: `archivist:: aread("pbiecek/models/16602")`
-->
</div>
</div>
<div id="python-regression-models-for-apartment-prices" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Python regression models for Apartment prices</h2>
<p>Examples in Python are based on the <code>apartmets</code> data set, which is available in the <code>dalex</code> library. The <code>m2_price</code> column is a target variable, the remaining columns will be used to construct the predictive model.</p>
<p>The following instructions load the <code>apartmets</code> dataset and split it into the target variable <code>y</code> and the predictive variables <code>X</code>.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="dataSetsIntro.html#cb62-1"></a><span class="im">import</span> dalex <span class="im">as</span> dx</span>
<span id="cb62-2"><a href="dataSetsIntro.html#cb62-2"></a>apartmets <span class="op">=</span> dx.datasets.load_apartmets()</span>
<span id="cb62-3"><a href="dataSetsIntro.html#cb62-3"></a>X <span class="op">=</span> apartmets.drop(columns<span class="op">=</span><span class="st">&#39;m2_price&#39;</span>)</span>
<span id="cb62-4"><a href="dataSetsIntro.html#cb62-4"></a>y <span class="op">=</span> apartmets.m2_price</span></code></pre></div>
<p>Data <code>X</code> contains numeric variables with different ranges (e.g. surface and no.rooms) and categorical variables (i.e. district). Machine Learning algorithms in <code>sklearn</code> require the data to be preprocessed into numeric form. Therefore, before modeling, we prepared a pipeline that performs data preprocessing. That is scaling for continuous variables (<em>construction.year</em>, <em>surface</em>, <em>floor</em>, <em>no.rooms</em>) and one-hot-encoding for categorical variables (<em>district</em>).</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="dataSetsIntro.html#cb63-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, OneHotEncoder</span>
<span id="cb63-2"><a href="dataSetsIntro.html#cb63-2"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> make_column_transformer</span>
<span id="cb63-3"><a href="dataSetsIntro.html#cb63-3"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb63-4"><a href="dataSetsIntro.html#cb63-4"></a></span>
<span id="cb63-5"><a href="dataSetsIntro.html#cb63-5"></a>preprocess <span class="op">=</span> make_column_transformer(</span>
<span id="cb63-6"><a href="dataSetsIntro.html#cb63-6"></a>    (StandardScaler(), [<span class="st">&#39;construction.year&#39;</span>, <span class="st">&#39;surface&#39;</span>, <span class="st">&#39;floor&#39;</span>, <span class="st">&#39;no.rooms&#39;</span>]),</span>
<span id="cb63-7"><a href="dataSetsIntro.html#cb63-7"></a>    (OneHotEncoder(), [<span class="st">&#39;district&#39;</span>]))</span></code></pre></div>
<div id="model-Apartments-python-lr" class="section level3" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Linear-regression model</h3>
<p>The dependent variable of interest, <em>m2.price</em>, is continuous. Thus, a natural choice to build a predictive model is linear regression. We treat all the other variables in the <code>apartments</code> data frame as explanatory and include them in the model. Here we use the <code>LinearRegression</code> algorithm from <code>sklearn</code> library.</p>
<p>The result is a model stored in object <code>apartments_lm</code>, which will be used in subsequent chapters.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="dataSetsIntro.html#cb64-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb64-2"><a href="dataSetsIntro.html#cb64-2"></a></span>
<span id="cb64-3"><a href="dataSetsIntro.html#cb64-3"></a>apartments_lm <span class="op">=</span> make_pipeline(</span>
<span id="cb64-4"><a href="dataSetsIntro.html#cb64-4"></a>    preprocess,</span>
<span id="cb64-5"><a href="dataSetsIntro.html#cb64-5"></a>    LinearRegression())</span>
<span id="cb64-6"><a href="dataSetsIntro.html#cb64-6"></a>    </span>
<span id="cb64-7"><a href="dataSetsIntro.html#cb64-7"></a>apartments_lm.fit(X, y)</span></code></pre></div>
</div>
<div id="model-Apartments-python-rf" class="section level3" number="5.6.2">
<h3><span class="header-section-number">5.6.2</span> Random-forest model</h3>
<p>As an alternative to linear regression, we consider a random-forest model. Again, we treat all the variables in the <code>apartments</code> data frame other than <em>m2.price</em> as explanatory and include them in the model.</p>
<p>To fit the model, we use <code>RandomForestRegressor</code> algorithm from <code>sklearn</code> library. We use the default settings with tree not deeper than 3 levels, and the number of trees in the random forest is set to 500.</p>
<p>The result is a model stored in object <code>apartments_rf</code>, which will be used in subsequent chapters.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="dataSetsIntro.html#cb65-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb65-2"><a href="dataSetsIntro.html#cb65-2"></a></span>
<span id="cb65-3"><a href="dataSetsIntro.html#cb65-3"></a>apartments_rf <span class="op">=</span> make_pipeline(</span>
<span id="cb65-4"><a href="dataSetsIntro.html#cb65-4"></a>    preprocess,</span>
<span id="cb65-5"><a href="dataSetsIntro.html#cb65-5"></a>    RandomForestClassifier(max_depth<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">0</span>, n_estimators<span class="op">=</span><span class="dv">500</span>))</span>
<span id="cb65-6"><a href="dataSetsIntro.html#cb65-6"></a>    </span>
<span id="cb65-7"><a href="dataSetsIntro.html#cb65-7"></a>apartments_rf.fit(X, y)</span></code></pre></div>
</div>
<div id="model-Apartments-python-svm" class="section level3" number="5.6.3">
<h3><span class="header-section-number">5.6.3</span> Support Vector Machine model for Regression</h3>
<p>Finally, we also consider a support vector machine model <span class="citation">(Cortes and Vapnik <a href="#ref-svm95vapnik" role="doc-biblioref">1995</a>)</span>. We use the Support Vector for Regression with linear kernel.
To fit the model, we use <code>SVR</code> algorithm from <code>sklearn</code> library.</p>
<p>The result is a model stored in object <code>apartments_svm</code>, which will be used in subsequent chapters.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="dataSetsIntro.html#cb66-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVR</span>
<span id="cb66-2"><a href="dataSetsIntro.html#cb66-2"></a></span>
<span id="cb66-3"><a href="dataSetsIntro.html#cb66-3"></a>apartments_svm <span class="op">=</span> make_pipeline(</span>
<span id="cb66-4"><a href="dataSetsIntro.html#cb66-4"></a>    preprocess,</span>
<span id="cb66-5"><a href="dataSetsIntro.html#cb66-5"></a>    SVR())</span>
<span id="cb66-6"><a href="dataSetsIntro.html#cb66-6"></a>    </span>
<span id="cb66-7"><a href="dataSetsIntro.html#cb66-7"></a>apartments_svm.fit(X, y)</span></code></pre></div>
</div>
<div id="predictions-apartments-python" class="section level3" number="5.6.4">
<h3><span class="header-section-number">5.6.4</span> Models’ predictions</h3>
<p>Let us now compare predictions that are obtained from the different models. Subsequently, we use the method <code>predict()</code> to obtain the predicted price of square metter for the linear regression model.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="dataSetsIntro.html#cb67-1"></a>apartments_lr.predict(apartments_test)</span></code></pre></div>
<p>We do the same for the remaining two models.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="dataSetsIntro.html#cb68-1"></a>apartments_rf.predict(apartments_test)</span>
<span id="cb68-2"><a href="dataSetsIntro.html#cb68-2"></a></span>
<span id="cb68-3"><a href="dataSetsIntro.html#cb68-3"></a>apartments_svm.predict(apartments_test)</span></code></pre></div>
</div>
<div id="ExplainersApartmentsPythonCode" class="section level3" number="5.6.5">
<h3><span class="header-section-number">5.6.5</span> Models’ explainers</h3>
<p>The examples that we show in this chapter are based on the <code>sklearn</code> library, which makes it possible to work with models in a uniform way. But often we also want to work with models built in other libraries. To make it easier to work models with different structures, the <code>dalex</code> library wraps models in the objects of the class <code>Explainer</code>, that have all the necessary functions of the model available in a uniform way. More detailed description is given in Section <a href="dataSetsIntro.html#ExplainersTitanicPythonCode">5.3.6</a>.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="dataSetsIntro.html#cb69-1"></a>apartments_lr_exp <span class="op">=</span> dx.Explainer(apartments_lr, X, y, label <span class="op">=</span> <span class="st">&quot;Apartments LR Pipeline&quot;</span>)</span>
<span id="cb69-2"><a href="dataSetsIntro.html#cb69-2"></a></span>
<span id="cb69-3"><a href="dataSetsIntro.html#cb69-3"></a>apartments_rf_exp <span class="op">=</span> dx.Explainer(apartments_rf, X, y, label <span class="op">=</span> <span class="st">&quot;Apartments RF Pipeline&quot;</span>)</span>
<span id="cb69-4"><a href="dataSetsIntro.html#cb69-4"></a></span>
<span id="cb69-5"><a href="dataSetsIntro.html#cb69-5"></a>apartments_svm_exp <span class="op">=</span> dx.Explainer(apartments_svm, X, y, label <span class="op">=</span> <span class="st">&quot;Apartments SVM Pipeline&quot;</span>)</span></code></pre></div>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-DALEX">
<p>Biecek, Przemyslaw. 2018. <em>DALEX: Explainers for Complex Predictive Models in R</em>. <em>Journal of Machine Learning Research</em>. Vol. 19. <a href="http://jmlr.org/papers/v19/18-416.html">http://jmlr.org/papers/v19/18-416.html</a>.</p>
</div>
<div id="ref-archivist">
<p>Biecek, Przemyslaw, and Marcin Kosinski. 2017. “archivist: An R Package for Managing, Recording and Restoring Data Analysis Results.” <em>Journal of Statistical Software</em> 82 (11): 1–28. <a href="https://doi.org/10.18637/jss.v082.i11">https://doi.org/10.18637/jss.v082.i11</a>.</p>
</div>
<div id="ref-randomForestBreiman">
<p>Breiman, Leo. 2001a. “Random Forests.” In <em>Machine Learning</em>, 45:5–32. <a href="https://doi.org/10.1023/a:1010933404324">https://doi.org/10.1023/a:1010933404324</a>.</p>
</div>
<div id="ref-vanBuuren2012">
<p>Buuren, S. van. 2012. <em>Flexible Imputation of Missing Data</em>. Boca Raton, FL: Chapman &amp; Hall/CRC.</p>
</div>
<div id="ref-svm95vapnik">
<p>Cortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” In <em>Machine Learning</em>, 273–97.</p>
</div>
<div id="ref-Friedman00greedyfunction">
<p>Friedman, Jerome H. 2000. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>Annals of Statistics</em> 29: 1189–1232.</p>
</div>
<div id="ref-rms">
<p>Harrell Jr, Frank E. 2018. <em>Rms: Regression Modeling Strategies</em>. <a href="https://CRAN.R-project.org/package=rms">https://CRAN.R-project.org/package=rms</a>.</p>
</div>
<div id="ref-randomForest">
<p>Liaw, Andy, and Matthew Wiener. 2002. “Classification and Regression by randomForest.” <em>R News</em> 2 (3): 18–22. <a href="http://CRAN.R-project.org/doc/Rnews/">http://CRAN.R-project.org/doc/Rnews/</a>.</p>
</div>
<div id="ref-LittleRubin2002">
<p>Little, R. J. A., and D. B. Rubin. 2002. <em>Statistical Analysis with Missing Data (2nd Ed.)</em>. Hoboken, NJ: Wiley.</p>
</div>
<div id="ref-R-e1071">
<p>Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2017. <em>E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), Tu Wien</em>. <a href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a>.</p>
</div>
<div id="ref-e1071">
<p>Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2019. <em>E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), Tu Wien</em>. <a href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a>.</p>
</div>
<div id="ref-MolKen2007">
<p>Molenberghs, G., and M. G. Kenward. 2007. <em>Missing Data in Clinical Studies</em>. Chichester, England: Wiley.</p>
</div>
<div id="ref-gbm">
<p>Ridgeway, Greg. 2017. <em>Gbm: Generalized Boosted Regression Models</em>. <a href="https://CRAN.R-project.org/package=gbm">https://CRAN.R-project.org/package=gbm</a>.</p>
</div>
<div id="ref-Schafer1997">
<p>Schafer, J. L. 1997. <em>Analysis of Incomplete Multivariate Data</em>. Boca Raton, FL: Chapman &amp; Hall/CRC.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="doItYourselfWithPython.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="InstanceLevelExploration.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ema.pdf", "ema.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
