<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Local Interpretable Model-agnostic Explanations (LIME) | Explanatory Model Analysis</title>
  <meta name="description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Local Interpretable Model-agnostic Explanations (LIME) | Explanatory Model Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="figure/front4.png" />
  <meta property="og:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="github-repo" content="pbiecek/ema" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Local Interpretable Model-agnostic Explanations (LIME) | Explanatory Model Analysis" />
  
  <meta name="twitter:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="twitter:image" content="figure/front4.png" />

<meta name="author" content="Przemyslaw Biecek and Tomasz Burzykowski" />


<meta name="date" content="2021-03-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shapley.html"/>
<link rel="next" href="ceterisParibus.html"/>
<script src="libs/header-attrs-2.4/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-5650686-15', 'https://pbiecek.github.io/ema/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>
<style>
.figure {
   padding:40px 0px;
}
</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><span style="font-size: large">Explanatory Model Analysis</span><br/>Explore, Explain, and Examine<br/>Predictive Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#the-aim-of-the-book"><i class="fa fa-check"></i><b>1.1</b> The aim of the book</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#three-single-laws"><i class="fa fa-check"></i><b>1.2</b> A bit of philosophy: three laws of model explanation</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#teminology"><i class="fa fa-check"></i><b>1.3</b> Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#glassblack"><i class="fa fa-check"></i><b>1.4</b> Black-box models and glass-box models</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#agnosticspecific"><i class="fa fa-check"></i><b>1.5</b> Model-agnostic and model-specific approach</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#bookstructure"><i class="fa fa-check"></i><b>1.6</b> The structure of the book</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#whatisinthebook"><i class="fa fa-check"></i><b>1.7</b> What is included in this book and what is not</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#thanksto"><i class="fa fa-check"></i><b>1.8</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html"><i class="fa fa-check"></i><b>2</b> Model Development</a>
<ul>
<li class="chapter" data-level="2.1" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPIntro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPprocess"><i class="fa fa-check"></i><b>2.2</b> Model-development process</a></li>
<li class="chapter" data-level="2.3" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#notation"><i class="fa fa-check"></i><b>2.3</b> Notation</a></li>
<li class="chapter" data-level="2.4" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#dataunderstanding"><i class="fa fa-check"></i><b>2.4</b> Data understanding</a></li>
<li class="chapter" data-level="2.5" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#fitting"><i class="fa fa-check"></i><b>2.5</b> Model assembly (fitting)</a></li>
<li class="chapter" data-level="2.6" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#validation"><i class="fa fa-check"></i><b>2.6</b> Model audit</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="do-it-yourself.html"><a href="do-it-yourself.html"><i class="fa fa-check"></i><b>3</b> Do-it-yourself</a>
<ul>
<li class="chapter" data-level="3.1" data-path="do-it-yourself.html"><a href="do-it-yourself.html#doItYourselfWithR"><i class="fa fa-check"></i><b>3.1</b> Do-it-yourself with R</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="do-it-yourself.html"><a href="do-it-yourself.html#what-to-install"><i class="fa fa-check"></i><b>3.1.1</b> What to install?</a></li>
<li class="chapter" data-level="3.1.2" data-path="do-it-yourself.html"><a href="do-it-yourself.html#infoDALEX"><i class="fa fa-check"></i><b>3.1.2</b> How to work with <code>DALEX</code>?</a></li>
<li class="chapter" data-level="3.1.3" data-path="do-it-yourself.html"><a href="do-it-yourself.html#how-to-work-with-archivist"><i class="fa fa-check"></i><b>3.1.3</b> How to work with <code>archivist</code>?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="do-it-yourself.html"><a href="do-it-yourself.html#doItYourselfWithPython"><i class="fa fa-check"></i><b>3.2</b> Do-it-yourself with Python</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="do-it-yourself.html"><a href="do-it-yourself.html#what-to-install-1"><i class="fa fa-check"></i><b>3.2.1</b> What to install?</a></li>
<li class="chapter" data-level="3.2.2" data-path="do-it-yourself.html"><a href="do-it-yourself.html#infoDALEXpy"><i class="fa fa-check"></i><b>3.2.2</b> How to work with <code>dalex</code>?</a></li>
<li class="chapter" data-level="3.2.3" data-path="do-it-yourself.html"><a href="do-it-yourself.html#code-snippets-for-python"><i class="fa fa-check"></i><b>3.2.3</b> Code snippets for Python</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html"><i class="fa fa-check"></i><b>4</b> Datasets and Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#TitanicDataset"><i class="fa fa-check"></i><b>4.1</b> Sinking of the RMS Titanic</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-titanic"><i class="fa fa-check"></i><b>4.1.1</b> Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#models-for-rms-titanic-snippets-for-r"><i class="fa fa-check"></i><b>4.2</b> Models for RMS Titanic, snippets for R</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-lmr"><i class="fa fa-check"></i><b>4.2.1</b> Logistic regression model</a></li>
<li class="chapter" data-level="4.2.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-rf"><i class="fa fa-check"></i><b>4.2.2</b> Random forest model</a></li>
<li class="chapter" data-level="4.2.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-gbm"><i class="fa fa-check"></i><b>4.2.3</b> Gradient boosting model</a></li>
<li class="chapter" data-level="4.2.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-svm"><i class="fa fa-check"></i><b>4.2.4</b> Support vector machine model</a></li>
<li class="chapter" data-level="4.2.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic"><i class="fa fa-check"></i><b>4.2.5</b> Models’ predictions</a></li>
<li class="chapter" data-level="4.2.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicRCode"><i class="fa fa-check"></i><b>4.2.6</b> Models’ explainers</a></li>
<li class="chapter" data-level="4.2.7" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsTitanic"><i class="fa fa-check"></i><b>4.2.7</b> List of model-objects</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#models-for-rms-titanic-snippets-for-python"><i class="fa fa-check"></i><b>4.3</b> Models for RMS Titanic, snippets for Python</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-lr"><i class="fa fa-check"></i><b>4.3.1</b> Logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-rf"><i class="fa fa-check"></i><b>4.3.2</b> Random forest model</a></li>
<li class="chapter" data-level="4.3.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-gbm"><i class="fa fa-check"></i><b>4.3.3</b> Gradient boosting model</a></li>
<li class="chapter" data-level="4.3.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-svm"><i class="fa fa-check"></i><b>4.3.4</b> Support vector machine model</a></li>
<li class="chapter" data-level="4.3.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic-python"><i class="fa fa-check"></i><b>4.3.5</b> Models’ predictions</a></li>
<li class="chapter" data-level="4.3.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicPythonCode"><i class="fa fa-check"></i><b>4.3.6</b> Models’ explainers</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ApartmentDataset"><i class="fa fa-check"></i><b>4.4</b> Apartment prices</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-apartments"><i class="fa fa-check"></i><b>4.4.1</b> Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#models-for-apartment-prices-snippets-for-r"><i class="fa fa-check"></i><b>4.5</b> Models for apartment prices, snippets for R</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-lr"><i class="fa fa-check"></i><b>4.5.1</b> Linear regression model</a></li>
<li class="chapter" data-level="4.5.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-rf"><i class="fa fa-check"></i><b>4.5.2</b> Random forest model</a></li>
<li class="chapter" data-level="4.5.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-svm"><i class="fa fa-check"></i><b>4.5.3</b> Support vector machine model</a></li>
<li class="chapter" data-level="4.5.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictionsApartments"><i class="fa fa-check"></i><b>4.5.4</b> Models’ predictions</a></li>
<li class="chapter" data-level="4.5.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsRCode"><i class="fa fa-check"></i><b>4.5.5</b> Models’ explainers</a></li>
<li class="chapter" data-level="4.5.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsApartments"><i class="fa fa-check"></i><b>4.5.6</b> List of model-objects</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#models-for-apartment-prices-snippets-for-python"><i class="fa fa-check"></i><b>4.6</b> Models for apartment prices, snippets for Python</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-lr"><i class="fa fa-check"></i><b>4.6.1</b> Linear regression model</a></li>
<li class="chapter" data-level="4.6.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-rf"><i class="fa fa-check"></i><b>4.6.2</b> Random forest model</a></li>
<li class="chapter" data-level="4.6.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-svm"><i class="fa fa-check"></i><b>4.6.3</b> Support vector machine model</a></li>
<li class="chapter" data-level="4.6.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-apartments-python"><i class="fa fa-check"></i><b>4.6.4</b> Models’ predictions</a></li>
<li class="chapter" data-level="4.6.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsPythonCode"><i class="fa fa-check"></i><b>4.6.5</b> Models’ explainers</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Instance Level</b></span></li>
<li class="chapter" data-level="5" data-path="InstanceLevelExploration.html"><a href="InstanceLevelExploration.html"><i class="fa fa-check"></i><b>5</b> Introduction to Instance-level Exploration</a></li>
<li class="chapter" data-level="6" data-path="breakDown.html"><a href="breakDown.html"><i class="fa fa-check"></i><b>6</b> Break-down Plots for Additive Attributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="breakDown.html"><a href="breakDown.html#BDIntroduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="breakDown.html"><a href="breakDown.html#BDIntuition"><i class="fa fa-check"></i><b>6.2</b> Intuition</a></li>
<li class="chapter" data-level="6.3" data-path="breakDown.html"><a href="breakDown.html#BDMethod"><i class="fa fa-check"></i><b>6.3</b> Method</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="breakDown.html"><a href="breakDown.html#BDMethodLin"><i class="fa fa-check"></i><b>6.3.1</b> Break-down for linear models</a></li>
<li class="chapter" data-level="6.3.2" data-path="breakDown.html"><a href="breakDown.html#BDMethodGen"><i class="fa fa-check"></i><b>6.3.2</b> Break-down for a general case</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="breakDown.html"><a href="breakDown.html#BDExample"><i class="fa fa-check"></i><b>6.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="6.5" data-path="breakDown.html"><a href="breakDown.html#BDProsCons"><i class="fa fa-check"></i><b>6.5</b> Pros and cons</a></li>
<li class="chapter" data-level="6.6" data-path="breakDown.html"><a href="breakDown.html#BDR"><i class="fa fa-check"></i><b>6.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="breakDown.html"><a href="breakDown.html#basic-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>6.6.1</b> Basic use of the <code>predict_parts()</code> function</a></li>
<li class="chapter" data-level="6.6.2" data-path="breakDown.html"><a href="breakDown.html#advanced-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>6.6.2</b> Advanced use of the <code>predict_parts()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="breakDown.html"><a href="breakDown.html#BDPython"><i class="fa fa-check"></i><b>6.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="iBreakDown.html"><a href="iBreakDown.html"><i class="fa fa-check"></i><b>7</b> Break-down Plots for Interactions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDIntuition"><i class="fa fa-check"></i><b>7.1</b> Intuition</a></li>
<li class="chapter" data-level="7.2" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDMethod"><i class="fa fa-check"></i><b>7.2</b> Method</a></li>
<li class="chapter" data-level="7.3" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDExample"><i class="fa fa-check"></i><b>7.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="7.4" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDProsCons"><i class="fa fa-check"></i><b>7.4</b> Pros and cons</a></li>
<li class="chapter" data-level="7.5" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDRcode"><i class="fa fa-check"></i><b>7.5</b> Code snippets for R</a></li>
<li class="chapter" data-level="7.6" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDPythonCode"><i class="fa fa-check"></i><b>7.6</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>8</b> Shapley Additive Explanations (SHAP) for Average Attributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="shapley.html"><a href="shapley.html#SHAPIntuition"><i class="fa fa-check"></i><b>8.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2" data-path="shapley.html"><a href="shapley.html#SHAPMethod"><i class="fa fa-check"></i><b>8.2</b> Method</a></li>
<li class="chapter" data-level="8.3" data-path="shapley.html"><a href="shapley.html#SHAPExample"><i class="fa fa-check"></i><b>8.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="8.4" data-path="shapley.html"><a href="shapley.html#SHAProsCons"><i class="fa fa-check"></i><b>8.4</b> Pros and cons</a></li>
<li class="chapter" data-level="8.5" data-path="shapley.html"><a href="shapley.html#SHAPRcode"><i class="fa fa-check"></i><b>8.5</b> Code snippets for R</a></li>
<li class="chapter" data-level="8.6" data-path="shapley.html"><a href="shapley.html#SHAPPythonCode"><i class="fa fa-check"></i><b>8.6</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="LIME.html"><a href="LIME.html"><i class="fa fa-check"></i><b>9</b> Local Interpretable Model-agnostic Explanations (LIME)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="LIME.html"><a href="LIME.html#LIMEIntroduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="LIME.html"><a href="LIME.html#LIMEIntuition"><i class="fa fa-check"></i><b>9.2</b> Intuition</a></li>
<li class="chapter" data-level="9.3" data-path="LIME.html"><a href="LIME.html#LIMEMethod"><i class="fa fa-check"></i><b>9.3</b> Method</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="LIME.html"><a href="LIME.html#LIMErepr"><i class="fa fa-check"></i><b>9.3.1</b> Interpretable data representation</a></li>
<li class="chapter" data-level="9.3.2" data-path="LIME.html"><a href="LIME.html#LIMEsample"><i class="fa fa-check"></i><b>9.3.2</b> Sampling around the instance of interest</a></li>
<li class="chapter" data-level="9.3.3" data-path="LIME.html"><a href="LIME.html#LIMEglas"><i class="fa fa-check"></i><b>9.3.3</b> Fitting the glass-box model</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="LIME.html"><a href="LIME.html#LIMEExample"><i class="fa fa-check"></i><b>9.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="9.5" data-path="LIME.html"><a href="LIME.html#LIMEProsCons"><i class="fa fa-check"></i><b>9.5</b> Pros and cons</a></li>
<li class="chapter" data-level="9.6" data-path="LIME.html"><a href="LIME.html#LIMERcode"><i class="fa fa-check"></i><b>9.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="LIME.html"><a href="LIME.html#LIMERcodelime"><i class="fa fa-check"></i><b>9.6.1</b> The <code>lime</code> package</a></li>
<li class="chapter" data-level="9.6.2" data-path="LIME.html"><a href="LIME.html#LIMERcodelocMod"><i class="fa fa-check"></i><b>9.6.2</b> The <code>localModel</code> package</a></li>
<li class="chapter" data-level="9.6.3" data-path="LIME.html"><a href="LIME.html#LIMERcodeiml"><i class="fa fa-check"></i><b>9.6.3</b> The <code>iml</code> package</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="LIME.html"><a href="LIME.html#LIMEPythoncode"><i class="fa fa-check"></i><b>9.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ceterisParibus.html"><a href="ceterisParibus.html"><i class="fa fa-check"></i><b>10</b> Ceteris-paribus Profiles</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntro"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntuition"><i class="fa fa-check"></i><b>10.2</b> Intuition</a></li>
<li class="chapter" data-level="10.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPMethod"><i class="fa fa-check"></i><b>10.3</b> Method</a></li>
<li class="chapter" data-level="10.4" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPExample"><i class="fa fa-check"></i><b>10.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="10.5" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPProsCons"><i class="fa fa-check"></i><b>10.5</b> Pros and cons</a></li>
<li class="chapter" data-level="10.6" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPR"><i class="fa fa-check"></i><b>10.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#basic-use-of-the-predict_profile-function"><i class="fa fa-check"></i><b>10.6.1</b> Basic use of the <code>predict_profile()</code> function</a></li>
<li class="chapter" data-level="10.6.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#advanced-use-of-the-predict_profile-function"><i class="fa fa-check"></i><b>10.6.2</b> Advanced use of the <code>predict_profile()</code> function</a></li>
<li class="chapter" data-level="10.6.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#comparison-of-models-champion-challenger"><i class="fa fa-check"></i><b>10.6.3</b> Comparison of models (champion-challenger)</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPPython"><i class="fa fa-check"></i><b>10.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html"><i class="fa fa-check"></i><b>11</b> Ceteris-paribus Oscillations</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntro"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntuition"><i class="fa fa-check"></i><b>11.2</b> Intuition</a></li>
<li class="chapter" data-level="11.3" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscMethod"><i class="fa fa-check"></i><b>11.3</b> Method</a></li>
<li class="chapter" data-level="11.4" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscExample"><i class="fa fa-check"></i><b>11.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="11.5" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscProsCons"><i class="fa fa-check"></i><b>11.5</b> Pros and cons</a></li>
<li class="chapter" data-level="11.6" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscR"><i class="fa fa-check"></i><b>11.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#basic-use-of-the-predict_parts-function-1"><i class="fa fa-check"></i><b>11.6.1</b> Basic use of the <code>predict_parts()</code> function</a></li>
<li class="chapter" data-level="11.6.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#advanced-use-of-the-predict_parts-function-1"><i class="fa fa-check"></i><b>11.6.2</b> Advanced use of the <code>predict_parts()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscPython"><i class="fa fa-check"></i><b>11.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="localDiagnostics.html"><a href="localDiagnostics.html"><i class="fa fa-check"></i><b>12</b> Local-diagnostics Plots</a>
<ul>
<li class="chapter" data-level="12.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntuition"><i class="fa fa-check"></i><b>12.2</b> Intuition</a></li>
<li class="chapter" data-level="12.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagMethod"><i class="fa fa-check"></i><b>12.3</b> Method</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagneighbours"><i class="fa fa-check"></i><b>12.3.1</b> Nearest neighbours</a></li>
<li class="chapter" data-level="12.3.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagLFplot"><i class="fa fa-check"></i><b>12.3.2</b> Local-fidelity plot</a></li>
<li class="chapter" data-level="12.3.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProfiles"><i class="fa fa-check"></i><b>12.3.3</b> Local-stability plot</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagExample"><i class="fa fa-check"></i><b>12.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProsCons"><i class="fa fa-check"></i><b>12.5</b> Pros and cons</a></li>
<li class="chapter" data-level="12.6" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagR"><i class="fa fa-check"></i><b>12.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="12.7" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagPython"><i class="fa fa-check"></i><b>12.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html"><i class="fa fa-check"></i><b>13</b> Summary of Instance-level Exploration</a>
<ul>
<li class="chapter" data-level="13.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#summaryInstanceLevelIntro"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#number-of-explanatory-variables-in-the-model"><i class="fa fa-check"></i><b>13.2</b> Number of explanatory variables in the model</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#low-to-medium-number-of-explanatory-variables"><i class="fa fa-check"></i><b>13.2.1</b> Low to medium number of explanatory variables</a></li>
<li class="chapter" data-level="13.2.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#medium-to-a-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>13.2.2</b> Medium to a large number of explanatory variables</a></li>
<li class="chapter" data-level="13.2.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#very-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>13.2.3</b> Very large number of explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#correlated-explanatory-variables"><i class="fa fa-check"></i><b>13.3</b> Correlated explanatory variables</a></li>
<li class="chapter" data-level="13.4" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#models-with-interactions"><i class="fa fa-check"></i><b>13.4</b> Models with interactions</a></li>
<li class="chapter" data-level="13.5" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#sparse-explanations"><i class="fa fa-check"></i><b>13.5</b> Sparse explanations</a></li>
<li class="chapter" data-level="13.6" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#additional-uses-of-model-exploration-and-explanation"><i class="fa fa-check"></i><b>13.6</b> Additional uses of model exploration and explanation</a></li>
<li class="chapter" data-level="13.7" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#comparison-of-models-champion-challenger-analysis"><i class="fa fa-check"></i><b>13.7</b> Comparison of models (champion-challenger analysis)</a></li>
</ul></li>
<li class="part"><span><b>III Dataset Level</b></span></li>
<li class="chapter" data-level="14" data-path="modelLevelExploration.html"><a href="modelLevelExploration.html"><i class="fa fa-check"></i><b>14</b> Introduction to Dataset-level Exploration</a></li>
<li class="chapter" data-level="15" data-path="modelPerformance.html"><a href="modelPerformance.html"><i class="fa fa-check"></i><b>15</b> Model-performance Measures</a>
<ul>
<li class="chapter" data-level="15.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntro"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntuition"><i class="fa fa-check"></i><b>15.2</b> Intuition</a></li>
<li class="chapter" data-level="15.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethod"><i class="fa fa-check"></i><b>15.3</b> Method</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCont"><i class="fa fa-check"></i><b>15.3.1</b> Continuous dependent variable</a></li>
<li class="chapter" data-level="15.3.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodBin"><i class="fa fa-check"></i><b>15.3.2</b> Binary dependent variable</a></li>
<li class="chapter" data-level="15.3.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCateg"><i class="fa fa-check"></i><b>15.3.3</b> Categorical dependent variable</a></li>
<li class="chapter" data-level="15.3.4" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCount"><i class="fa fa-check"></i><b>15.3.4</b> Count dependent variable</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="modelPerformance.html"><a href="modelPerformance.html#example"><i class="fa fa-check"></i><b>15.4</b> Example</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceApartments"><i class="fa fa-check"></i><b>15.4.1</b> Apartment prices</a></li>
<li class="chapter" data-level="15.4.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceTitanic"><i class="fa fa-check"></i><b>15.4.2</b> Titanic data</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceProsCons"><i class="fa fa-check"></i><b>15.5</b> Pros and cons</a></li>
<li class="chapter" data-level="15.6" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceR"><i class="fa fa-check"></i><b>15.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="15.7" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformancePython"><i class="fa fa-check"></i><b>15.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="featureImportance.html"><a href="featureImportance.html"><i class="fa fa-check"></i><b>16</b> Variable-importance Measures</a>
<ul>
<li class="chapter" data-level="16.1" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntro"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntuition"><i class="fa fa-check"></i><b>16.2</b> Intuition</a></li>
<li class="chapter" data-level="16.3" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceMethod"><i class="fa fa-check"></i><b>16.3</b> Method</a></li>
<li class="chapter" data-level="16.4" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceTitanic"><i class="fa fa-check"></i><b>16.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="16.5" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceProsCons"><i class="fa fa-check"></i><b>16.5</b> Pros and cons</a></li>
<li class="chapter" data-level="16.6" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceR"><i class="fa fa-check"></i><b>16.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="16.7" data-path="featureImportance.html"><a href="featureImportance.html#featureImportancePython"><i class="fa fa-check"></i><b>16.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html"><i class="fa fa-check"></i><b>17</b> Partial-dependence Profiles</a>
<ul>
<li class="chapter" data-level="17.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntro"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntuition"><i class="fa fa-check"></i><b>17.2</b> Intuition</a></li>
<li class="chapter" data-level="17.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPMethod"><i class="fa fa-check"></i><b>17.3</b> Method</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPs"><i class="fa fa-check"></i><b>17.3.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="17.3.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clusteredPDPs"><i class="fa fa-check"></i><b>17.3.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="17.3.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#groupedPDPs"><i class="fa fa-check"></i><b>17.3.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="17.3.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastivePDPs"><i class="fa fa-check"></i><b>17.3.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPExample"><i class="fa fa-check"></i><b>17.4</b> Example: apartment-prices data</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependence-profiles"><i class="fa fa-check"></i><b>17.4.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="17.4.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles"><i class="fa fa-check"></i><b>17.4.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="17.4.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles"><i class="fa fa-check"></i><b>17.4.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="17.4.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles"><i class="fa fa-check"></i><b>17.4.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPProsCons"><i class="fa fa-check"></i><b>17.5</b> Pros and cons</a></li>
<li class="chapter" data-level="17.6" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPR"><i class="fa fa-check"></i><b>17.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependence-profiles-1"><i class="fa fa-check"></i><b>17.6.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="17.6.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>17.6.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="17.6.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>17.6.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="17.6.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>17.6.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPPython"><i class="fa fa-check"></i><b>17.7</b> Code snippets for Python</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles-2"><i class="fa fa-check"></i><b>17.7.1</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="17.7.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles-2"><i class="fa fa-check"></i><b>17.7.2</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html"><i class="fa fa-check"></i><b>18</b> Local-dependence and Accumulated-local Profiles</a>
<ul>
<li class="chapter" data-level="18.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntro"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntuition"><i class="fa fa-check"></i><b>18.2</b> Intuition</a></li>
<li class="chapter" data-level="18.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPMethod"><i class="fa fa-check"></i><b>18.3</b> Method</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#local-dependence-profile"><i class="fa fa-check"></i><b>18.3.1</b> Local-dependence profile</a></li>
<li class="chapter" data-level="18.3.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#accumulated-local-profile"><i class="fa fa-check"></i><b>18.3.2</b> Accumulated-local profile</a></li>
<li class="chapter" data-level="18.3.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#summaryFeatureEffects"><i class="fa fa-check"></i><b>18.3.3</b> Illustrative examples</a></li>
<li class="chapter" data-level="18.3.4" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#dependence-profiles-for-a-model-with-interaction-and-correlated-explanatory-variables-an-example"><i class="fa fa-check"></i><b>18.3.4</b> Dependence profiles for a model with interaction and correlated explanatory variables: an example</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#CDPExample"><i class="fa fa-check"></i><b>18.4</b> Example: apartment-prices data</a></li>
<li class="chapter" data-level="18.5" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPProsCons"><i class="fa fa-check"></i><b>18.5</b> Pros and cons</a></li>
<li class="chapter" data-level="18.6" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPR"><i class="fa fa-check"></i><b>18.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="18.7" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPPython"><i class="fa fa-check"></i><b>18.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html"><i class="fa fa-check"></i><b>19</b> Residual-diagnostics Plots</a>
<ul>
<li class="chapter" data-level="19.1" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntroResidualDiagnostic"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntuitionResidualDiagnostic"><i class="fa fa-check"></i><b>19.2</b> Intuition</a></li>
<li class="chapter" data-level="19.3" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#MethodResidualDiagnostic"><i class="fa fa-check"></i><b>19.3</b> Method</a></li>
<li class="chapter" data-level="19.4" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ExampleResidualDiagnostic"><i class="fa fa-check"></i><b>19.4</b> Example: apartment-prices data</a></li>
<li class="chapter" data-level="19.5" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ProsConsResidualDiagnostic"><i class="fa fa-check"></i><b>19.5</b> Pros and cons</a></li>
<li class="chapter" data-level="19.6" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#RcodeResidualDiagnostic"><i class="fa fa-check"></i><b>19.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="19.7" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#PythoncodeResidualDiagnostic"><i class="fa fa-check"></i><b>19.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html"><i class="fa fa-check"></i><b>20</b> Summary of Dataset-level Exploration</a>
<ul>
<li class="chapter" data-level="20.1" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html#summaryModelLevelIntro"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
<li class="chapter" data-level="20.2" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html#exploration-on-trainingtesting-data"><i class="fa fa-check"></i><b>20.2</b> Exploration on training/testing data</a></li>
<li class="chapter" data-level="20.3" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html#correlated-explanatory-variables-1"><i class="fa fa-check"></i><b>20.3</b> Correlated explanatory variables</a></li>
<li class="chapter" data-level="20.4" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html#comparison-of-models-champion-challenger-analysis-1"><i class="fa fa-check"></i><b>20.4</b> Comparison of models (champion-challenger analysis)</a></li>
</ul></li>
<li class="part"><span><b>IV Use-cases</b></span></li>
<li class="chapter" data-level="21" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html"><i class="fa fa-check"></i><b>21</b> FIFA 19</a>
<ul>
<li class="chapter" data-level="21.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAintro"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAdataprep"><i class="fa fa-check"></i><b>21.2</b> Data preparation</a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r"><i class="fa fa-check"></i><b>21.2.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.2.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-1"><i class="fa fa-check"></i><b>21.2.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAdataunderst"><i class="fa fa-check"></i><b>21.3</b> Data understanding</a></li>
<li class="chapter" data-level="21.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelassembly"><i class="fa fa-check"></i><b>21.4</b> Model assembly</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-1"><i class="fa fa-check"></i><b>21.4.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.4.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-2"><i class="fa fa-check"></i><b>21.4.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelaudit"><i class="fa fa-check"></i><b>21.5</b> Model audit</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-2"><i class="fa fa-check"></i><b>21.5.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.5.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-3"><i class="fa fa-check"></i><b>21.5.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelunderst"><i class="fa fa-check"></i><b>21.6</b> Model understanding (dataset-level explanations)</a>
<ul>
<li class="chapter" data-level="21.6.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-3"><i class="fa fa-check"></i><b>21.6.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.6.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-4"><i class="fa fa-check"></i><b>21.6.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.7" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAinstanceunderst"><i class="fa fa-check"></i><b>21.7</b> Instance-level explanations</a>
<ul>
<li class="chapter" data-level="21.7.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFALewy"><i class="fa fa-check"></i><b>21.7.1</b> Robert Lewandowski</a></li>
<li class="chapter" data-level="21.7.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-4"><i class="fa fa-check"></i><b>21.7.2</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.7.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-5"><i class="fa fa-check"></i><b>21.7.3</b> Code snippets for Python</a></li>
<li class="chapter" data-level="21.7.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFACR7"><i class="fa fa-check"></i><b>21.7.4</b> CR7</a></li>
<li class="chapter" data-level="21.7.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFASzczesny"><i class="fa fa-check"></i><b>21.7.5</b> Wojciech Szczęsny</a></li>
<li class="chapter" data-level="21.7.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAMessi"><i class="fa fa-check"></i><b>21.7.6</b> Lionel Messi</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="reproducibility.html"><a href="reproducibility.html"><i class="fa fa-check"></i><b>22</b> Reproducibility</a>
<ul>
<li class="chapter" data-level="22.1" data-path="reproducibility.html"><a href="reproducibility.html#package-versions-for-r"><i class="fa fa-check"></i><b>22.1</b> Package versions for R</a></li>
<li class="chapter" data-level="22.2" data-path="reproducibility.html"><a href="reproducibility.html#package-versions-for-python"><i class="fa fa-check"></i><b>22.2</b> Package versions for Python</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/ModelOriented/DALEX" target="blank">DALEX website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Explanatory Model Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="LIME" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Local Interpretable Model-agnostic Explanations (LIME)</h1>
<div id="LIMEIntroduction" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Introduction</h2>
<p>Break-down (BD) plots and Shapley values, introduced in Chapters <a href="breakDown.html#breakDown">6</a> and <a href="shapley.html#shapley">8</a>, respectively, are most suitable for models with a small or moderate number of explanatory variables.</p>
<p>None of those approaches is well-suited for models with a very large number of explanatory variables, because they usually determine non-zero attributions for all variables in the model. However, in domains like, for instance, genomics or image recognition, models with hundreds of thousands, or even millions, of explanatory (input) variables are not uncommon. In such cases, sparse explanations with a small number of variables offer a useful alternative. The most popular example of such sparse explainers is the Local Interpretable Model-agnostic Explanations (LIME) method and its modifications.</p>
<p>The LIME method was originally proposed by <span class="citation">Ribeiro, Singh, and Guestrin (<a href="#ref-lime" role="doc-biblioref">2016</a>)</span>. The key idea behind it is to locally approximate a black-box model by a simpler glass-box model, which is easier to interpret. In this chapter, we describe this approach.</p>
</div>
<div id="LIMEIntuition" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Intuition</h2>
<p>The intuition behind the LIME method is explained in Figure <a href="LIME.html#fig:limeIntroduction">9.1</a>. We want to understand the factors that influence a complex black-box model around a single instance of interest (black cross). The coloured areas presented in Figure <a href="LIME.html#fig:limeIntroduction">9.1</a> correspond to decision regions for a binary classifier, i.e., they pertain to a prediction of a value of a binary dependent variable. The axes represent the values of two continuous explanatory variables. The coloured areas indicate combinations of values of the two variables for which the model classifies the observation to one of the two classes. To understand the local behavior of the complex model around the point of interest, we generate an artificial dataset, to which we fit a glass-box model. The dots in Figure <a href="LIME.html#fig:limeIntroduction">9.1</a> represent the generated artificial data; the size of the dots corresponds to proximity to the instance of interest. We can fit a simpler glass-box model to the artificial data so that it will locally approximate the predictions of the black-box model. In Figure <a href="LIME.html#fig:limeIntroduction">9.1</a>, a simple linear model (indicated by the dashed line) is used to construct the local approximation. The simpler model serves as a “local explainer” for the more complex model.</p>
<p>We may select different classes of glass-box models. The most typical choices are regularized linear models like LASSO regression <span class="citation">(Tibshirani <a href="#ref-Tibshirani94regressionshrinkage" role="doc-biblioref">1994</a>)</span> or decision trees <span class="citation">(Hothorn, Hornik, and Zeileis <a href="#ref-party2006" role="doc-biblioref">2006</a>)</span>. Both lead to sparse models that are easier to understand. The important point is to limit the complexity of the models, so that they are easier to explain. </p>

<div class="figure" style="text-align: center"><span id="fig:limeIntroduction"></span>
<img src="figure/lime_introduction.png" alt="The idea behind the LIME approximation with a local glass-box model. The coloured areas correspond to decision regions for a complex binary classification model. The black cross indicates the instance (observation) of interest. Dots correspond to artificial data around the instance of interest. The dashed line represents a simple linear model fitted to the artificial data. The simple model “explains” local behavior of the black-box model around the instance of interest." width="70%" />
<p class="caption">
Figure 9.1: The idea behind the LIME approximation with a local glass-box model. The coloured areas correspond to decision regions for a complex binary classification model. The black cross indicates the instance (observation) of interest. Dots correspond to artificial data around the instance of interest. The dashed line represents a simple linear model fitted to the artificial data. The simple model “explains” local behavior of the black-box model around the instance of interest.
</p>
</div>
</div>
<div id="LIMEMethod" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Method</h2>
<p>We want to find a model that locally approximates a black-box model <span class="math inline">\(f()\)</span> around the instance of interest <span class="math inline">\(\underline{x}_*\)</span>. Consider class <span class="math inline">\(G\)</span> of simple, interpretable models like, for instance, linear models or decision trees. To find the required approximation, we minimize a “loss function”:
<span class="math display">\[
\hat g = \arg \min_{g \in \mathcal{G}} L\{f, g, \nu(\underline{x}_*)\} + \Omega (g), 
\]</span>
where model <span class="math inline">\(g()\)</span> belongs to class <span class="math inline">\(\mathcal{G}\)</span>, <span class="math inline">\(\nu(\underline{x}_*)\)</span> defines a neighborhood of <span class="math inline">\(\underline{x}_*\)</span> in which approximation is sought, <span class="math inline">\(L()\)</span> is a function measuring the discrepancy between models <span class="math inline">\(f()\)</span> and <span class="math inline">\(g()\)</span> in the neighborhood <span class="math inline">\(\nu(\underline{x}_*)\)</span>, and <span class="math inline">\(\Omega(g)\)</span> is a penalty for the complexity of model <span class="math inline">\(g()\)</span>. The penalty is used to favour simpler models from class <span class="math inline">\(\mathcal{G}\)</span>. In applications, this criterion is very often simplified by limiting class <span class="math inline">\(G\)</span> to models with the same complexity, i.e., with the same number of coefficients. In such a situation, <span class="math inline">\(\Omega(g)\)</span> is the same for each model <span class="math inline">\(g()\)</span>, so it can be omitted in optimization.</p>
<p>Note that models <span class="math inline">\(f()\)</span> and <span class="math inline">\(g()\)</span> may operate on different data spaces. The black-box model (function) <span class="math inline">\(f(\underline{x}):\mathcal X \rightarrow \mathcal R\)</span> is defined on a large, <span class="math inline">\(p\)</span>-dimensional space <span class="math inline">\(\mathcal X\)</span> corresponding to the <span class="math inline">\(p\)</span> explanatory variables used in the model. The glass-box model (function) <span class="math inline">\(g(\underline{x}):\tilde{ \mathcal X} \rightarrow \mathcal R\)</span> is defined on a <span class="math inline">\(q\)</span>-dimensional space <span class="math inline">\(\tilde{ \mathcal X}\)</span> with <span class="math inline">\(q &lt;&lt; p\)</span>, often called the “space for interpretable representation”. We will present some examples of <span class="math inline">\(\tilde{ \mathcal X}\)</span> in the next section. For now we will just assume that some function <span class="math inline">\(h()\)</span> transforms <span class="math inline">\(\mathcal X\)</span> into <span class="math inline">\(\tilde{ \mathcal X}\)</span>.</p>
<p>If we limit class <span class="math inline">\(\mathcal{G}\)</span> to linear models with a limited number, say <span class="math inline">\(K\)</span>, of non-zero coefficients, then the following algorithm may be used to find an interpretable glass-box model <span class="math inline">\(g()\)</span> that includes <span class="math inline">\(K\)</span> most important, interpretable, explanatory variables:</p>
<pre><code>Input: x* - observation to be explained
Input: N  - sample size for the glass-box model 
Input: K  - complexity, the number of variables for the glass-box model
Input: similarity - a distance function in the original data space
1. Let x&#39; = h(x*) be a version of x* in the lower-dimensional space
2. for i in 1...N {
3.   z&#39;[i] &lt;- sample_around(x&#39;) 
4.   y&#39;[i] &lt;- f(z&#39;[i])       # prediction for new observation z&#39;[i]
5.   w&#39;[i] &lt;- similarity(x&#39;, z&#39;[i]) 
6. }
7. return K-LASSO(y&#39;, x&#39;, w&#39;)</code></pre>
<p>In Step 7, <code>K-LASSO(y', x', w')</code> stands for a weighted LASSO linear-regression that selects <span class="math inline">\(K\)</span> variables based on the new data <code>y'</code> and <code>x'</code> with weights <code>w'</code>.</p>
<p>Practical implementation of this idea involves three important steps, which are discussed in the subsequent subsections.</p>
<div id="LIMErepr" class="section level3" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Interpretable data representation</h3>
<p>As it has been mentioned, the black-box model <span class="math inline">\(f()\)</span> and the glass-box model <span class="math inline">\(g()\)</span> operate on different data spaces. For example, let us consider a VGG16 neural network <span class="citation">(Simonyan and Zisserman <a href="#ref-Simonyan15" role="doc-biblioref">2015</a>)</span> trained on the ImageNet data <span class="citation">(Deng et al. <a href="#ref-ImageNet" role="doc-biblioref">2009</a>)</span>. The model uses an image of the size of 244 <span class="math inline">\(\times\)</span> 244 pixels as input and predicts to which of 1000 potential categories does the image belong to. The original space <span class="math inline">\(\mathcal X\)</span> is of dimension 3 <span class="math inline">\(\times\)</span> 244 <span class="math inline">\(\times\)</span> 244 (three single-color channels (<em>red, green, blue</em>) for a single pixel <span class="math inline">\(\times\)</span> 244 <span class="math inline">\(\times\)</span> 244 pixels), i.e., the input space is 178,608-dimensional. Explaining predictions in such a high-dimensional space is difficult. Instead, from the perspective of a single instance of interest, the space can be transformed into superpixels, which are treated as binary features that can be turned on or off. Figure <a href="LIME.html#fig:duckHorse06">9.2</a> (right-hand-side panel) presents an example of 100 superpixels created for an ambiguous picture. Thus, in this case the black-box model <span class="math inline">\(f()\)</span> operates on space <span class="math inline">\(\mathcal X=\mathcal{R}^{178608}\)</span>, while the glass-box model <span class="math inline">\(g()\)</span> applies to space <span class="math inline">\(\tilde{ \mathcal X} = \{0,1\}^{100}\)</span>.</p>
<p>It is worth noting that superpixels, based on image segmentation, are frequent choices for image data. For text data, groups of words are frequently used as interpretable variables. For tabular data, continuous variables are often discretized to obtain interpretable categorical data. In the case of categorical variables, combination of categories is often used. We will present examples in the next section.</p>

<div class="figure" style="text-align: center"><span id="fig:duckHorse06"></span>
<img src="figure/duck_horse_06.png" alt="The left-hand-side panel shows an ambiguous picture, half-horse and half-duck (source Twitter). The right-hand-side panel shows 100 superpixels identified for this figure." width="100%" />
<p class="caption">
Figure 9.2: The left-hand-side panel shows an ambiguous picture, half-horse and half-duck (source <a href="https://twitter.com/finmaddison/status/352128550704398338">Twitter</a>). The right-hand-side panel shows 100 superpixels identified for this figure.
</p>
</div>
</div>
<div id="LIMEsample" class="section level3" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Sampling around the instance of interest</h3>
<p>To develop a local-approximation glass-box model, we need new data points in the low-dimensional interpretable data space around the instance of interest. One could consider sampling the data points from the original dataset. However, there may not be enough points to sample from, because the data in high-dimensional datasets are usually very sparse and data points are “far” from each other. Thus, we need new, artificial data points. For this reason, the data for the development of the glass-box model is often created by using perturbations of the instance of interest.</p>
<p>For binary variables in the low-dimensional space, the common choice is to switch (from 0 to 1 or from 1 to 0) the value of a randomly-selected number of variables describing the instance of interest.</p>
<p>For continuous variables, various proposals have been formulated in different papers. For example, <span class="citation">Molnar, Bischl, and Casalicchio (<a href="#ref-imlRPackage" role="doc-biblioref">2018</a>)</span> and <span class="citation">Molnar (<a href="#ref-molnar2019" role="doc-biblioref">2019</a>)</span> suggest adding Gaussian noise to continuous variables. <span class="citation">Pedersen and Benesty (<a href="#ref-limePackage" role="doc-biblioref">2019</a>)</span> propose to discretize continuous variables by using quantiles and then perturb the discretized versions of the variables. <span class="citation">Staniak et al. (<a href="#ref-localModelPackage" role="doc-biblioref">2019</a>)</span> discretize continuous variables based on segmentation of local ceteris-paribus profiles (for more information about the profiles, see Chapter <a href="ceterisParibus.html#ceterisParibus">10</a>).</p>
<p>In the example of the duck-horse image in Figure <a href="LIME.html#fig:duckHorse06">9.2</a>, the perturbations of the image could be created by randomly excluding some of the superpixels. An illustration of this process is shown in Figure <a href="LIME.html#fig:duckHorseProcess">9.3</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:duckHorseProcess"></span>
<img src="figure/duck_horse_process.png" alt="The original image (left-hand-side panel) is transformed into a lower-dimensional data space by defining 100 super pixels (panel in the middle). The artificial data are created by using subsets of superpixels (right-hand-side panel)." width="100%" />
<p class="caption">
Figure 9.3: The original image (left-hand-side panel) is transformed into a lower-dimensional data space by defining 100 super pixels (panel in the middle). The artificial data are created by using subsets of superpixels (right-hand-side panel).
</p>
</div>
</div>
<div id="LIMEglas" class="section level3" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Fitting the glass-box model</h3>
<p>Once the artificial data around the instance of interest have been created, we may attempt to fit an interpretable glass-box model <span class="math inline">\(g()\)</span> from class <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p>The most common choices for class <span class="math inline">\(\mathcal{G}\)</span> are generalized linear models. To get sparse models, i.e., models with a limited number of variables, LASSO (least absolute shrinkage and selection operator) <span class="citation">(Tibshirani <a href="#ref-Tibshirani94regressionshrinkage" role="doc-biblioref">1994</a>)</span> or similar regularization-modelling techniques are used. For instance, in the algorithm presented in Section <a href="LIME.html#LIMEMethod">9.3</a>, the K-LASSO method with K non-zero coefficients has been mentioned. An alternative choice are classification-and-regression trees models <span class="citation">(Breiman et al. <a href="#ref-CARTtree" role="doc-biblioref">1984</a>)</span>.</p>
<p>For the example of the duck-horse image in Figure <a href="LIME.html#fig:duckHorse06">9.2</a>, the VGG16 network provides 1000 probabilities that the image belongs to one of the 1000 classes used for training the network. It appears that the two most likely classes for the image are <em>‘standard poodle’</em> (probability of 0.18) and <em>‘goose’</em> (probability of 0.15). Figure <a href="LIME.html#fig:duckHorse04">9.4</a> presents LIME explanations for these two predictions. The explanations were obtained with the K-LASSO method, which selected <span class="math inline">\(K=15\)</span> superpixels that were the most influential from a model-prediction point of view. For each of the selected two classes, the <span class="math inline">\(K\)</span> superpixels with non-zero coefficients are highlighted. It is interesting to observe that the superpixel which contains the beak is influential for the <em>‘goose’</em> prediction, while superpixels linked with the white colour are influential for the <em>‘standard poodle’</em> prediction. At least for the former, the influential feature of the plot does correspond to the intended content of the image. Thus, the results of the explanation increase confidence in the model’s predictions.</p>

<div class="figure" style="text-align: center"><span id="fig:duckHorse04"></span>
<img src="figure/duck_horse_04.png" alt="LIME for two predictions (‘standard poodle’ and ‘goose’) obtained by the VGG16 network with ImageNet weights for the half-duck, half-horse image." width="100%" />
<p class="caption">
Figure 9.4: LIME for two predictions (‘standard poodle’ and ‘goose’) obtained by the VGG16 network with ImageNet weights for the half-duck, half-horse image. TODO: fix apostrophes!
</p>
</div>
</div>
</div>
<div id="LIMEExample" class="section level2" number="9.4">
<h2><span class="header-section-number">9.4</span> Example: Titanic data</h2>
<p>Most examples of the LIME method are related to the text or image data. In this section, we present an example of a binary classification for tabular data to facilitate comparisons between methods introduced in different chapters.</p>
<p>Let us consider the random forest model <code>titanic_rf</code> (see Section <a href="dataSetsIntro.html#model-titanic-rf">4.2.2</a>) and passenger Johnny D (see Section <a href="dataSetsIntro.html#predictions-titanic">4.2.5</a>) as the instance of interest for the Titanic data.</p>
<p>First, we have got to define an interpretable data space. One option would be to gather similar variables into larger constructs corresponding to some concepts. For example <em>class</em> and <em>fare</em> variables can be combined into “wealth”, <em>age</em> and <em>gender</em> into “demography”, and so on. In this example, however, we have got a relatively small number of variables, so we will use a simpler data representation in the form of a binary vector. Toward this aim, each variable is dichotomized into two levels. For example, <em>age</em> is transformed into a binary variable with categories “<span class="math inline">\(\leq\)</span> 15.36” and “&gt;15.36”, <em>class</em> is transformed into a binary variable with categories “1st/2nd/deck crew” and “other”, and so on. Once the lower-dimension data space is defined, the LIME algorithm is applied to this space. In particular, we first have got to appropriately transform data for Johnny D. Subsequently, we generate a new artificial dataset that will be used for K-LASSO approximations of the random forest model. In particular, the K-LASSO method with <span class="math inline">\(K=3\)</span> is used to identify the three most influential (binary) variables that will provide an explanation for the prediction for Johnny D. The three variables are: <em>age</em>, <em>gender</em>, and <em>class</em>. This result agrees with the conclusions drawn in the previous chapters. Figure <a href="LIME.html#fig:LIMEexample01">9.5</a> shows the coefficients estimated for the K-LASSO model.</p>

<div class="figure" style="text-align: center"><span id="fig:LIMEexample01"></span>
<img src="figure/LIMEexample01.png" alt="LIME method for the prediction for Johnny D for the random forest model titanic_rf and the Titanic data. Presented values are the coefficients of the K-LASSO model fitted locally to the predictions from the original model." width="80%" />
<p class="caption">
Figure 9.5: LIME method for the prediction for Johnny D for the random forest model <code>titanic_rf</code> and the Titanic data. Presented values are the coefficients of the K-LASSO model fitted locally to the predictions from the original model.
</p>
</div>
<!---
The interpretable features can be defined in a many different ways. One idea would to be use quartiles for the feature of interest. Another idea is to use Ceteris Paribus profiles (see Chapter \@ref(ceterisParibus) and change-point method [@picard_1985] to find a instance specific discretization.
Different implementations of LIME differ in the way how the interpretable feature space is created.
--->
</div>
<div id="LIMEProsCons" class="section level2" number="9.5">
<h2><span class="header-section-number">9.5</span> Pros and cons</h2>
<p>As mentioned by <span class="citation">Ribeiro, Singh, and Guestrin (<a href="#ref-lime" role="doc-biblioref">2016</a>)</span>, the LIME method</p>
<ul>
<li>is <em>model-agnostic</em>, as it does not imply any assumptions about the black-box model structure;</li>
<li>offers an <em>interpretable representation</em>, because the original data space is transformed (for instance, by replacing individual pixels by superpixels for image data) into a more interpretable, lower-dimension space;</li>
<li>provides <em>local fidelity</em>, i.e., the explanations are locally well-fitted to the black-box model.</li>
</ul>
<p>The method has been widely adopted in the text and image analysis, partly due to the interpretable data representation. In that case, the explanations are delivered in the form of fragments of an image/text, and users can easily find the justification of such explanations. The underlying intuition for the method is easy to understand: a simpler model is used to approximate a more complex one. By using a simpler model, with a smaller number of interpretable explanatory variables, predictions are easier to explain. The LIME method can be applied to complex, high-dimensional models.</p>
<p>There are several important limitations, however. For instance, as mentioned in Section <a href="LIME.html#LIMEsample">9.3.2</a>, there have been various proposals for finding interpretable representations for continuous and categorical explanatory variables in case of tabular data. The issue has not been solved yet. This leads to different implementations of LIME, which use different variable-transformation methods and, consequently, that can lead to different results.</p>
<p>Another important point is that, because the glass-box model is selected to approximate the black-box model, and not the data themselves, the method does not control the quality of the local fit of the glass-box model to the data. Thus, the latter model may be misleading.</p>
<p>Finally, in high-dimensional data, data points are sparse. Defining a “local neighborhood” of the instance of interest may not be straightforward. Importance of the selection of the neighborhood is discussed, for example, by <span class="citation">Alvarez-Melis and Jaakkola (<a href="#ref-LIMESHAPstability" role="doc-biblioref">2018</a>)</span>. Sometimes even slight changes in the neighborhood strongly affect the obtained explanations.</p>
<p>To summarize, the most useful applications of LIME are limited to high-dimensional data for which one can define a low-dimensional interpretable data representation, as in image analysis, text analysis, or genomics.</p>
</div>
<div id="LIMERcode" class="section level2" number="9.6">
<h2><span class="header-section-number">9.6</span> Code snippets for R</h2>
<p>LIME and its variants are implemented in various R and Python packages. For example, <code>lime</code> <span class="citation">(Pedersen and Benesty <a href="#ref-limePackage" role="doc-biblioref">2019</a>)</span> started as a port of the LIME Python library <span class="citation">(Lundberg <a href="#ref-shapPackage" role="doc-biblioref">2019</a>)</span>, while <code>localModel</code> <span class="citation">(Staniak et al. <a href="#ref-localModelPackage" role="doc-biblioref">2019</a>)</span>, and <code>iml</code> <span class="citation">(Molnar, Bischl, and Casalicchio <a href="#ref-imlRPackage" role="doc-biblioref">2018</a>)</span> are separate packages that implement a version of this method entirely in R. </p>
<p>Different implementations of LIME offer different algorithms for extraction of interpretable features, different methods for sampling, and different methods of weighting. For instance, regarding transformation of continuous variables into interpretable features, <code>lime</code> performs global discretization using quartiles, <code>localModel</code> performs local discretization using ceteris-paribus profiles (for more information about the profiles, see Chapter <a href="ceterisParibus.html#ceterisParibus">10</a>), while <code>iml</code> works directly on continuous variables. Due to these differences, the packages yield different results (explanations).</p>
<p>Also, <code>lime</code>, <code>localModel</code>, and <code>iml</code> use different functions to implement the LIME method. Thus, we will use the <code>predict_surrogate()</code> method from the <code>DALEXtra</code> package. The function offers a uniform interface to the functions from the three packages.</p>
<p>In what follows, for illustration purposes, we use the <code>titanic_rf</code> random forest model for the Titanic data developed in Section <a href="dataSetsIntro.html#model-titanic-rf">4.2.2</a>. Recall that it is developed to predict the probability of survival from the sinking of the Titanic. Instance-level explanations are calculated for Johnny D, an 8-year-old passenger that travelled in the first class. We first retrieve the <code>titanic_rf</code> model-object and the data frame for Johnny D via the <code>archivist</code> hooks, as listed in Section <a href="dataSetsIntro.html#ListOfModelsTitanic">4.2.7</a>. We also retrieve the version of the <code>titanic</code> data with imputed missing values.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="LIME.html#cb3-1"></a>titanic_imputed &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/27e5c&quot;</span>)</span>
<span id="cb3-2"><a href="LIME.html#cb3-2"></a>titanic_rf &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="st"> </span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/4e0fc&quot;</span>)</span>
<span id="cb3-3"><a href="LIME.html#cb3-3"></a>johnny_d &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="st"> </span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/e3596&quot;</span>)</span></code></pre></div>
<pre><code>  class gender age sibsp parch fare    embarked
1   1st   male   8     0     0   72 Southampton</code></pre>
<p>Then we construct the explainer for the model by using the function <code>explain()</code> from the <code>DALEX</code> package (see Section <a href="dataSetsIntro.html#ExplainersTitanicRCode">4.2.6</a>). We also load the <code>randomForest</code> package, as the model was fitted by using function <code>randomForest()</code> from this package (see Section <a href="dataSetsIntro.html#model-titanic-rf">4.2.2</a>) and it is important to have the corresponding <code>predict()</code> function available.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="LIME.html#cb5-1"></a><span class="kw">library</span>(<span class="st">&quot;randomForest&quot;</span>)</span>
<span id="cb5-2"><a href="LIME.html#cb5-2"></a><span class="kw">library</span>(<span class="st">&quot;DALEX&quot;</span>)</span>
<span id="cb5-3"><a href="LIME.html#cb5-3"></a>titanic_rf_exp &lt;-<span class="st"> </span>DALEX<span class="op">::</span><span class="kw">explain</span>(<span class="dt">model =</span> titanic_rf,  </span>
<span id="cb5-4"><a href="LIME.html#cb5-4"></a>                        <span class="dt">data =</span> titanic_imputed[, <span class="dv">-9</span>],</span>
<span id="cb5-5"><a href="LIME.html#cb5-5"></a>                           <span class="dt">y =</span> titanic_imputed<span class="op">$</span>survived <span class="op">==</span><span class="st"> &quot;yes&quot;</span>, </span>
<span id="cb5-6"><a href="LIME.html#cb5-6"></a>                       <span class="dt">label =</span> <span class="st">&quot;Random Forest&quot;</span>)</span></code></pre></div>
<div id="LIMERcodelime" class="section level3" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> The <code>lime</code> package</h3>
<p>The key functions in the <code>lime</code> package are <code>lime()</code>, which creates an explanation, and <code>explain()</code>, which evaluates explanations. As mentioned earlier, we will apply the <code>predict_surrogate()</code> function from the <code>DALEXtra</code> package to access the functions via an interface that is consistent with the approach used in the previous chapters.</p>
<p>The <code>predict_surrogate()</code> function requires two arguments: <code>explainer</code>, which specifies the name of the explainer-object created with the help of function <code>explain()</code> from the <code>DALEX</code> package, and <code>new_observation</code>, which specifies the name of the data frame for the instance for which prediction is of interest. An additional, important argument is <code>type</code> that indicates the package with the desired implementation of the LIME method: either <code>"localModel"</code> (default), <code>"lime"</code>, or <code>"iml"</code>. In case of the <code>lime</code>-package implementation, we can specify two additional arguments: <code>n_features</code> to indicate the maximum number (<span class="math inline">\(K\)</span>) of explanatory variables to be selected by the K-LASSO method, and <code>n_permutations</code> to specify the number of artifical data points to be sampled for the local-model approximation.</p>
<p>In the code below, we apply the <code>predict_surrogate()</code> function to the explainer-object for the random forest model <code>titanic_rf</code> and data for Johnny D. Additionally, we specify that the K-LASSO method should select no more than <code>n_features=3</code> explanatory variables based on a fit to <code>n_permutations=1000</code> sampled data points. Note that we use the <code>set.seed()</code> function to ensure repeatability of the sampling.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="LIME.html#cb6-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb6-2"><a href="LIME.html#cb6-2"></a><span class="kw">library</span>(<span class="st">&quot;DALEXtra&quot;</span>)</span>
<span id="cb6-3"><a href="LIME.html#cb6-3"></a><span class="kw">library</span>(<span class="st">&quot;lime&quot;</span>)</span>
<span id="cb6-4"><a href="LIME.html#cb6-4"></a>model_type.dalex_explainer &lt;-<span class="st"> </span>DALEXtra<span class="op">::</span>model_type.dalex_explainer</span>
<span id="cb6-5"><a href="LIME.html#cb6-5"></a>predict_model.dalex_explainer &lt;-<span class="st"> </span>DALEXtra<span class="op">::</span>predict_model.dalex_explainer</span>
<span id="cb6-6"><a href="LIME.html#cb6-6"></a></span>
<span id="cb6-7"><a href="LIME.html#cb6-7"></a>lime_johnny &lt;-<span class="st"> </span><span class="kw">predict_surrogate</span>(<span class="dt">explainer =</span> titanic_rf_exp, </span>
<span id="cb6-8"><a href="LIME.html#cb6-8"></a>                  <span class="dt">new_observation =</span> johnny_d, </span>
<span id="cb6-9"><a href="LIME.html#cb6-9"></a>                  <span class="dt">n_features =</span> <span class="dv">3</span>, </span>
<span id="cb6-10"><a href="LIME.html#cb6-10"></a>                  <span class="dt">n_permutations =</span> <span class="dv">1000</span>,</span>
<span id="cb6-11"><a href="LIME.html#cb6-11"></a>                  <span class="dt">type =</span> <span class="st">&quot;lime&quot;</span>)</span></code></pre></div>
<!-- Subsequently, we create an explainer, i.e., an object with all elements needed for calculation of explanations. This can be done by using the `lime()` function with the data frame used for model fitting and the model object as arguments `x` and `model`, respectively.


```r
lime_rf <- lime(x = titanic_imputed[,colnames(johnny_d)], model = titanic_rf)
```

Finally, we generate an explanation. Toward this aim, we use the `lime::explain()` function. Note that it is worthwhile to indicate we use the `explain()` function from the `lime` package, because there is a similarly-named function in the `DALEX` package. The main arguments are `x`, which specifies the data frame for the instance of interest, and  `explainer`, which indicates the name of the explainer object. In the code below, we additionally apply the `labels = "yes"` argument to specifiy the value of the dependent binary variable for which predictions are of interest. -->
<p>The contents of the resulting object can be printed out in the form of a data frame with 11 variables.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="LIME.html#cb7-1"></a>(<span class="kw">as.data.frame</span>(lime_johnny))</span></code></pre></div>
<pre><code>##   model_type case  model_r2 model_intercept model_prediction feature
## 1 regression    1 0.6826437       0.5541115        0.4784804  gender
## 2 regression    1 0.6826437       0.5541115        0.4784804     age
## 3 regression    1 0.6826437       0.5541115        0.4784804   class
##   feature_value feature_weight  feature_desc                 data
## 1             2     -0.4038175 gender = male 1, 2, 8, 0, 0, 72, 4
## 2             8      0.1636630     age &lt;= 22 1, 2, 8, 0, 0, 72, 4
## 3             1      0.1645234   class = 1st 1, 2, 8, 0, 0, 72, 4
##   prediction
## 1      0.422
## 2      0.422
## 3      0.422</code></pre>
<!-- Currently, function `lime::explain()` does not include any argument that would allow fixing the settings of the random-permutation algorithm to obtain a repeateable execution. Hence, in the output below, we present an exemplary set of results. Note that, to get all the details, it is advisable to print the object, obtained from the apllication of the `lime::explain()` function, as a data frame. 


```r
print(as.data.frame(lime_expl))

#      model_type case label label_prob  model_r2 model_intercept model_prediction feature
#1 classification    1   yes      0.422 0.6522297       0.5481286        0.4887004  gender
#2 classification    1   yes      0.422 0.6522297       0.5481286        0.4887004     age
#3 classification    1   yes      0.422 0.6522297       0.5481286        0.4887004   class
#4 classification    1   yes      0.422 0.6522297       0.5481286        0.4887004    fare
#  feature_value feature_weight  feature_desc                 data   prediction
#1             2    -0.39647493 gender = male 1, 2, 8, 0, 0, 72, 4 0.578, 0.422
#2             8     0.14277999     age <= 22 1, 2, 8, 0, 0, 72, 4 0.578, 0.422
#3             1     0.15175337   class = 1st 1, 2, 8, 0, 0, 72, 4 0.578, 0.422
#4            72     0.04251332  21.00 < fare 1, 2, 8, 0, 0, 72, 4 0.578, 0.422
```
-->
<p>The output includes column <code>case</code> that provides indices of observations for which the explanations are calculated. In our case there is only one index equal to 1, because we asked for an explanation for only one observation, Johnny D. The <code>feature</code> column indicates which explanatory variables were given non-zero coefficients in the K-LASSO method. The <code>feature_value</code> column provides information about the values of the original explanatory variables for the observations for which the explanations are calculated. On the other hand, the <code>feature_desc</code> column indicates how the original explanatory variable was transformed. Note that the applied implementation of the LIME method dichotomizes continuous variables by using quartiles. Hence, for instance, <em>age</em> for Johnny D was transformed into a binary variable <code>age &lt;= 22</code>.</p>
<p>Column <code>feature_weight</code> provides the estimated coefficients for the variables selected by the K-LASSO method for the explanation. The <code>model_intercept</code> column provides of the value of the intercept. Thus, the linear combination of the transformed explanatory variables used in the glass-box model approximating the random forest model around the instance of interest, Johnny D, is given by the following equation (see Section <a href="modelDevelopmentProcess.html#fitting">2.5</a>):</p>
<p><span class="math display">\[
\hat p_{lime} = 0.55411 - 0.40381 \cdot 1_{male} + 0.16366 \cdot 1_{age &lt;= 22}  + 0.16452 \cdot 1_{class = 1st} = 0.47848,
\]</span>
where <span class="math inline">\(1_A\)</span> denotes the indicator variable for condition <span class="math inline">\(A\)</span>. Note that the computed value corresponds to the number given in the column <code>model_prediction</code> in the printed output.</p>
<!-- Consequently, the predicted survival probability for the glass-box model is equal to 

$$ 
p = e^{0.489}/(1+e^{0.489}) = 0.620.
$$ 

For the random forest model `titanic_rf`, the predicted probability was equal to 0.422, as seen in the `prediction` column (see also Section \@ref(predictions-titanic)). [TOMASZ: A SURPRISINGLY LARGE DIFFERENCE. CHECK?]-->
<p>By applying the <code>plot()</code> function to the object containing the explanation, we obtain a graphical presentation of the results.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="LIME.html#cb9-1"></a><span class="kw">plot</span>(lime_johnny)</span></code></pre></div>
<p>The resulting plot is shown in Figure <a href="LIME.html#fig:limeExplLIMETitanic">9.6</a>. The length of the bar indicates the magnitude (absolute value), while the color indicates the sign (red for negative, blue for positive) of the estimated coefficient.</p>

<div class="figure" style="text-align: center"><span id="fig:limeExplLIMETitanic"></span>
<img src="ema_files/figure-html/limeExplLIMETitanic-1.png" alt="Illustration of the LIME-method results for the prediction for Johnny D for the random forest model titanic_rf and the Titanic data, generated by the lime package." width="80%" />
<p class="caption">
Figure 9.6: Illustration of the LIME-method results for the prediction for Johnny D for the random forest model <code>titanic_rf</code> and the Titanic data, generated by the <code>lime</code> package.
</p>
</div>
</div>
<div id="LIMERcodelocMod" class="section level3" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> The <code>localModel</code> package</h3>
<p>The key function of the <code>localModel</code> package is the <code>individual_surrogate_model()</code> function that fits the local glass-box model. The function is applied to the explainer-object obtained with the help of the <code>DALEX::explain()</code> function (see Section <a href="dataSetsIntro.html#ExplainersTitanicRCode">4.2.6</a>). As mentioned earlier, we will apply the <code>predict_surrogate()</code> function from the <code>DALEXtra</code> package to access the functions via an interface that is consistent with the approach used in the previous chapters. To choose the <code>localModel</code>-implementation of LIME, we set argument <code>type="localMode"</code> (see Section <a href="LIME.html#LIMERcodelime">9.6.1</a>). In that case, the method accepts, apart from the required arguments <code>explainer</code> and <code>new_observation</code>, two additional arguments: <code>size</code>, which specifies the number of artificial data points to be sampled for the local-model approximation, and <code>seed</code>, which sets the seed for the random-number generation allowing for a repeatable execution.</p>
<p>In the code below, we apply the <code>predict_surrogate()</code> function to the explainer-object for the random forest model <code>titanic_rf</code> and data for Johnny D. Additionally, we specify that 1000 data points are to be sampled and we set the random-number-generation seed.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="LIME.html#cb10-1"></a><span class="kw">library</span>(<span class="st">&quot;localModel&quot;</span>)</span>
<span id="cb10-2"><a href="LIME.html#cb10-2"></a>locMod_johnny &lt;-<span class="st"> </span><span class="kw">predict_surrogate</span>(<span class="dt">explainer =</span> titanic_rf_exp, </span>
<span id="cb10-3"><a href="LIME.html#cb10-3"></a>                  <span class="dt">new_observation =</span> johnny_d, </span>
<span id="cb10-4"><a href="LIME.html#cb10-4"></a>                  <span class="dt">size =</span> <span class="dv">1000</span>, </span>
<span id="cb10-5"><a href="LIME.html#cb10-5"></a>                  <span class="dt">seed =</span> <span class="dv">1</span>,</span>
<span id="cb10-6"><a href="LIME.html#cb10-6"></a>                  <span class="dt">type =</span> <span class="st">&quot;localModel&quot;</span>)</span></code></pre></div>
<p>The resulting object is a data frame with seven variables (columns). For brevity, we only print out the first three variables.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="LIME.html#cb11-1"></a>locMod_johnny[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</span></code></pre></div>
<pre><code>##     estimated                        variable original_variable
## 1  0.23530947                    (Model mean)                  
## 2  0.30331646                     (Intercept)                  
## 3  0.06004988                   gender = male            gender
## 4 -0.05222505                    age &lt;= 15.36               age
## 5  0.20988506     class = 1st, 2nd, deck crew             class
## 6  0.00000000 embarked = Belfast, Southampton          embarked</code></pre>
<!----
#   estimated                    variable dev_ratio response
#1 0.23479837                (Model mean) 0.6521442         
#2 0.14483341                 (Intercept) 0.6521442         
#3 0.08081853 class = 1st, 2nd, deck crew 0.6521442         
#4 0.00000000     gender = female, NA, NA 0.6521442         
#5 0.23282293                age <= 15.36 0.6521442         
#6 0.02338929                fare > 31.05 0.6521442    
---->
<p>The printed output includes column <code>estimated</code> that contains the estimated coefficients of the LASSO regression model, which is used to approximate the predictions from the random forest model. Column <code>variable</code> provides the information about the corresponding variables, which are transformations of <code>original_variable</code>. Note that the version of LIME, implemented in the <code>localModel</code> package, dichotomizes continuous variables by using ceteris-paribus profiles (for more information about the profiles, see Chapter <a href="ceterisParibus.html#ceterisParibus">10</a>). The profile for variable <em>age</em> for Johnny D can be obtained by using function <code>plot_interpretable_feature()</code>, as shown below.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="LIME.html#cb13-1"></a><span class="kw">plot_interpretable_feature</span>(locMod_johnny, <span class="st">&quot;age&quot;</span>)</span></code></pre></div>
<p>The resulting plot is presented in Figure <a href="LIME.html#fig:LIMEexample02">9.7</a>. The profile indicates that the largest drop in the predicted probability of survival is observed when the value of <em>age</em> increases beyond about 15 years. Hence, in the output of the <code>predict_surrogate()</code> function, we see a binary variable <code>age &lt;= 15.36</code>, as Johnny D was 8 years old.</p>

<div class="figure" style="text-align: center"><span id="fig:LIMEexample02"></span>
<img src="ema_files/figure-html/LIMEexample02-1.png" alt="Discretization of the age variable for Johnny D based on the ceteris-paribus profile. The optimal change-point is around 15 years of age." width="70%" />
<p class="caption">
Figure 9.7: Discretization of the <em>age</em> variable for Johnny D based on the ceteris-paribus profile. The optimal change-point is around 15 years of age.
</p>
</div>
<p>By applying the generic <code>plot()</code> function to the object containing the LIME-method results, we obtain a graphical representation of the results.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="LIME.html#cb14-1"></a><span class="kw">plot</span>(locMod_johnny)</span></code></pre></div>
<p>The resulting plot is shown in Figure <a href="LIME.html#fig:limeExplLocalModelTitanic">9.8</a>. The lengths of the bars indicate the magnitude (absolute value) of the estimated coefficients of the LASSO logistic regression model. The bars are placed relative to the value of the mean prediction, 0.235.</p>

<div class="figure" style="text-align: center"><span id="fig:limeExplLocalModelTitanic"></span>
<img src="ema_files/figure-html/limeExplLocalModelTitanic-1.png" alt="Illustration of the LIME-method results for the prediction for Johnny D for the random forest model titanic_rf and the Titanic data, generated by the localModel package." width="80%" />
<p class="caption">
Figure 9.8: Illustration of the LIME-method results for the prediction for Johnny D for the random forest model <code>titanic_rf</code> and the Titanic data, generated by the <code>localModel</code> package.
</p>
</div>
<!---
(ref:limeExplLocalModelTitanicDesc1) Illustration of the LIME-method results for the prediction for `johny_d` for the random forest model `titanic_rf` and the Titanic data, generated by the `localModel` package.

<div class="figure" style="text-align: center">
<img src="figure/lime_expl_localModel_titanic.png" alt="(ref:limeExplLocalModelTitanicDesc1)" width="60%" />
<p class="caption">(\#fig:limeExplLocalModelTitanic1)(ref:limeExplLocalModelTitanicDesc1)</p>
</div>
--->
</div>
<div id="LIMERcodeiml" class="section level3" number="9.6.3">
<h3><span class="header-section-number">9.6.3</span> The <code>iml</code> package</h3>
<p>The key functions of the <code>iml</code> package are <code>Predictor$new()</code>, which creates an explainer, and <code>LocalModel$new()</code>, which develops the local glass-box model. The main arguments of the <code>Predictor$new()</code> function are <code>model</code>, which specifies the model-object, and <code>data</code>, the data frame used for fitting the model. As mentioned earlier, we will apply the <code>predict_surrogate()</code> function from the <code>DALEXtra</code> package to access the functions via an interface that is consistent with the approach used in the previous chapters. To choose the <code>iml</code>-implementation of LIME, we set argument <code>type="iml"</code> (see Section <a href="LIME.html#LIMERcodelime">9.6.1</a>). In that case, the method accepts, apart from the required arguments <code>explainer</code>and <code>new_observation</code>, an additional argument <code>k</code> that specifies the number of explanatory variables included in the local-approximation model.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="LIME.html#cb15-1"></a><span class="kw">library</span>(<span class="st">&quot;DALEXtra&quot;</span>)</span>
<span id="cb15-2"><a href="LIME.html#cb15-2"></a><span class="kw">library</span>(<span class="st">&quot;iml&quot;</span>)</span>
<span id="cb15-3"><a href="LIME.html#cb15-3"></a>iml_johnny &lt;-<span class="st"> </span><span class="kw">predict_surrogate</span>(<span class="dt">explainer =</span> titanic_rf_exp, </span>
<span id="cb15-4"><a href="LIME.html#cb15-4"></a>                  <span class="dt">new_observation =</span> johnny_d, </span>
<span id="cb15-5"><a href="LIME.html#cb15-5"></a>                  <span class="dt">k =</span> <span class="dv">3</span>, </span>
<span id="cb15-6"><a href="LIME.html#cb15-6"></a>                  <span class="dt">type =</span> <span class="st">&quot;iml&quot;</span>)</span></code></pre></div>
<p>The resulting object includes data frame <code>results</code> with seven variables that provides results of the LASSO logistic regression model which is used to approximate the predictions of the random forest model. For brevity, we print out selected variables.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="LIME.html#cb16-1"></a>iml_johnny<span class="op">$</span>results[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">7</span>)]</span></code></pre></div>
<pre><code>##            beta x.recoded      effect x.original     feature .class
## 1 -0.1992616770         1 -0.19926168        1st   class=1st     no
## 2  1.6005493672         1  1.60054937       male gender=male     no
## 3 -0.0002111346        72 -0.01520169         72        fare     no
## 4  0.1992616770         1  0.19926168        1st   class=1st    yes
## 5 -1.6005493672         1 -1.60054937       male gender=male    yes
## 6  0.0002111346        72  0.01520169         72        fare    yes</code></pre>
<p>The printed output includes column <code>beta</code> that provides the estimated coefficients of the local-approximation model. Note that two sets of three coefficients (six in total) are given, corresponding to the prediction of the probability of death (column <code>.class</code> assuming value <code>no</code>, corresponding to the value <code>"no"</code> of the <code>survived</code> dependent-variable) and survival (<code>.class</code> asuming value <code>yes</code>). Column <code>x.recoded</code> contains the information about the value of the corresponding transformed (interpretable) variable. The value of the original explanatory variable is given in column <code>x.original</code>, with column <code>feature</code> providing the information about the corresponding variable. Note that the implemented version of LIME does not transform continuous variables. Categorical variables are dichotomized, with the resulting binary variable assuming the value of 1 for the category observed for the instance of interest and 0 for other categories.</p>
<p>The <code>effect</code> column provides the product of the estimated coefficient (from column <code>beta</code>) and the value of the interpretable covariate (from column <code>x.recoded</code>) of the model approximating the random forest model.</p>
<!---
Interestingly, unlike in the case of the results obtained for the `lime` and `localModel` packages, it appears that *age* is not included in the list of important explanatory variables. The ceteris-paribus profile for *age* and Johny D, presented in Figure \@ref(fig:LIMEexample02), indicates that, for boys younger than 15-years of age, the predicted probability of survival does not change very much with age. Hence, given that *age* was used as a continuous variable in the model, it does not appear as an important variable.  

#Interpretation method:  LocalModel 
#
#Analysed predictor: 
#Prediction task: unknown 
#
#Analysed data:
#Sampling from data.frame with 2207 rows and 7 columns.
#
#Head of results:
#          beta x.recoded     effect  x.original              feature
#1 -0.158368701         1 -0.1583687         1st            class=1st
#2  1.739826204         1  1.7398262        male          gender=male
#3  0.018515945         0  0.0000000           0                sibsp
#4 -0.001484918        72 -0.1069141          72                 fare
#5  0.131819869         1  0.1318199 Southampton embarked=Southampton
#6  0.158368701         1  0.1583687         1st            class=1st
--->
<p>By applying the generic <code>plot()</code> function to the object containing the LIME-method results, we obtain a graphical representation of the results.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="LIME.html#cb18-1"></a><span class="kw">plot</span>(iml_johnny) </span></code></pre></div>
<p>The resulting plot is shown in Figure <a href="LIME.html#fig:limeExplIMLTitanic">9.9</a>. It shows values of the two sets of three coefficients for both types of predictions (probability of death and survival).</p>

<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="LIME.html#cb19-1"></a><span class="kw">plot</span>(iml_johnny) </span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:limeExplIMLTitanic"></span>
<img src="ema_files/figure-html/limeExplIMLTitanic-1.png" alt="Illustration of the LIME-method results for the prediction for Johnny D for the random forest model titanic_rf and the Titanic data, generated by the iml package." width="80%" />
<p class="caption">
Figure 9.9: Illustration of the LIME-method results for the prediction for Johnny D for the random forest model <code>titanic_rf</code> and the Titanic data, generated by the <code>iml</code> package.
</p>
</div>
<p>It is worth noting that <em>age</em>, <em>gender</em>, and <em>class</em> are correlated. For instance, crew members are only adults and mainly men. This is probably the reason why the three packages implementing the LIME method generate slightly different explanations for the model prediction for Johnny D.</p>
</div>
</div>
<div id="LIMEPythoncode" class="section level2" number="9.7">
<h2><span class="header-section-number">9.7</span> Code snippets for Python</h2>
<p>In this section, we use the <code>lime</code> library for Python, which is probably the most popular implementation of the LIME method <span class="citation">(Ribeiro, Singh, and Guestrin <a href="#ref-lime" role="doc-biblioref">2016</a>)</span>. The <code>lime</code> library requires categorical variables to be encoded in a numerical format. This requires some additional work with the data. Therefore, below we will show you how to use this method in Python step by step.</p>
<p>For illustration purposes, we use the random forest model for the Titanic data. Instance-level explanations are calculated for Henry, a 47-year-old passenger that travelled in the 1st class.</p>
<p>In the first step, we read the Titanic data and encode categorical variables. In this case, we use the simplest encoding for <em>gender</em>, <em>class</em>, and <em>embarked</em>, i.e., the label-encoding.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="LIME.html#cb20-1"></a><span class="im">import</span> dalex <span class="im">as</span> dx</span>
<span id="cb20-2"><a href="LIME.html#cb20-2"></a></span>
<span id="cb20-3"><a href="LIME.html#cb20-3"></a>titanic <span class="op">=</span> dx.datasets.load_titanic()</span>
<span id="cb20-4"><a href="LIME.html#cb20-4"></a>X <span class="op">=</span> titanic.drop(columns<span class="op">=</span><span class="st">&#39;survived&#39;</span>)</span>
<span id="cb20-5"><a href="LIME.html#cb20-5"></a>y <span class="op">=</span> titanic.survived</span>
<span id="cb20-6"><a href="LIME.html#cb20-6"></a></span>
<span id="cb20-7"><a href="LIME.html#cb20-7"></a><span class="im">from</span> sklearn <span class="im">import</span> preprocessing</span>
<span id="cb20-8"><a href="LIME.html#cb20-8"></a>le <span class="op">=</span> preprocessing.LabelEncoder()</span>
<span id="cb20-9"><a href="LIME.html#cb20-9"></a></span>
<span id="cb20-10"><a href="LIME.html#cb20-10"></a>X[<span class="st">&#39;gender&#39;</span>]   <span class="op">=</span> le.fit_transform(X[<span class="st">&#39;gender&#39;</span>])</span>
<span id="cb20-11"><a href="LIME.html#cb20-11"></a>X[<span class="st">&#39;class&#39;</span>]    <span class="op">=</span> le.fit_transform(X[<span class="st">&#39;class&#39;</span>])</span>
<span id="cb20-12"><a href="LIME.html#cb20-12"></a>X[<span class="st">&#39;embarked&#39;</span>] <span class="op">=</span> le.fit_transform(X[<span class="st">&#39;embarked&#39;</span>])</span></code></pre></div>
<p>In the next step we train a random forest model.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="LIME.html#cb21-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier <span class="im">as</span> rfc</span>
<span id="cb21-2"><a href="LIME.html#cb21-2"></a>titanic_fr <span class="op">=</span> rfc()</span>
<span id="cb21-3"><a href="LIME.html#cb21-3"></a>titanic_fr.fit(X, y)</span></code></pre></div>
<p>It is time to define the observation for which model prediction will be explained. We write Henry’s data into <code>pandas.Series</code> object.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="LIME.html#cb22-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb22-2"><a href="LIME.html#cb22-2"></a>henry <span class="op">=</span> pd.Series([<span class="dv">1</span>, <span class="fl">47.0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">25.0</span>, <span class="dv">0</span>, <span class="dv">0</span>], </span>
<span id="cb22-3"><a href="LIME.html#cb22-3"></a>                  index <span class="op">=</span>[<span class="st">&#39;gender&#39;</span>, <span class="st">&#39;age&#39;</span>, <span class="st">&#39;class&#39;</span>, <span class="st">&#39;embarked&#39;</span>,</span>
<span id="cb22-4"><a href="LIME.html#cb22-4"></a>                          <span class="st">&#39;fare&#39;</span>, <span class="st">&#39;sibsp&#39;</span>, <span class="st">&#39;parch&#39;</span>]) </span></code></pre></div>
<p>The <code>lime</code> library explains models that operate on images, text, or tabular data. In the latter case, we have to use the <code>LimeTabularExplainer</code> module.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="LIME.html#cb23-1"></a><span class="im">from</span> lime.lime_tabular <span class="im">import</span> LimeTabularExplainer </span>
<span id="cb23-2"><a href="LIME.html#cb23-2"></a>explainer <span class="op">=</span> LimeTabularExplainer(X, </span>
<span id="cb23-3"><a href="LIME.html#cb23-3"></a>                      feature_names<span class="op">=</span>X.columns, </span>
<span id="cb23-4"><a href="LIME.html#cb23-4"></a>                      class_names<span class="op">=</span>[<span class="st">&#39;died&#39;</span>, <span class="st">&#39;survived&#39;</span>], </span>
<span id="cb23-5"><a href="LIME.html#cb23-5"></a>                      discretize_continuous<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb23-6"><a href="LIME.html#cb23-6"></a>                      verbose<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>The result is an explainer that can be used to interpret a model around specific observations.
In the following example, we explain the behaviour of the model for Henry.
The <code>explain_instance()</code> method finds a local approximation with an interpretable linear model.
The result can be presented graphically with the <code>show_in_notebook()</code> method.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="LIME.html#cb24-1"></a>lime <span class="op">=</span> explainer.explain_instance(henry, titanic_fr.predict_proba)</span>
<span id="cb24-2"><a href="LIME.html#cb24-2"></a>lime.show_in_notebook(show_table<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>The resulting plot is shown in Figure <a href="LIME.html#fig:limePython1">9.10</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:limePython1"></span>
<img src="figure/lime_python_1.png" alt="A plot of LIME model values for the random forest model and passenger Henry for the Titanic data." width="100%" />
<p class="caption">
Figure 9.10: A plot of LIME model values for the random forest model and passenger Henry for the Titanic data.
</p>
</div>
<!--
To calculate LIME surrogate we use the `predict_surrogate()` method which is a wrapper to the `lime` library. The first argument indicates the data observation for which the values are to be calculated. All other arguments are passed to the `explain` function in the `lime` library. As a result we get an explanation.


```python
import pandas as pd
henry = pd.DataFrame({'gender'   : ['male'],
                       'age'     : [47],
                       'class'   : ['1st'],
                       'embarked': ['Cherbourg'],
                       'fare'    : [25],
                       'sibsp'   : [0],
                       'parch'   : [0]},
                      index = ['Henry'])

import dalex as dx
titanic_rf_exp = dx.Explainer(titanic_rf, X, y, 
                  label = "Titanic RF Pipeline")
                  
                  
print(titanic.survived)
lime_henry = titanic_rf_exp.predict_surrogate(henry,
                 feature_names=['age', 'fare', 'parch', 'sibsp', 
                     'gender', 'class', 'embarked'],
                 class_names=['0','1'],
                 discretize_continuous=True,
                 num_features=4,
                 top_labels=1)
lime_john
```

TODO: pokazać wynik LIMEa

To visualize the obtained values, we simply call the `show_in_notebook()` method from `lime` library.


```python
bd_henry.show_in_notebook()
```

The resulting plot is shown in Figure \@ref(fig:limePython2). 

(ref:limePython2Desc) A plot of LIME model values for the random forest model and passenger Henry for the Titanic data. 

<div class="figure" style="text-align: center">
<img src="figure/shap_python_1.png" alt="(ref:limePython2Desc)" width="100%" />
<p class="caption">(\#fig:limePython2)(ref:limePython2Desc)</p>
</div>
-->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-LIMESHAPstability">
<p>Alvarez-Melis, David, and Tommi S. Jaakkola. 2018. “On the Robustness of Interpretability Methods.” <em>ICML Workshop on Human Interpretability in Machine Learning (WHI 2018)</em>, June. <a href="http://arxiv.org/abs/1806.08049">http://arxiv.org/abs/1806.08049</a>.</p>
</div>
<div id="ref-CARTtree">
<p>Breiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. <em>Classification and Regression Trees</em>. Monterey, CA: Wadsworth; Brooks.</p>
</div>
<div id="ref-ImageNet">
<p>Deng, J., W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. 2009. “ImageNet: A large-scale hierarchical image database.” In <em>2009 Ieee Conference on Computer Vision and Pattern Recognition</em>, 248–55. Los Alamitos, CA, USA: IEEE Computer Society. <a href="https://doi.org/10.1109/cvpr.2009.5206848">https://doi.org/10.1109/cvpr.2009.5206848</a>.</p>
</div>
<div id="ref-party2006">
<p>Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. “Unbiased Recursive Partitioning: A Conditional Inference Framework.” <em>Journal of Computational and Graphical Statistics</em> 15 (3): 651–74.</p>
</div>
<div id="ref-shapPackage">
<p>Lundberg, Scott. 2019. <em>SHAP (SHapley Additive exPlanations)</em>. <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>.</p>
</div>
<div id="ref-molnar2019">
<p>Molnar, Christoph. 2019. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>.</p>
</div>
<div id="ref-imlRPackage">
<p>Molnar, Christoph, Bernd Bischl, and Giuseppe Casalicchio. 2018. “iml: An R package for Interpretable Machine Learning.” <em>Journal of Open Source Software</em> 3 (26): 786. <a href="https://doi.org/10.21105/joss.00786">https://doi.org/10.21105/joss.00786</a>.</p>
</div>
<div id="ref-limePackage">
<p>Pedersen, Thomas Lin, and Michaël Benesty. 2019. <em>lime: Local Interpretable Model-Agnostic Explanations</em>. <a href="https://CRAN.R-project.org/package=lime">https://CRAN.R-project.org/package=lime</a>.</p>
</div>
<div id="ref-lime">
<p>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “"Why should I trust you?": Explaining the Predictions of Any Classifier.” In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Kdd San Francisco, ca</em>, 1135–44. New York, NY: Association for Computing Machinery.</p>
</div>
<div id="ref-Simonyan15">
<p>Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” In <em>International Conference on Learning Representations</em>. San Diego, CA: ICLR 2015.</p>
</div>
<div id="ref-localModelPackage">
<p>Staniak, Mateusz, Przemyslaw Biecek, Krystian Igras, and Alicja Gosiewska. 2019. <em>localModel: LIME-Based Explanations with Interpretable Inputs Based on Ceteris Paribus Profiles</em>. <a href="https://CRAN.R-project.org/package=localModel">https://CRAN.R-project.org/package=localModel</a>.</p>
</div>
<div id="ref-Tibshirani94regressionshrinkage">
<p>Tibshirani, Robert. 1994. “Regression Shrinkage and Selection via the lasso.” <em>Journal of the Royal Statistical Society, Series B</em> 58: 267–88.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shapley.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ceterisParibus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ema.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
