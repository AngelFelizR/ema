[
["index.html", "Predictive Models: Visualisation, Exploration and Explanation Chapter 1 Introduction 1.1 Model Lifecycle 1.2 Why do we need model explainers? 1.3 Black-box models vs White-box models 1.4 Model agnostic vs Model specific 1.5 Glossary / Notation 1.6 Thanks to", " Predictive Models: Visualisation, Exploration and Explanation Przemyslaw Biecek and Tomasz Burzykowski 2018-09-18 Chapter 1 Introduction Machine Learning (ML) models have a wide range of applications in classification or regression problems. Due to the increasing computational power of computers and complexity of data sources, ML models are becoming more and more sophisticated. Models created with the use of techniques such as boosting or bagging of neural networks are parametrized by thousands of coefficients. They are obscure; it is hard to trace the link between input variables and model outcomes - in fact they are treated as black boxes. They are used because of their elasticity and high performance, but their deficiency in interpretability is one of their weakest sides. In many applications we need to know, understand or prove how the input variables are used in the model. We need to know the impact of particular variables on the final model predictions. Thus we need tools that extract useful information from thousands of model parameters. This book is about We present techniques to examine particular predictions from ML models. In this book you will find theory and examples that explains model locally like break down, ceteris paribus, LIME or Shapley. We present techniques to examine fully trained ML models as a whole. In this book you will find theory and examples that explains model globally like Partial Dependency Plots, Variable Importance Plots and others. We present tools and methods for model comparison. This book is NOT about. We do not focus on any specific model. Presented techniques are model agnostic and do not have any assumptions related to model structure. We do not focus on the data exploration. There are very good books and techniques related to this, like R for Data Science http://r4ds.had.co.nz/ or TODO We do not focus on the process of model building. There are also very good books about this, see An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani http://www-bcf.usc.edu/~gareth/ISL/ or TODO We do not focus on particular tools for model building, see Applied Predictive Modeling By Max Kuhn and Kjell Johnson http://appliedpredictivemodeling.com/ 1.1 Model Lifecycle Workflow of a typical machine learning modeling. A) Modeling is a process in which domain knowledge and data are turned into models. B) Models are used to generate predictions. C) Understanding of a model structure may increase our knowledge, and in consequence it may lead to a better model. DALEX helps here. D) Understanding of drivers behind a particular model’s predictions may help to correct wrong decisions, and in consequence it leads to a better model. DALEX helps here. Variable importance Model response as a function of a variable Model performance / diagnostic / validation 1.2 Why do we need model explainers? AutoML Feature Extraction Model Improvement 1.3 Black-box models vs White-box models (P. Biecek 2018c) 1.4 Model agnostic vs Model specific 1.5 Glossary / Notation feature / variable Let \\(f_{M}(x): \\mathcal R^{d} \\rightarrow \\mathcal R\\) denote a predictive model, i.e. function that takes \\(d\\) dimensional vector and calculate numerical score. In section in which we work with larger number of models we use subscript \\(M\\) to index models. But to simplify notation, this subscript is omitted if profiles for only one model are considered. Symbol \\(x \\in \\mathcal R^d\\) refers to a point in the feature space. We use subscript \\(x_i\\) to refer to a different data points and superscript \\(x^j\\) to refer to specific dimensions. Additionally, let \\(x^{-j}\\) denote all coordinates except \\(j\\)-th and let \\(x|^j=z\\) denote a data point \\(x^*\\) with all coordinates equal to \\(x\\) except coordinate \\(j\\) equal to value \\(z\\). I.e. \\(\\forall_{i \\neq {j}} x^i = x^{*,i}\\) and \\(x^j = z\\). In other words \\(x|^j=z\\) denote a \\(x\\) with \\(j\\)th coordinate changed to \\(z\\). Now we can define Ceteris Paribus Profile for model \\(f\\), variable \\(j\\) and point \\(x\\) as \\[ CP^{f, j, x}(z) := f(x|^j = z). \\] I.e. CP profile is a model response obtained for observations created based on \\(x\\) with \\(j\\) coordinated changes and all other coordinates kept unchanged. It is convenient to use an alternative name for this plot: What-If Plots. CP profiles show what would happen if only a single variable is changed. Figure 5.1 shows an example of Ceteris Paribus profile. The black dot stands for prediction for a single observation. Grey line show how the model response would change if in this single observation coordinate surface will be changes to selected value. From this profile one may read that the model response is non monotonic. If construction.year for this observation would be below 1935 the model response would be higher, but if construction year were between 1935 and 1995 the model response would be lower. 1.6 Thanks to We are using the bookdown package (Xie 2018) in this sample book Chris Drake and Janusz Holyst References "],
["prediction-level-explanations.html", "Prediction level explanations", " Prediction level explanations "],
["PredictionExplainers.html", "Chapter 2 Introduction 2.1 Variable atribution vs What-if analysis 2.2 When to use? 2.3 A bit of philosophy: Three Laws for Prediction Level Explanations", " Chapter 2 Introduction Prediction level explainers help to understand how the model works for a single prediction. This is the main difference from the model level explainers that were focused on the model in general. Prediction level explainers are always in context of a single observation. Think about following use-cases One wants to attribute effects of variables to a model predictions. Think about model for hart attack. Having a final score for a patient one wants to understand how much of this score come from smoking or age or gender. One wants to understand how the model response would change if some inputs are changed. Think about model for hart attack. How the model response would change if a patient cuts the number of smoked cigarettes by half. Model is not working correctly for a particular point and one wants to understand why predictions for this point are wrong. 2.1 Variable atribution vs What-if analysis There are many different tools that may be used to explore model around a single data point and in following sections we will describe the most popular approaches. They can be divided into two classes. Analysis of the model curvature. Here we treat the model as a function and we are interested in the curvature of this function around the point of interest (see Figure 2.1). In section 7 we present the LIME method that approximates the black-box model in a point of interest while in section 8 we present Ceteris Paribus profiles that are more focused on conditional changes of model response given only one coordinate is modified. Analysis of the probabilistic behavior of the model. Here we are interested in decomposition of the model response to parts that can be attributed to particular features. Figure 2.1: (fig:modelResponseCurve) Model response surface. We are interested in understanding the model behavior in a single point 2.2 When to use? There are several use-cases for such explainers. Think about following. Model improvement. If model works particular bad for a selected observation (the residual is very high) then investigation of model responses for miss fitted points may give some hints how to improve the model. For individual predictions it is easier to notice that selected variable should have different a effect. Additional domain specific validation. Understanding which factors are important for model predictions helps to be critical about model response. If model contributions are against domain knowledge then we may be more skeptical and willing to try another model. On the other hand, if the model response is aligned with domain knowledge we may trust more in these responses. Such trust is important in decisions that may lead to serious consequences like predictive models in medicine. Model selection. Having multiple candidate models one may select the final response based on model explanations. Even if one model is better in terms of global model performance it may happen that locally other model is better fitted. This moves us towards model consultations that identify different options and allow human to select one of them. 2.3 A bit of philosophy: Three Laws for Prediction Level Explanations 76 years ago Isaac Asimov devised Three Laws of Robotics: 1) a robot may not injure a human being, 2) a robot must obey the orders given it by human beings and 3) A robot must protect its own existence. These laws impact discussion around Ethics of AI. Today’s robots, like cleaning robots, robotic pets or autonomous cars are far from being conscious enough to be under Asimov’s ethics. Today we are surrounded by complex predictive algorithms used for decision making. Machine learning models are used in health care, politics, education, judiciary and many other areas. Black box predictive models have far larger influence on our lives than physical robots. Yet, applications of such models are left unregulated despite many examples of their potential harmfulness. See Weapons of Math Destruction by Cathy O’Neil for an excellent overview of potential problems. It’s clear that we need to control algorithms that may affect us. Such control is in our civic rights. Here we propose three requirements that any predictive model should fulfill. Prediction’s justifications. For every prediction of a model one should be able to understand which variables affect the prediction and how strongly. Variable attribution to final prediction. Prediction’s speculations. For every prediction of a model one should be able to understand how the model prediction would change if input variables were changed. Hypothesizing about what-if scenarios. Prediction’s validations For every prediction of a model one should be able to verify how strong are evidences that confirm this particular prediction. There are two ways to comply with these requirements. One is to use only models that fulfill these conditions by design. White-box models like linear regression or decision trees. In many cases the price for transparency is lower performance. The other way is to use approximated explainers – techniques that find only approximated answers, but work for any black box model. Here we present such techniques. "],
["variable-attribution.html", "Chapter 3 Variable attribution 3.1 Variable attribution for linear models 3.2 Model agnostic variable attribution", " Chapter 3 Variable attribution In this section we introduce method for additive decomposition of predictions. The main goal for these tools is to help understand how model output may be attributed to input variables or sets of variables. Presented explainers are linked with the first law introduced in Section 2.3, i.e. law for prediction’s justifications. Note that there are more tools for variable attribution, some of them will be presented in next sections. Think of following use cases: Think about a model for heart attack. A patient wants to know which factors have highest impact on the final heart risk score. Think about a model for apartment prices. An investor wants to know how much of the final price may be attributed to the location of an apartment. Think about a model for credit scoring. A customer wants to know if factors like gender, age or number of kids influence model decisions. In the section 3.1 we will introduce key concepts and intuitions beyond variable attribution based on linear models. This approach may be easily applied to additive models and generalized linear models. In sections 4 and 6 we will present model agnostic extentions of this concept. 3.1 Variable attribution for linear models Linear model with coefficients \\(\\beta = (\\beta_0, \\beta_1, .., \\beta_p)\\) has following form. \\[ f(x) = \\beta_0 + x_1 \\beta_1 + \\ldots + x_p \\beta_p. \\] In other words, model response is the sum of weighted elements of \\(x = (x_1, x_2, \\ldots, x_p)\\). From a global perspective of a model, we are usually interested in questions like, how good is the model, which variables are significant or how accurate are model predictions. But in this chapter we are focues in a local perspective, i.e. for a single observation \\(x^*\\) how to measure the contribution of a variable \\(x_i\\) on model prediction \\(f(x^*)\\). Let \\(v(f, x^*, i)\\) stands for the contribution of variable \\(x_i\\) on prediction of model \\(f()\\) in point \\(x^*\\). For linear models it is easy to define such contribution as \\[ v(f, x^*, i) = f(x^*) - E[f(x)|x_{-1} = x^*_{-1}] = \\beta_i x^*_i - E \\beta_i X_i \\] where the expected value can be estimated from the data \\[ v(f, x^*, i) = \\beta_i x^*_i - \\beta_i \\bar x_i = \\beta_i (x^*_i - \\bar x_i) \\] The logic behind the attribution is the following. Contribution of variable \\(x_i\\) is the difference between model response for value \\(x_i^*\\) minus the average model response. 3.1.1 Wine quality example It may be a surprise, that the attribution for variable \\(x_i\\) is not the \\(\\beta_i x_i\\). But, please consider following example. Figure 3.1 shows the relation between alcohol and wine quality, based on the wine dataset (Cortez et al. 2009). The corresponding linear model is \\[ quality(alcohol) = 2.5820 + 0.3135 * alcohol \\] The weakest wine in this dataset has 8% of alcohol, average alcohol concentration is 10.51, so the contribution of alcohol to the model prediction is \\(0.3135 *(8-10.51) = -0.786885\\). It means that low value of alcohol for this wine (8%) lower the prediction of quality by \\(-0.786885\\). Note, that it would be confusing to forget about normalisation and say, that for the alcohol contribution on quality is \\(0.3135*8 = 2.508\\) as this is high positive value. Figure 3.1: (fig:attribution1)Relation between wine quality and concentration of alcohol assessed with linear model Note that the linear model ma be rewritten in a following way \\[ f(x) = baseline + (x_1 - \\bar x_1) \\beta_1 + ... + (x_p - \\bar x_p) \\beta_p \\] where \\[ baseline = \\mu + \\bar x_1 \\beta_1 + ... + \\bar x_p \\beta_p. \\] Here \\(baseline\\) is an average model response and variable contributions show how prediction for particular \\(x^*\\) is different from the average response. ** NOTE for careful readers ** There is a gap between expected value of \\(X_i\\) and average calculated on some dataset \\(\\bar x_i\\). The latter depends on the data used for calculation of averages. For the sake of simplicity we do not emphasise these differences. To live with this just assume that we have access to a very large validation data that allows us to calculate \\(\\bar x_i\\) very accurately. 3.2 Model agnostic variable attribution In the Section 3.1 we introduced a method for calculation of variable attributions for linear models. This method is accurate, based directly on the structure of the model. But for most popular machine learning models we cannot assume that they are linear nor even additive. In next sections we introduce a model agnostic approach. Note that even if the model itself is not additive, the model attribution will be additive. Again, let \\(v(f, x^*, i)\\) stands for the contribution of variable \\(x_i\\) on prediction of model \\(f()\\) in point \\(x^*\\). We expect that such contribution will sum up to the model prediction in a given point (property called local accuracy), so \\[ f(x^*) = baseline + \\sum_{i=1}^p v(f, x^*, i) \\] where \\(baseline\\) stands for average model response. Note that the equation above may be rewritten as \\[ E [f(X)|X_1 = x_1^*, \\ldots, X+p = x_p^*] = E[f(X)] + \\sum_{i=1}^p v(f, x^*, i) \\] what leads to quite natural proposition for \\(v(f, x^*_i, i)\\), such as \\[ v(f, x^*_i, i) = E [f(X) | X_1 = x_1^*, \\ldots, X_i = x_i^*] - E [f(X) | X_1 = x_1^*, \\ldots, X_{i-1} = x_{i-1}^*] \\] In other words the contribution of variable \\(i\\) is the difference between expected model response conditioned on first \\(i\\) variables minus the model response conditioned on first \\(i-1\\) variables. Such proposition fulfills the local accuracy condition, but unfortunatelly variable contributions depends on the ordering of variables. Figure 3.2: (fig:ordering) Two different paths between average model prediction and the model prediction for a selected observation. Black dots stand for conditional average, red arrows stands for changes between conditional averages. See for example Figure 3.2. In the first ordering the contribution of variable age is calculated as 0.01, while in the second the contribution is calculated as 0.13. Such differences are related to the lack of additivness of the model \\(f()\\). Propositions presented in next two sections present different solutions for this problem. References "],
["breakDown.html", "Chapter 4 Break Down 4.1 The Algorithm 4.2 HR dataset: Hire or Fire? 4.3 Break Down Plots 4.4 Pros and cons 4.5 Code snippets for R", " Chapter 4 Break Down The approach for variable attribution presented in the Section 3.2 has the property of local accuracy, but variable contributions depends on the variable ordering. The Break Down method solves this problem by using two-step procedure. In the first step variables are ordered and in the second step the consecutive conditioning is applied to ordered variables. 4.1 The Algorithm First step of this algorithm is to determine the order of variables for conditioning. It seems to be reasonable to include first variables that are likely to be most important, leaving the noise variables at the end. This leads to order based on following scores \\[ score(f, x^*, i) = \\left| E [f(X)] - E [f(X)|X_i = x^*_i] \\right| \\] Note, that the absolute value is needed as variable contributions can be both positive and negative. Once the ordering is determined in the second step variable contributions are calculated as \\[ v(f, x^*_i, i) = E [f(X) | X_{I \\cup \\{i\\}} = x_{I \\cup \\{i\\}}^*] - E [f(X) | X_{I} = x_{I}^*] \\] where \\(I\\) is the set of variables that have scores smaller than score for variable \\(i\\). \\[ I = \\{j: score(f, x^*, j) &lt; score(f, x^*, i)\\} \\] The time complexity of the first step id \\(O(p)\\) where \\(p\\) is the number of variables and the time complexity of the second step is also \\(O(p)\\). 4.2 HR dataset: Hire or Fire? Let us consider a random forest model created for HR data. The average model response is \\(\\bar f(x) = 0.385586\\). For a selected observation \\(x^*\\) the table below presents scores for particular variables. Ei f(X) scorei hours 0.616200 0.230614 salary 0.225528 0.160058 evaluation 0.430994 0.045408 age 0.364258 0.021328 gender 0.391060 0.005474 Once we determine the order we can calculate sequential contributions variable cumulative contribution (Intercept) 0.385586 0.385586 * hours = 42 0.616200 0.230614 * salary = 2 0.400206 -0.215994 * evaluation = 2 0.405776 0.005570 * age = 58 0.497314 0.091538 * gender = male 0.778000 0.280686 final_prognosis 0.778000 0.778000 4.3 Break Down Plots Once we calculated variable attributions we may plot them in an intuitive form. This intuition behind Break Down Plots is described in Figure ??. The variable ordering determined in the first step of Break Down Algorithm is reflected by the ordering of variables in rows of the plot. The last row of a plot shows the \\(baseline\\), i.e. an average model prediction. The next row corresponds to average model prediction for observations with variable surface fixed to value 35. The next for corresponds to average model prediction with variables surface set to 35 and floor set to 1, and so on. The first row corresponds to model response for \\(x^*\\). In panels A and B violines show distribution of model predictions for selected points, while red dots stands for averages. The most minimal form that shows important information is presented in the panel C. Positive values are presented with green bars while negative differences are marked with yellow bar. They sum up to final model prediction, which is denoted by a grey bar in this example. Figure 4.1: (fig:BDPrice4) Break Down Plots show how variables move the model prediction from population average to the model prognosis for a single observation. A) The last row shows distribution of model predictions. Next rows show conditional distributions, every row a new variable is added to conditioning. The first row shows model prediction for a single point. Red dots stand for averages. B) Blue arrows shows how the average conditional response change, these values are variables contributions. C) Only variable contributions are presented. 4.4 Pros and cons Break Down approach is model agnostic, can be applied to any predictive model that returns a single number. It leads to additive variable attribution. Below we summarize key strengths and weaknesses of this approach. Pros Break Down Plots are easy to understand and decipher. Break Down Plots are compact; many variables may be presented in a small space. Break Down Plots are model agnostic yet they reduce to intuitive interpretation for linear Gaussian and generalized models. Complexity of Break Down Algorithm is linear in respect to the number of variables. Cons If the model is non-additive then showing only additive contributions may be misleading. Selection of the ordering based on scores is subjective. Different orderings may lead to different contributions. For large number of variables the Break Down Plot may be messy with many variables having small contributions. 4.5 Code snippets for R In this section we present key features of the breakDown package for R (P. Biecek 2018a). This package covers all features presented in this chapter. It is available on CRAN and GitHub. Find more examples at the website of this package https://pbiecek.github.io/breakDown/. Model preparation In this section we will present an example based on the HR dataset and Random Forest model (Breiman et al. 2018). See the Section 15.1 for more details. library(&quot;DALEX&quot;) library(&quot;randomForest&quot;) model &lt;- randomForest(status ~ gender + age + hours + evaluation + salary, data = HR) model ## ## Call: ## randomForest(formula = status ~ gender + age + hours + evaluation + salary, data = HR) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 27.28% ## Confusion matrix: ## fired ok promoted class.error ## fired 2265 393 197 0.2066550 ## ok 528 1255 438 0.4349392 ## promoted 201 384 2186 0.2111151 Model exploration with the breakDown package is performed in three steps. 1. Create an explainer - wrapper around model and validation data. Since all other functions work in a model agnostic fashion, first we need to define a wrapper around the model. Here we are using the explain() function from DALEX package (P. Biecek 2018c). explainer_rf_fired &lt;- explain(model, data = HR, y = HR$status == &quot;fired&quot;, predict_function = function(m,x) predict(m,x, type = &quot;prob&quot;)[,1], label = &quot;fired&quot;) 2. Select an observation of interest. Break Down Plots decompose model prediction around a single observation. Let’s construct a data frame with corresponding values. new_observation &lt;- data.frame(gender = factor(&quot;male&quot;, levels = c(&quot;male&quot;, &quot;female&quot;)), age = 57.7, hours = 42.3, evaluation = 2, salary = 2) predict(model, new_observation, type = &quot;prob&quot;) ## fired ok promoted ## 1 0.782 0.214 0.004 ## attr(,&quot;class&quot;) ## [1] &quot;matrix&quot; &quot;votes&quot; 3. Calculate Break Down decomposition The break_down() function calculates Break Down contributions for a selected model around a selected observation. The result from break_down() function is a data frame with variable attributions. library(&quot;breakDown&quot;) bd_rf &lt;- break_down(explainer_rf_fired, new_observation, check_interactions = FALSE, keep_distributions = TRUE) bd_rf ## contribution ## (Intercept) 0.375 ## * hours = 42 0.238 ## * salary = 2 -0.211 ## * evaluation = 2 0.014 ## * age = 58 0.092 ## * gender = male 0.273 ## final_prognosis 0.782 ## baseline: 0 The generic plot() function creates a Break Down plots. plot(bd_rf) Add the plot_distributions = TRUE argument to enrich model response with additional information. plot(bd_rf, plot_distributions = TRUE) References "],
["break-down-for-interactions.html", "Chapter 5 Break Down for Interactions 5.1 The Algorithm 5.2 HR dataset: Hire or Fire? 5.3 Break Down Plots 5.4 Pros and cons 5.5 Code snippets for R", " Chapter 5 Break Down for Interactions In the Section 4 we presented model agnostic approach for additive decomposition of a model prediction for a single observation. For non-additive models the variables contributions depend on values of other variables. In this section we present an algorithm that identifies interactions between pairs of variables and include such interactions in variable decomposition plots. Here we present an algorithm for pairs of variables, but it can be easily generalized to larger number of variables. 5.1 The Algorithm This algorithm is also composed out of two steps. In the first step variables and pairs of variables are ordered in terms of their importance, while in the second step the consecutive conditioning is applied to ordered variables. To determine an importance of variables and pairs of variables following scores are being calculated. For a single variable \\[ score_1(f, x^*, i) = \\left| E [f(X)|X_i = x^*_i] - E [f(X)]\\right| \\] For pairs of variables \\[ score_2(f, x^*, (i,j)) = \\left| E [f(X)|X_i = x^*_i, X_j = x^*_j] - E [f(X)|X_i = x^*_i] - E [f(X)| X_j = x^*_j]+ E [f(X)] \\right| \\] Note that this is equivalent to \\[ score_2(f, x^*, (i,j)) = \\left| E [f(X)|X_i = x^*_i, X_j = x^*_j] - score_1 (f, x^*, i) - score_1 (f, x^*, j) + baseline \\right| \\] In other words the \\(score_1(f, x^*, i)\\) measures how much the average model response changes if variable \\(x_i\\) is set to \\(x_i^*\\), which is some index of local variable importance. On the other hand the \\(score_2(f, x^*, (i,j))\\) measures how much the change is different than additive composition of changes for \\(x_i\\) and \\(x_j\\), which is some index of local interaction importance. Note, that for additive models \\(score_2(f, x^*, (i,j))\\) shall be close to zero. So the larger is this value the larger deviation from additivness. The second step of the algorithm is the sequential conditioning. In this version in every new step we condition on a single variable of pair of variables in an order determined by \\(score_1\\) and \\(score_2\\). The complexity of the first step id \\(O(p^2)\\) where \\(p\\) stands for the number of variables. The complexity of the second step is \\(O(p)\\). 5.2 HR dataset: Hire or Fire? Again, let us consider a HR dataset. The table below shows \\(score_1\\) and \\(score_2\\) calculated for consecutive variables. Ei f(X) score1 score2 hours 0.616200 0.230614 salary 0.225528 -0.160058 age:gender 0.516392 0.146660 salary:age 0.266226 0.062026 salary:hours 0.400206 -0.055936 evaluation 0.430994 0.045408 hours:age 0.635662 0.040790 salary:evaluation 0.238126 -0.032810 age 0.364258 -0.021328 evaluation:hours 0.677798 0.016190 salary:gender 0.223292 -0.007710 evaluation:age 0.415688 0.006022 gender 0.391060 0.005474 hours:gender 0.626478 0.004804 evaluation:gender 0.433814 -0.002654 Once we determined the order, we can calculate sequential conditionings. In the first step we condition over variable hours, then over salary. The third position is occupied by interaction between age:gender thus we add both variables to the conditioning variable cumulative contribution (Intercept) 0.385586 0.385586 * hours = 42 0.616200 0.230614 * salary = 2 0.400206 -0.215994 * age:gender = 58:male 0.796856 0.396650 * evaluation = 2 0.778000 -0.018856 final_prognosis 0.778000 0.778000 5.3 Break Down Plots Break Down Plots for interactions are similar in structure as plots for single variables. The only difference is that in some rows pair of variable is listed in a single row. See an example in Figure ??. Figure 5.1: (fig:bdInter1) Break Down Plot for variable attrbution with interactions 5.4 Pros and cons Break Down for interactions shares many features of Break Down for single variables. Below we summarize unique strengths and weaknesses of this approach. Pros If interactions are present in the model, then additive contributions may be misleading. In such case the identification of interactions leads to better explanations. Complexity of Break Down Algorithm is quadratic, what is not that bad if number of features is small or moderate. Cons For large number of variables, the consideration of all interactions is both time consuming and sensitive to noise as the number of \\(score_2\\) scores grow faster than number of \\(score_1\\). 5.5 Code snippets for R The algorithm for Break Down for Interactions is also implemented in the break_down function from breakDown package. It is enough to set argument check_interactions = TRUE to identify interactions. Model preparation First a model needs to be trained. library(&quot;DALEX&quot;) library(&quot;randomForest&quot;) model &lt;- randomForest(status ~ gender + age + hours + evaluation + salary, data = HR) model ## ## Call: ## randomForest(formula = status ~ gender + age + hours + evaluation + salary, data = HR) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 27.25% ## Confusion matrix: ## fired ok promoted class.error ## fired 2275 382 198 0.2031524 ## ok 541 1243 437 0.4403422 ## promoted 200 380 2191 0.2093107 Model exploration with the breakDown package is performed in three steps. 1. Create an explainer - wrapper around model and validation data. Since all other functions work in a model agnostic fashion, first we need to define a wrapper around the model. Here we are using the explain() function from DALEX package. explainer_rf_fired &lt;- explain(model, data = HR, y = HR$status == &quot;fired&quot;, predict_function = function(m,x) predict(m,x, type = &quot;prob&quot;)[,1], label = &quot;fired&quot;) 2. Select an observation of interest. Break Down Plots decompose model prediction around a single observation. Let’s construct a data frame with corresponding values. new_observation &lt;- data.frame(gender = factor(&quot;male&quot;, levels = c(&quot;male&quot;, &quot;female&quot;)), age = 57.7, hours = 42.3, evaluation = 2, salary = 2) predict(model, new_observation, type = &quot;prob&quot;) ## fired ok promoted ## 1 0.772 0.226 0.002 ## attr(,&quot;class&quot;) ## [1] &quot;matrix&quot; &quot;votes&quot; 3. Calculate Break Down decomposition The break_down() function calculates Break Down contributions for a selected model around a selected observation. Note that check_interactions = TRUE is needed to identify interactions. The result from break_down() function is a data frame with variable attributions. library(&quot;breakDown&quot;) bd_rf &lt;- break_down(explainer_rf_fired, new_observation, check_interactions = TRUE) bd_rf ## contribution ## (Intercept) 0.375 ## * hours = 42 0.238 ## * salary = 2 -0.206 ## * age:gender = 58:male 0.378 ## * evaluation = 2 -0.013 ## final_prognosis 0.772 ## baseline: 0 The generic plot() function creates a Break Down plots. plot(bd_rf) "],
["shapley.html", "Chapter 6 Shapley Values 6.1 Introduction 6.2 Pros and cons", " Chapter 6 Shapley Values In this section we introduce other, very popular tool for additive variable attribution. As in Break Down the model response is decomposed into parts that may be assigned to variables. Also, in similar spirit to Break Down, this explainers are linked the first law introduced in Section @ref(#PredictionExplainers), i.e. law for prediction’s justifications. This method was first introduced in (Štrumbelj and Kononenko 2014) but the wide adoption comes with a NIPS 2017 paper (Lundberg and Lee 2017) and python library SHAP https://github.com/slundberg/shap. 6.1 Introduction The name Shapley Values comes from the solution in cooperative game theory attributed to Lloyd Shapley. The original problem was to assess how important is each player to the overall cooperation, and what payoff can he or she reasonably expect from the coalition? (Shapley 1953) The idea is similar to the Break Down 4 plots. The main difference comes from the way how variable attributions are calculated. \\[ \\phi_i (f) = \\frac 1{|N|}\\sum_{S \\subseteq N\\setminus \\{i\\}} {{|N|-1}\\choose{|S|}}^{-1} \\left(f(S \\cup \\{i\\}) - f(S)\\right) \\] The original concept is straightforward. (Lundberg, Erion, and Lee 2018) It is straightforward for linear (and more general: additive) models. But not that obvious for more complex models. In this section we present uniform, model agnostic approach to variable attribution. Figure 6.1: (fig:BDPricee) An illustration of Break Down Plots. Model prediction for Random Forest models for a single observation (grey bar, 4763.9) is decomposed into parts that can be attributed to population average (the bottom bar, 3515.9) and effects of particular variables. Similar to the Shapley method introduced in the Section 6, Break Down Plots show additive decomposition of model output. As we will show later, the Shapely method may be perceived as an average from all possible Break Down Paths. In the last subsection we discuss pros and cons of this approach. Variable Atribution This approach can be seen as an approximation of Shapley values where feature contribution is linked with the average effect of a feature across all possible relaxations. These approaches are identical for additive models. For non-additive models the additive attribution is just an approximation in both cases, yet the greedy strategy produces explanations that are easier to interpret. It is worth noting that similar decomposition of predictions and measures of contribution for classifiers have been examined in . 6.2 Pros and cons Shapley Values give a uniform approach to decompose model prediction into parts that can be attributed additively to variables. Below we summarize key strengths and weaknesses of this approach. Pros There is a nice theory. Cons For non additive models the summation over all possible orderings is consistent, but on the other hand it has to be wrong, since the model is non additive anyway. References "],
["LIME.html", "Chapter 7 LIME: Local Interpretable Model-Agnostic Explanations", " Chapter 7 LIME: Local Interpretable Model-Agnostic Explanations (Pedersen and Benesty 2018) (Staniak and Biecek 2018) library(lime) library(live) Sparse model approximation / variable selection / feature ranking live: Local Interpretable (Model-Agnostic) Visual Explanations References "],
["ceterisParibus.html", "Chapter 8 Ceteris Paribus Principle 8.1 Introduction 8.2 1D profiles 8.3 Profile oscillations 8.4 2D profiles 8.5 Local model fidelity 8.6 Pros and cons 8.7 Code snippets for R", " Chapter 8 Ceteris Paribus Principle In this section we introduce tools based on Ceteris Paribus principle. The main goal for these tools is to help understand how changes in model input affect changes in model output. Presented explainers are linked with the second law introduced in Section 2.3, i.e. law for prediction’s speculations. This is why these explainers are also known as What-If model analysis or Individual Conditional EXpectations (Goldstein et al. 2015). It turns out that it is easier to understand how blacx-box model is working if we can play with it by changing variable by variable. Think of following usecases: Think about a model for hart attack. How the model response would change if a patient cuts the number of smoked cigarettes by half or increase physical activity. Think about a model for credit scoring. A customer gets a low score and is asking what he needs to change to increase this score to a certain level, to pass the bank criteria. Think about a model for apartment prices. An investor wants to know how much the price may increase if apartment standard is upgraded. 8.1 Introduction Ceteris paribus is a Latin phrase meaning “other things held constant” or “all else unchanged”. Using this principle we examine input variable per variable separatly, asumming that effects of all other variables are unchanged. See Figure 8.1 Figure 8.1: (fig:modelResponseCurveLine) A) Model response surface. Ceteris Paribus profiles marked with black curves helps to understand the curvature of the model response by updating only a single variable. B) CP profiles are individual conditional model responses Similar to the LIME method introduced in the section 7, Ceteris Paribus profiles examine curvature of a model response function. The difference between these two methods that LIME approximates the model curvature with a simpler white-box model that is easier to present. Usually the LIME model is sparse, thus our attention may be limited to smaller number of dimensions. In contrary, the CP plots show conditional model response for every variable. In the last subsection we discuss pros and cons of this approach. 8.2 1D profiles Let \\(f_{M}(x): \\mathcal R^{d} \\rightarrow \\mathcal R\\) denote a predictive model, i.e. function that takes \\(d\\) dimensional vector and calculate numerical score. Symbol \\(x \\in \\mathcal R^d\\) refers to a point in the feature space. We use subscript \\(x_i\\) to refer to a different data points and superscript \\(x^j\\) to refer to specific dimensions. Additionally, let \\(x^{-j}\\) denote all coordinates except \\(j\\)-th and let \\(x|^j=z\\) denote a data point \\(x^*\\) with all coordinates equal to \\(x\\) except coordinate \\(j\\) equal to value \\(z\\). I.e. \\(\\forall_{i \\neq {j}} x^i = x^{*,i}\\) and \\(x^j = z\\). In other words \\(x|^j=z\\) denote a \\(x\\) with \\(j\\)th coordinate changed to \\(z\\). Now we can define uni-dimensional Ceteris Paribus Profile for model \\(f\\), variable \\(j\\) and point \\(x\\) as \\[ CP^{f, j, x}(z) := f(x|^j = z). \\] I.e. CP profile is a model response obtained for observations created based on \\(x\\) with coordinate \\(j\\) changed and all other coordinates kept unchanged. A natural way to visualise CP profiles is to use a profile plot as in Figure 8.2. Figure 8.2 shows an example of Ceteris Paribus profile. The black dot stands for prediction for a single observation. Grey line show how the model response would change if in this single observation coordinate hours will be changed to selected value. One thing that we can read is that the model response is not smooth and there is some variability along the profile. Second thing is that for this particular observation the model response would drop significantly if the variable hours will be higher than 45. Figure 8.2: (fig:HRCPHiredHours) Ceteris Paribus profile for Random Forest model that assess the probability of being fired in call center as a function of average number of working hours Since in the example dataset we are struggling with model for three classes, one can plot CP profiles for each class in the same panel. See an example in the Figure 8.3. Figure 8.3: (fig:HRCPAllHours) Ceteris Paribus profiles for three classess predicted by the Random Forest model as a function of average number of working hours Usually model input consist many variables, then it is beneficial to show more variables at the same time. The easiest way to do so is to plot consecutive variables on separate panels. See an example in Figure 8.4. Figure 8.4: (fig:HRCPFiredAll) Ceteris Paribus profiles for all continuous variables 8.3 Profile oscillations Visual examination of variables is insightful, but for large number of variables we end up with large number of panels, most of which are flat. This is why we want to asses variable importance and show only profiles for important variables. The advantage of CP profiles is that they lead to a very natural and intuitive way of assessing the variable importance for a single prediction. The intuition is: the more important variable the larger are changes along the CP profile. If variable is not important then model response will barely change. If variable is important the CP profile change a lot for different values of a variable. Let’s write it down in a more formal way. Let \\(vip^{CP}_j(x)\\) denotes variable importance calculated based on CP profiles in point \\(x\\) for variable \\(j\\). \\[ vip^{CP}_j(x) = \\int_{-\\inf}^{inf} |CP^{f,j,x}(z) - f(x)| dz \\] So it’s an absolute deviation from \\(f(x)\\). Note that one can consider different modification of this coefficient: Deviations can be calculated not as a distance from \\(f(x)\\) but from average \\(\\bar CP^{f,j,x}(z)\\). The integral may be weighted based on the density of variable \\(x^j\\). Instead of absolute deviations one may use root from average squares. TODO: we need to verify which approach is better. Anna Kozak is working on this The straightforward estimator for \\(vip^{CP}_j(x)\\) is \\[ \\widehat{ vip^{CP}_j(x)} = \\frac 1n \\sum_{i=1}^n |CP^{f,j,x}(x_i) - f(x)|. \\] Figure 8.5 shows the idea behind measuring oscillations. The larger the highlighted area the more important is the variable. Figure 8.5: (fig:CPVIPprofiles) CP oscillations are average deviations between CP profiles and the model response Figure 8.6 summarizes variable oscillations. Such visuals help to quickly grasp how large are model oscillations around a specific point. Figure 8.6: (fig:CPVIP1) Variable importance plots calculated for Ceteris Paribus profiles for observation ID: 1001 NOTE Variable importance for single prediction may be very different than variable importance for the full model. For example, consider a model \\[ f(x_1, x_2) = x_1 * x_2 \\] where variables \\(x_1\\) and \\(x_2\\) takes values in \\([0,1]\\). From the global perspective both variables are equally important. But local variable importance is very different. Around point \\(x = (0, 1)\\) the importance of \\(x_1\\) is much larger than \\(x_2\\). This is because profile for \\(f(z, 1)\\) have larger oscillations than \\(f(0, z)\\). 8.4 2D profiles The definition of ceteris paribus profiles given in section 8.2 may be easily extended to two and more variables. Also definition of CP oscillations 8.3 have straight forward generalization for larger number of dimensions. Such generalisations are usefull when model is non additive. Presence of pairwise interactions may be detected with 2D Ceteris Paribus plots. Let’s define two-dimensional Ceteris Paribus Profile for model \\(f\\), variables \\(j\\) and \\(k\\) and point \\(x\\) as \\[ CP^{f, (j,k), x}(z_1, z_2) := f(x|^{(j,k)} = (z_1,z_2)). \\] I.e. CP profile is a model response obtained for observations created based on \\(x\\) with \\(j\\) and \\(k\\) coordinates changed to \\((z_1, z_2)\\) and all other coordinates kept unchanged. A natural way to visualise 2D CP profiles is to use a level plot as in Figure 8.7. Figure 8.7: (fig:CP2Dsurflor) Ceteris Paribus plot for a pair of variales. Black cross marks coordinated for the observation of interest. Presented model estimates price of an appartment If number of variables is small or moderate thein it is possible to present all pairs of variables. See an example in Figure 8.8. Figure 8.8: (fig:CP2Dall) Ceteris Paribus plot for all pairs of variales. 8.5 Local model fidelity Ceteris Paribus profiles are also a useful tool to validate local model fidelity. It may happen that global performance of the model is good, while for some points the local fit is very bad. Local fidelity helps to understand how good is the model fit around point of interest. How does it work? The idea behind fidelity plots is to select some number of points from the validation dataset that are closes to the point of interest. It’s a similar approach as in k nearest neighbours. Then for these neighbours we may plot Ceteris Paribus Profiles and check how stable they are. Also, if we know true taget values for points from the validation dataset we may plot residuals to show how large are residuals. An example fidelity plot is presented in Figure 8.9. Black line shows the CP profiles for the point of interest, while grey lines show CP profiles for neihgbors. Red intervals stand for residuals and in this example it looks like residuals for neighbours are all negative. Thus maybe model is biased around the point of interest. Figure 8.9: (fig:CPfidelity1) Local fidelity plots. Black line shows the CP profile for the point of interest. Grey lines show CP profiles for nearest neighbors. Red intervals correspond to residuals. Each red interval starts in a model prediction for a selected neighbor and ends in its true value of target variable. This observation may be confirmed by plots that compare distribution of all residuals against distribution of residuals for neighbors. See Figure ?? for an example. Here residuals for neighbors are shifted towards highest values. This suggests that the model response is biased around the observation of interest. Figure 8.10: (fig:CPfidelityBoxplot) Distribution of residuals for whole validation data (grey boxplot) and for selected closes 15 neighbors (red boxplot). 8.6 Pros and cons Ceteris Paribus principle gives a uniform and extendable approach to model exploration. Below we summarize key strengths and weaknesses of this approach. Pros Graphical representation of Ceteris Paribus profile is easy to understand. Ceteris Paribus profiles are compact and it is easy to fit many models or many variables in a small space. Ceteris Paribus profiles helps to understand how model response would change and how stable it is Oscillations calculated for CP profiles helps to select the most important variables. 2D Ceteris Paribus profiles help to identify pairwise interactions between variables. Cons If variables are correlated (like surface and number of rooms) then the ‘everything else kept unchanged’ approach leads to unrealistic settings. Interactions between variables are not visible in 1D plots. This tool is not suited for very wide data, like hundreds or thousands of variables. Visualization of categorical variables is non trivial. 8.7 Code snippets for R In this section we present key features of the ceterisParibus package for R (P. Biecek 2018b). This package covers all features presented in this chapter. It is available on CRAN and GitHub. Find more examples at the website of this package https://pbiecek.github.io/ceterisParibus/. Model preparation In this section we will present examples based on the apartments dataset. See section TODO for more details. library(&quot;DALEX&quot;) head(apartments) ## m2.price construction.year surface floor no.rooms district ## 1 5897 1953 25 3 1 Srodmiescie ## 2 1818 1992 143 9 5 Bielany ## 3 3643 1937 56 1 2 Praga ## 4 3517 1995 93 7 3 Ochota ## 5 3013 1992 144 6 5 Mokotow ## 6 5795 1926 61 6 2 Srodmiescie The problem here is to predict average price for square meter for an apartment. Let’s build a random forest model with randomForest package (Breiman et al. 2018). library(&quot;randomForest&quot;) rf_model &lt;- randomForest(m2.price ~ construction.year + surface + floor + no.rooms, data = apartments) rf_model ## ## Call: ## randomForest(formula = m2.price ~ construction.year + surface + floor + no.rooms, data = apartments) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 485698.2 ## % Var explained: 40.86 Model exploration with ceterisParibus package is performed in four steps. 1. Create an explainer - wrapper around model and validation data. Since all other functions work in a model agnostic fashion, first we need to define a wrapper around the model. Here we are using the explain() function from DALEX package (P. Biecek 2018c). library(&quot;DALEX&quot;) explainer_rf &lt;- explain(rf_model, data = apartmentsTest, y = apartmentsTest$m2.price) explainer_rf ## Model label: randomForest ## Model class: randomForest.formula,randomForest ## Data head : ## m2.price construction.year surface floor no.rooms district ## 1001 4644 1976 131 3 5 Srodmiescie ## 1002 3082 1978 112 9 4 Mokotow 2. Define point of interest. Certeris Paribus profiles explore model around a single point. new_apartment &lt;- data.frame(construction.year = 1965, no.rooms = 5, surface = 142, floor = 8) new_apartment ## construction.year no.rooms surface floor ## 1 1965 5 142 8 predict(rf_model, new_apartment) ## 1 ## 2357.997 3. Calculate CP profiles The ceteris_paribus() function calculates CP profiles for selected model around selected observation. By default CP profiles are calculated for all numerical variables. Use the variables argument to select subset of interesting variables. The result from ceteris_paribus()function is a data frame with model predictions for modified points around the point of interest. library(&quot;ceterisParibus&quot;) cp_rf &lt;- ceteris_paribus(explainer_rf, new_apartment, variables = c(&quot;construction.year&quot;, &quot;floor&quot;)) cp_rf ## Top profiles : ## construction.year no.rooms surface floor _yhat_ _vname_ ## 1 1920 5 142 8 3062.657 construction.year ## 1.1 1921 5 142 8 3076.156 construction.year ## 1.2 1922 5 142 8 3059.154 construction.year ## 1.3 1923 5 142 8 3029.844 construction.year ## 1.4 1923 5 142 8 3029.844 construction.year ## 1.5 1924 5 142 8 3059.722 construction.year ## _ids_ _label_ ## 1 1 randomForest ## 1.1 1 randomForest ## 1.2 1 randomForest ## 1.3 1 randomForest ## 1.4 1 randomForest ## 1.5 1 randomForest ## ## ## Top observations: ## construction.year no.rooms surface floor _yhat_ _label_ ## 1 1965 5 142 8 2357.997 randomForest 4. Plot CP profiles. Generic plot() function plot CP profiles. It returns a ggplot2 object that can be polished if needed. Use additional arguments of this function to select colors and sizes for elements visible in the plot. plot(cp_rf) One of very useful features of ceterisParibus explainers is that profiles for two or more models may be superimposed in a single plot. This helps in model comparisons. Let’s create a linear model for this dataset and repeat steps 1-3 for the lm model. lm_model &lt;- lm(m2.price ~ construction.year + surface + floor + no.rooms, data = apartments) explainer_lm &lt;- explain(lm_model, data = apartmentsTest, y = apartmentsTest$m2.price) cp_lm &lt;- ceteris_paribus(explainer_lm, new_apartment, variables = c(&quot;construction.year&quot;, &quot;floor&quot;)) Now we can use function plot() to compare both models in a single chart. Additional argument color = &quot;_label_&quot; set color as a key for model. plot(cp_rf, cp_lm, color = &quot;_label_&quot;) Oscillations The calculate_oscillations() function calculates oscillations for CP profiles. cp_rf_all &lt;- ceteris_paribus(explainer_rf, new_apartment) co_rf_all &lt;- calculate_oscillations(cp_rf_all) co_rf_all ## _vname_ _ids_ oscillations ## 2 surface 1 588.5873 ## 4 no.rooms 1 425.4092 ## 3 floor 1 312.9632 ## 1 construction.year 1 223.8619 plot(co_rf_all) 2D Ceteris Paribus profiles And the what_if_2d() function calculates 2D CP profiles. wi_rf_2d &lt;- what_if_2d(explainer_rf, observation = new_apartment, selected_variables = c(&quot;surface&quot;,&quot;floor&quot;, &quot;construction.year&quot;)) plot(wi_rf_2d, split_ncol = 2) References "],
["model-level-explanations.html", "Model level explanations", " Model level explanations "],
["introduction-3.html", "Chapter 9 Introduction 9.1 Example: Price prediction", " Chapter 9 Introduction 9.1 Example: Price prediction (P. Biecek 2018c) In this chapter we show examples for three predictive models trained on apartments dataset from the DALEX package. Random Forest model (elastic but biased), Support Vector Machines model (large variance on boundaries) and Linear Model (stable but not very elastic). Presented examples are for regression (prediction of square meter price), but the CP profiles may be used in the same way for classification. library(&quot;DALEX&quot;) # Linear model trained on apartments data model_lm &lt;- lm(m2.price ~ construction.year + surface + floor + no.rooms + district, data = apartments) library(&quot;randomForest&quot;) set.seed(59) # Random Forest model trained on apartments data model_rf &lt;- randomForest(m2.price ~ construction.year + surface + floor + no.rooms + district, data = apartments) library(&quot;e1071&quot;) # Support Vector Machinesr model trained on apartments data model_svm &lt;- svm(m2.price ~ construction.year + surface + floor + no.rooms + district, data = apartments) For these models we use DALEX explainers created with explain() function. There exapliners wrap models, predict functions and validation data. explainer_lm &lt;- explain(model_lm, data = apartmentsTest[,2:6], y = apartmentsTest$m2.price) explainer_rf &lt;- explain(model_rf, data = apartmentsTest[,2:6], y = apartmentsTest$m2.price) explainer_svm &lt;- explain(model_svm, data = apartmentsTest[,2:6], y = apartmentsTest$m2.price) Examples presented in this chapter are generated with the ceterisParibus package in version 0.3.1. library(&quot;ceterisParibus&quot;) References "],
["variable-importance.html", "Chapter 10 Variable Importance", " Chapter 10 Variable Importance Feature selection Beware Default Random Forest Importances Terence Parr, Kerem Turgutlu, Christopher Csiszar, and Jeremy Howard March 26, 2018. http://explained.ai/rf-importance/index.html "],
["marginal-response.html", "Chapter 11 Marginal Response 11.1 Partial Dependency Plots 11.2 Merging Path Plots", " Chapter 11 Marginal Response Feature extraction 11.1 Partial Dependency Plots Accumulated Local Effects (ALE) Plots (???) library(ALEPlot) Interactions - extraction 11.2 Merging Path Plots (???) library(factorMerger) "],
["performance-diagnostic.html", "Chapter 12 Performance Diagnostic", " Chapter 12 Performance Diagnostic Model selection "],
["residual-diagnostic.html", "Chapter 13 Residual Diagnostic", " Chapter 13 Residual Diagnostic Model validation "],
["other-topics.html", "Chapter 14 Other topics", " Chapter 14 Other topics "],
["appendixes.html", "Appendixes", " Appendixes "],
["DataSets.html", "Chapter 15 Data Sets 15.1 Hire or Fire? HR in Call Center 15.2 How much does it cost? Price prediction for a square meter", " Chapter 15 Data Sets 15.1 Hire or Fire? HR in Call Center In this chapter we present an artificial dataset from Human Resources department in a Call Center to present pros and cons for different techniques of prediction level explainers. The dataset is available in the DALEX package (P. Biecek 2018c). Each row corresponds to a single employee of a call center. Features like gender, age, average number of working hours per week, grade from the last evaluation and level of salary are used as predictive features. The problem here is to first build a model, that will determine when to fires and when to promote an employer, so it’s a classification problem with three classes. But having a model we will use prediction level explainers to better understand how the model works for selected cases. library(&quot;DALEX&quot;) head(HR) ## gender age hours evaluation salary status ## 1 male 32.58267 41.88626 3 1 fired ## 2 female 41.21104 36.34339 2 5 fired ## 3 male 37.70516 36.81718 3 0 fired ## 4 female 30.06051 38.96032 3 2 fired ## 5 male 21.10283 62.15464 5 3 promoted ## 6 male 40.11812 69.53973 2 0 fired In this book we are focused on model exploration rather than model building, thus for sake ok simplicity we will use two default models created with random forest (Breiman et al. 2018) and generalized linear model (???). set.seed(59) library(&quot;randomForest&quot;) model_rf &lt;- randomForest(status ~ gender + age + hours + evaluation + salary, data = HR) library(&quot;nnet&quot;) model_glm &lt;- multinom(status ~ gender + age + hours + evaluation + salary, data = HR) ## # weights: 21 (12 variable) ## initial value 8620.810629 ## iter 10 value 7002.127738 ## iter 20 value 6239.478146 ## iter 20 value 6239.478126 ## iter 20 value 6239.478124 ## final value 6239.478124 ## converged 15.2 How much does it cost? Price prediction for a square meter In this chapter we present an artificial dataset related to prediction of prices for appartments in Warsaw. This dataset wil be used to discuss pros and cons for different techniques of model level explainers. The dataset is available in the DALEX package (P. Biecek 2018c). Each row corresponds to a single apartment. Features like surface, number of rooms, district or floor are used as predictive features. The problem here is to predict price of a square meter for an appartment, so it’s a regression problem with continouse outcome. library(&quot;DALEX&quot;) head(apartments) ## m2.price construction.year surface floor no.rooms district ## 1 5897 1953 25 3 1 Srodmiescie ## 2 1818 1992 143 9 5 Bielany ## 3 3643 1937 56 1 2 Praga ## 4 3517 1995 93 7 3 Ochota ## 5 3013 1992 144 6 5 Mokotow ## 6 5795 1926 61 6 2 Srodmiescie The problem here is to predict average price for square meter for an apartment. Let’s build a random forest model with randomForest package (Breiman et al. 2018). library(&quot;randomForest&quot;) model_rf &lt;- randomForest(m2.price ~ construction.year + surface + floor + no.rooms + district, data = apartments) model_rf ## ## Call: ## randomForest(formula = m2.price ~ construction.year + surface + floor + no.rooms + district, data = apartments) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 82079.37 ## % Var explained: 90.01 And a linear model. model_lm &lt;- lm(m2.price ~ construction.year + surface + floor + no.rooms + district, data = apartments) model_lm ## ## Call: ## lm(formula = m2.price ~ construction.year + surface + floor + ## no.rooms + district, data = apartments) ## ## Coefficients: ## (Intercept) construction.year surface ## 5020.139 -0.229 -10.238 ## floor no.rooms districtBielany ## -99.482 -37.730 17.214 ## districtMokotow districtOchota districtPraga ## 918.380 926.254 -37.105 ## districtSrodmiescie districtUrsus districtUrsynow ## 2080.611 29.942 -18.865 ## districtWola districtZoliborz ## -16.891 889.973 References "],
["references.html", "References", " References "]
]
