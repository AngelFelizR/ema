<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Predictive Models: Visualisation, Exploration and Explanation</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Predictive Models: Visualisation, Exploration and Explanation" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Predictive Models: Visualisation, Exploration and Explanation" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Przemyslaw Biecek and Tomasz Burzykowski">


<meta name="date" content="2018-09-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="PredictionExplainers.html">
<link rel="next" href="breakDown.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Predictive Models:<br/> Visualisation, Exploration and Explanation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#white-box-models-vs-black-box-models"><i class="fa fa-check"></i><b>1.1</b> White-box models vs Black-box models</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#model-agnostic-vs-model-specific"><i class="fa fa-check"></i><b>1.2</b> Model agnostic vs Model specific</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#why-do-we-need-model-explainers"><i class="fa fa-check"></i><b>1.3</b> Why do we need model explainers?</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#how-model-exploration-is-different-from-data-exploration"><i class="fa fa-check"></i><b>1.4</b> How model exploration is different from data exploration?</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#glossary-notation"><i class="fa fa-check"></i><b>1.5</b> Glossary / Notation</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#thanksto"><i class="fa fa-check"></i><b>1.6</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="prediction-level-explanations.html"><a href="prediction-level-explanations.html"><i class="fa fa-check"></i>Prediction level explanations</a></li>
<li class="chapter" data-level="2" data-path="PredictionExplainers.html"><a href="PredictionExplainers.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="PredictionExplainers.html"><a href="PredictionExplainers.html#variable-attribution-vs-what-if-analysis"><i class="fa fa-check"></i><b>2.1</b> Variable attribution vs What-If analysis</a></li>
<li class="chapter" data-level="2.2" data-path="PredictionExplainers.html"><a href="PredictionExplainers.html#when-to-use"><i class="fa fa-check"></i><b>2.2</b> When to use?</a></li>
<li class="chapter" data-level="2.3" data-path="PredictionExplainers.html"><a href="PredictionExplainers.html#three-single-laws"><i class="fa fa-check"></i><b>2.3</b> A bit of philosophy: Three Laws for Prediction Level Explanations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html"><i class="fa fa-check"></i><b>3</b> Introduction to variable attribution methods</a><ul>
<li class="chapter" data-level="3.1" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#VAlinMod"><i class="fa fa-check"></i><b>3.1</b> Variable attribution for linear models</a><ul>
<li class="chapter" data-level="3.1.1" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#wine-quality-example"><i class="fa fa-check"></i><b>3.1.1</b> Wine quality example</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#modelAgnosticAttribution"><i class="fa fa-check"></i><b>3.2</b> Model agnostic approaches</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="breakDown.html"><a href="breakDown.html"><i class="fa fa-check"></i><b>4</b> Sequential Additive Attributions</a><ul>
<li class="chapter" data-level="4.1" data-path="breakDown.html"><a href="breakDown.html#the-algorithm"><i class="fa fa-check"></i><b>4.1</b> The Algorithm</a></li>
<li class="chapter" data-level="4.2" data-path="breakDown.html"><a href="breakDown.html#hr-dataset-hire-or-fire"><i class="fa fa-check"></i><b>4.2</b> HR dataset: Hire or Fire?</a></li>
<li class="chapter" data-level="4.3" data-path="breakDown.html"><a href="breakDown.html#break-down-plots"><i class="fa fa-check"></i><b>4.3</b> Break Down Plots</a></li>
<li class="chapter" data-level="4.4" data-path="breakDown.html"><a href="breakDown.html#pros-and-cons"><i class="fa fa-check"></i><b>4.4</b> Pros and cons</a></li>
<li class="chapter" data-level="4.5" data-path="breakDown.html"><a href="breakDown.html#code-snippets-for-r"><i class="fa fa-check"></i><b>4.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sequential-attributions-with-interactions.html"><a href="sequential-attributions-with-interactions.html"><i class="fa fa-check"></i><b>5</b> Sequential attributions with interactions</a><ul>
<li class="chapter" data-level="5.1" data-path="sequential-attributions-with-interactions.html"><a href="sequential-attributions-with-interactions.html#the-algorithm-1"><i class="fa fa-check"></i><b>5.1</b> The Algorithm</a></li>
<li class="chapter" data-level="5.2" data-path="sequential-attributions-with-interactions.html"><a href="sequential-attributions-with-interactions.html#hr-dataset-hire-or-fire-1"><i class="fa fa-check"></i><b>5.2</b> HR dataset: Hire or Fire?</a></li>
<li class="chapter" data-level="5.3" data-path="sequential-attributions-with-interactions.html"><a href="sequential-attributions-with-interactions.html#break-down-plots-1"><i class="fa fa-check"></i><b>5.3</b> Break Down Plots</a></li>
<li class="chapter" data-level="5.4" data-path="sequential-attributions-with-interactions.html"><a href="sequential-attributions-with-interactions.html#pros-and-cons-1"><i class="fa fa-check"></i><b>5.4</b> Pros and cons</a></li>
<li class="chapter" data-level="5.5" data-path="sequential-attributions-with-interactions.html"><a href="sequential-attributions-with-interactions.html#code-snippets-for-r-1"><i class="fa fa-check"></i><b>5.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>6</b> Average Additive Attributions</a><ul>
<li class="chapter" data-level="6.1" data-path="shapley.html"><a href="shapley.html#the-algorithm-2"><i class="fa fa-check"></i><b>6.1</b> The Algorithm</a></li>
<li class="chapter" data-level="6.2" data-path="shapley.html"><a href="shapley.html#code-snippets-for-r-2"><i class="fa fa-check"></i><b>6.2</b> Code snippets for R</a></li>
<li class="chapter" data-level="6.3" data-path="shapley.html"><a href="shapley.html#pros-and-cons-2"><i class="fa fa-check"></i><b>6.3</b> Pros and cons</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="LIME.html"><a href="LIME.html"><i class="fa fa-check"></i><b>7</b> Local approximations with white-box model</a><ul>
<li class="chapter" data-level="7.1" data-path="LIME.html"><a href="LIME.html#the-algorithm-3"><i class="fa fa-check"></i><b>7.1</b> The Algorithm</a></li>
<li class="chapter" data-level="7.2" data-path="LIME.html"><a href="LIME.html#hr-dataset-hire-or-fire-2"><i class="fa fa-check"></i><b>7.2</b> HR dataset: Hire or Fire?</a></li>
<li class="chapter" data-level="7.3" data-path="LIME.html"><a href="LIME.html#pros-and-cons-3"><i class="fa fa-check"></i><b>7.3</b> Pros and cons</a></li>
<li class="chapter" data-level="7.4" data-path="LIME.html"><a href="LIME.html#code-snippets-for-r-3"><i class="fa fa-check"></i><b>7.4</b> Code snippets for R</a><ul>
<li class="chapter" data-level="7.4.1" data-path="LIME.html"><a href="LIME.html#the-lime-pacakge"><i class="fa fa-check"></i><b>7.4.1</b> <strong>The lime pacakge</strong></a></li>
<li class="chapter" data-level="7.4.2" data-path="LIME.html"><a href="LIME.html#the-live-package"><i class="fa fa-check"></i><b>7.4.2</b> <strong>The live package</strong></a></li>
<li class="chapter" data-level="7.4.3" data-path="LIME.html"><a href="LIME.html#the-iml-package"><i class="fa fa-check"></i><b>7.4.3</b> <strong>The iml package</strong></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ceterisParibus.html"><a href="ceterisParibus.html"><i class="fa fa-check"></i><b>8</b> What-If analysis with the Ceteris Paribus Principle</a><ul>
<li class="chapter" data-level="8.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#introduction-1"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#ceterisParibus1d"><i class="fa fa-check"></i><b>8.2</b> 1D profiles</a></li>
<li class="chapter" data-level="8.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#oscillations"><i class="fa fa-check"></i><b>8.3</b> Profile oscillations</a></li>
<li class="chapter" data-level="8.4" data-path="ceterisParibus.html"><a href="ceterisParibus.html#d-profiles"><i class="fa fa-check"></i><b>8.4</b> 2D profiles</a></li>
<li class="chapter" data-level="8.5" data-path="ceterisParibus.html"><a href="ceterisParibus.html#local-model-fidelity"><i class="fa fa-check"></i><b>8.5</b> Local model fidelity</a></li>
<li class="chapter" data-level="8.6" data-path="ceterisParibus.html"><a href="ceterisParibus.html#pros-and-cons-4"><i class="fa fa-check"></i><b>8.6</b> Pros and cons</a></li>
<li class="chapter" data-level="8.7" data-path="ceterisParibus.html"><a href="ceterisParibus.html#code-snippets-for-r-4"><i class="fa fa-check"></i><b>8.7</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-level-explanations.html"><a href="model-level-explanations.html"><i class="fa fa-check"></i>Model level explanations</a></li>
<li class="chapter" data-level="9" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>9</b> Introduction</a><ul>
<li class="chapter" data-level="9.1" data-path="introduction-2.html"><a href="introduction-2.html#a-bit-of-philosophy"><i class="fa fa-check"></i><b>9.1</b> A bit of philosophy</a></li>
<li class="chapter" data-level="9.2" data-path="introduction-2.html"><a href="introduction-2.html#example-price-prediction"><i class="fa fa-check"></i><b>9.2</b> Example: Price prediction</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="variable-importance.html"><a href="variable-importance.html"><i class="fa fa-check"></i><b>10</b> Variable Importance</a></li>
<li class="chapter" data-level="11" data-path="marginal-response.html"><a href="marginal-response.html"><i class="fa fa-check"></i><b>11</b> Marginal Response</a><ul>
<li class="chapter" data-level="11.1" data-path="marginal-response.html"><a href="marginal-response.html#partialDependence"><i class="fa fa-check"></i><b>11.1</b> Partial Dependency Plots</a></li>
<li class="chapter" data-level="11.2" data-path="marginal-response.html"><a href="marginal-response.html#merging-path-plots"><i class="fa fa-check"></i><b>11.2</b> Merging Path Plots</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="modelComparisons.html"><a href="modelComparisons.html"><i class="fa fa-check"></i><b>12</b> Performance Diagnostic</a></li>
<li class="chapter" data-level="13" data-path="modelAuditing.html"><a href="modelAuditing.html"><i class="fa fa-check"></i><b>13</b> Residual Diagnostic</a></li>
<li class="chapter" data-level="14" data-path="other-topics.html"><a href="other-topics.html"><i class="fa fa-check"></i><b>14</b> Other topics</a></li>
<li class="chapter" data-level="" data-path="appendixes.html"><a href="appendixes.html"><i class="fa fa-check"></i>Appendixes</a></li>
<li class="chapter" data-level="15" data-path="DataSets.html"><a href="DataSets.html"><i class="fa fa-check"></i><b>15</b> Data Sets</a><ul>
<li class="chapter" data-level="15.1" data-path="DataSets.html"><a href="DataSets.html#HRdataset"><i class="fa fa-check"></i><b>15.1</b> Hire or Fire? HR in Call Center</a></li>
<li class="chapter" data-level="15.2" data-path="DataSets.html"><a href="DataSets.html#apartmentsDataset"><i class="fa fa-check"></i><b>15.2</b> How much does it cost? Price prediction for a square meter</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/pbiecek/DALEX" target="blank">DALEX website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Models: Visualisation, Exploration and Explanation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variableAttributionMethods" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Introduction to variable attribution methods</h1>
<p>In this section we introduce method for additive decomposition of predictions. The main goal for these tools is to help understand how model output may be attributed to input variables or sets of variables.</p>
<p>Presented explainers are linked with the first law introduced in Section <a href="PredictionExplainers.html#three-single-laws">2.3</a>, i.e. law for prediction’s justifications. Note that there are more tools for variable attribution, some of them will be presented in next sections.</p>
<p>Think of following use cases:</p>
<ul>
<li>Think about a model for heart attack. A patient wants to know which factors have highest impact on the final heart risk score.</li>
<li>Think about a model for apartment prices. An investor wants to know how much of the final price may be attributed to the location of an apartment.</li>
<li>Think about a model for credit scoring. A customer wants to know if factors like gender, age or number of kids influence model decisions.</li>
</ul>
<p>In the section <a href="variableAttributionMethods.html#VAlinMod">3.1</a> we will introduce key concepts and intuitions beyond variable attribution based on linear models. This approach may be easily applied to additive models and generalized linear models.
In sections <a href="breakDown.html#breakDown">4</a> and <a href="shapley.html#shapley">6</a> we will present model agnostic extentions of this concept.</p>
<div id="VAlinMod" class="section level2">
<h2><span class="header-section-number">3.1</span> Variable attribution for linear models</h2>
<p>Linear model with coefficients <span class="math inline">\(\beta = (\beta_0, \beta_1, .., \beta_p)\)</span> has following form.</p>
<p><span class="math display">\[
f(x) = \beta_0 + x_1 \beta_1 + \ldots + x_p \beta_p.
\]</span>
In other words, model response is the sum of weighted elements of <span class="math inline">\(x = (x_1, x_2, \ldots, x_p)\)</span>.</p>
<p>From a global perspective of a model, we are usually interested in questions like, how good is the model, which variables are significant or how accurate are model predictions.</p>
<p>But in this chapter we are focues in a local perspective, i.e. for a single observation <span class="math inline">\(x^*\)</span> how to measure the contribution of a variable <span class="math inline">\(x_i\)</span> on model prediction <span class="math inline">\(f(x^*)\)</span>.</p>
<p>Let <span class="math inline">\(v(f, x^*, i)\)</span> stands for the contribution of variable <span class="math inline">\(x_i\)</span> on prediction of model <span class="math inline">\(f()\)</span> in point <span class="math inline">\(x^*\)</span>. For linear models it is easy to define such contribution as
<span class="math display">\[
v(f, x^*, i) = f(x^*) - E[f(x)|x_{-1} = x^*_{-1}] = \beta_i x^*_i  - E \beta_i X_i
\]</span>
where the expected value can be estimated from the data
<span class="math display">\[
v(f, x^*, i) = \beta_i x^*_i - \beta_i \bar x_i = \beta_i (x^*_i - \bar x_i)
\]</span></p>
<p>The logic behind the attribution is the following.
Contribution of variable <span class="math inline">\(x_i\)</span> is the difference between model response for value <span class="math inline">\(x_i^*\)</span> minus the average model response.</p>
<div id="wine-quality-example" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Wine quality example</h3>
<p>It may be a surprise, that the attribution for variable <span class="math inline">\(x_i\)</span> is not the <span class="math inline">\(\beta_i x_i\)</span>.
But, please consider following example.
Figure <a href="variableAttributionMethods.html#fig:attribution1">3.1</a> shows the relation between alcohol and wine quality, based on the wine dataset <span class="citation">(Cortez et al. <a href="#ref-wine2009">2009</a>)</span>. The corresponding linear model is</p>
<p><span class="math display">\[
quality(alcohol) = 2.5820 + 0.3135 * alcohol
\]</span></p>
<p>The weakest wine in this dataset has 8% of alcohol, average alcohol concentration is 10.51, so the contribution of alcohol to the model prediction is <span class="math inline">\(0.3135 *(8-10.51) = -0.786885\)</span>. It means that low value of alcohol for this wine (8%) lower the prediction of quality by <span class="math inline">\(-0.786885\)</span>.</p>
<p>Note, that it would be confusing to forget about normalisation and say, that for the alcohol contribution on quality is <span class="math inline">\(0.3135*8 = 2.508\)</span> as this is high positive value.</p>
<div class="figure" style="text-align: center"><span id="fig:attribution1"></span>
<img src="figure/attribution_1.png" alt="(fig:attribution1)Relation between wine quality and concentration of alcohol assessed with linear model" width="50%" />
<p class="caption">
Figure 3.1: (fig:attribution1)Relation between wine quality and concentration of alcohol assessed with linear model
</p>
</div>
<p>Note that the linear model ma be rewritten in a following way</p>
<p><span class="math display">\[
f(x) = baseline + (x_1 - \bar x_1) \beta_1 + ... + (x_p - \bar x_p) \beta_p
\]</span></p>
<p>where
<span class="math display">\[
baseline = \mu + \bar x_1 \beta_1 + ... + \bar x_p \beta_p.
\]</span></p>
<p>Here <span class="math inline">\(baseline\)</span> is an average model response and variable contributions show how prediction for particular <span class="math inline">\(x^*\)</span> is different from the average response.</p>
<p>** NOTE for careful readers **</p>
<p>There is a gap between expected value of <span class="math inline">\(X_i\)</span> and average calculated on some dataset <span class="math inline">\(\bar x_i\)</span>. The latter depends on the data used for calculation of averages. For the sake of simplicity we do not emphasise these differences. To live with this just assume that we have access to a very large validation data that allows us to calculate <span class="math inline">\(\bar x_i\)</span> very accurately.</p>
</div>
</div>
<div id="modelAgnosticAttribution" class="section level2">
<h2><span class="header-section-number">3.2</span> Model agnostic approaches</h2>
<p>In the Section <a href="variableAttributionMethods.html#VAlinMod">3.1</a> we introduced a method for calculation of variable attributions for linear models.
This method is accurate, based directly on the structure of the model. But for most popular machine learning models we cannot assume that they are linear nor even additive.</p>
<p>In next sections we introduce a model agnostic approach. Note that even if the model itself is not additive, the model attribution will be additive.</p>
<p>Again, let <span class="math inline">\(v(f, x^*, i)\)</span> stands for the contribution of variable <span class="math inline">\(x_i\)</span> on prediction of model <span class="math inline">\(f()\)</span> in point <span class="math inline">\(x^*\)</span>.</p>
<p>We expect that such contribution will sum up to the model prediction in a given point (property called <em>local accuracy</em>), so
<span class="math display">\[
f(x^*) = baseline + \sum_{i=1}^p v(f, x^*, i)
\]</span>
where <span class="math inline">\(baseline\)</span> stands for average model response.</p>
<p>Note that the equation above may be rewritten as</p>
<p><span class="math display">\[
E [f(X)|X_1 = x_1^*, \ldots, X+p = x_p^*] = E[f(X)] + \sum_{i=1}^p v(f, x^*, i)
\]</span>
what leads to quite natural proposition for <span class="math inline">\(v(f, x^*_i, i)\)</span>, such as</p>
<p><span class="math display">\[
v(f, x^*_i, i) = E [f(X) | X_1 = x_1^*, \ldots, X_i = x_i^*] - E [f(X) | X_1 = x_1^*, \ldots, X_{i-1} = x_{i-1}^*] 
\]</span>
In other words the contribution of variable <span class="math inline">\(i\)</span> is the difference between expected model response conditioned on first <span class="math inline">\(i\)</span> variables minus the model response conditioned on first <span class="math inline">\(i-1\)</span> variables.</p>
<p>Such proposition fulfills the <em>local accuracy</em> condition, but unfortunatelly variable contributions depends on the ordering of variables.</p>
<div class="figure" style="text-align: center"><span id="fig:ordering"></span>
<img src="figure/ordering.png" alt="(fig:ordering) Two different paths between average model prediction and the model prediction for a selected observation. Black dots stand for conditional average, red arrows stands for changes between conditional averages." width="100%" />
<p class="caption">
Figure 3.2: (fig:ordering) Two different paths between average model prediction and the model prediction for a selected observation. Black dots stand for conditional average, red arrows stands for changes between conditional averages.
</p>
</div>
<p>See for example Figure <a href="variableAttributionMethods.html#fig:ordering">3.2</a>. In the first ordering the contribution of variable <code>age</code> is calculated as 0.01, while in the second the contribution is calculated as 0.13. Such differences are related to the lack of additivness of the model <span class="math inline">\(f()\)</span>. Propositions presented in next two sections present different solutions for this problem.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-wine2009">
<p>Cortez, Paulo, António Cerdeira, Fernando Almeida, Telmo Matos, and José Reis. 2009. “Modeling Wine Preferences by Data Mining from Physicochemical Properties.” <em>Decision Support Systems</em> 47 (4):547–53. <a href="https://doi.org/10.1016/j.dss.2009.05.016" class="uri">https://doi.org/10.1016/j.dss.2009.05.016</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="PredictionExplainers.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="breakDown.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["PM_VEE.pdf", "PM_VEE.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
