<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Predictive Models: Visualisal Exploration, Explanation and Debugging</title>
  <meta name="description" content="This book introduces key concepts for exploration, explanation and visualization of complex predictive models.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Predictive Models: Visualisal Exploration, Explanation and Debugging" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces key concepts for exploration, explanation and visualization of complex predictive models." />
  <meta name="github-repo" content="pbiecek/PM_VEE" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Predictive Models: Visualisal Exploration, Explanation and Debugging" />
  
  <meta name="twitter:description" content="This book introduces key concepts for exploration, explanation and visualization of complex predictive models." />
  

<meta name="author" content="Przemyslaw Biecek and Tomasz Burzykowski">


<meta name="date" content="2019-04-02">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="PredictionExplainers.html">
<link rel="next" href="variableAttributionMethods.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Predictive Models:<br/> Visualisation, Exploration and Explanation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#the-aim-of-the-book"><i class="fa fa-check"></i><b>1.1</b> The aim of the book</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#three-single-laws"><i class="fa fa-check"></i><b>1.2</b> A bit of philosophy: three laws of model explanation</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#terminology"><i class="fa fa-check"></i><b>1.3</b> Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#white-box-models-vs.black-box-models"><i class="fa fa-check"></i><b>1.4</b> White-box models vs. black-box models</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#model-visualization-exploration-and-explanation"><i class="fa fa-check"></i><b>1.5</b> Model visualization, exploration, and explanation</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#model-agnostic-vs.model-specific-approach"><i class="fa fa-check"></i><b>1.6</b> Model-agnostic vs. model-specific approach</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#code-snippets"><i class="fa fa-check"></i><b>1.7</b> Code snippets</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#the-structure-of-the-book"><i class="fa fa-check"></i><b>1.8</b> The structure of the book</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#thanksto"><i class="fa fa-check"></i><b>1.9</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html"><i class="fa fa-check"></i><b>2</b> Data Sets</a><ul>
<li class="chapter" data-level="2.1" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#TitanicDataset"><i class="fa fa-check"></i><b>2.1</b> Sinking of the RMS Titanic</a><ul>
<li class="chapter" data-level="2.1.1" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#data-cleaning"><i class="fa fa-check"></i><b>2.1.1</b> Data cleaning</a></li>
<li class="chapter" data-level="2.1.2" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#data-exploration"><i class="fa fa-check"></i><b>2.1.2</b> Data exploration</a></li>
<li class="chapter" data-level="2.1.3" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#model_titanic_lmr"><i class="fa fa-check"></i><b>2.1.3</b> Logistic regression is always a good choice</a></li>
<li class="chapter" data-level="2.1.4" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#model_titanic_rf"><i class="fa fa-check"></i><b>2.1.4</b> Random Forest to the rescue</a></li>
<li class="chapter" data-level="2.1.5" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#gradient-boosting-for-interactions"><i class="fa fa-check"></i><b>2.1.5</b> Gradient boosting for interactions</a></li>
<li class="chapter" data-level="2.1.6" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#model-predictions"><i class="fa fa-check"></i><b>2.1.6</b> Model predictions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#ApartmentDataset"><i class="fa fa-check"></i><b>2.2</b> Apartment Prices</a><ul>
<li class="chapter" data-level="2.2.1" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#a-tale-of-two-models"><i class="fa fa-check"></i><b>2.2.1</b> A tale of two models</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#HFDataset"><i class="fa fa-check"></i><b>2.3</b> Hire or Fire</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="instance-level-explanation.html"><a href="instance-level-explanation.html"><i class="fa fa-check"></i>Instance-level explanation</a></li>
<li class="chapter" data-level="3" data-path="PredictionExplainers.html"><a href="PredictionExplainers.html"><i class="fa fa-check"></i><b>3</b> Introduction</a></li>
<li class="chapter" data-level="4" data-path="ceterisParibus.html"><a href="ceterisParibus.html"><i class="fa fa-check"></i><b>4</b> What-If analysis with the Ceteris-paribus Profiles</a><ul>
<li class="chapter" data-level="4.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#intuition"><i class="fa fa-check"></i><b>4.2</b> Intuition</a></li>
<li class="chapter" data-level="4.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#method"><i class="fa fa-check"></i><b>4.3</b> Method</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#ceterisParibus1d"><i class="fa fa-check"></i><b>4.3.1</b> One-dimensional (1D) Ceteris-paribus Profiles</a></li>
<li class="chapter" data-level="4.3.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#oscillations"><i class="fa fa-check"></i><b>4.3.2</b> Profile Oscillations</a></li>
<li class="chapter" data-level="4.3.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#ceterisParibus2d"><i class="fa fa-check"></i><b>4.3.3</b> Two-dimensional (2D) Ceteris-paribus Profiles</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ceterisParibus.html"><a href="ceterisParibus.html#example-local-model-fidelity"><i class="fa fa-check"></i><b>4.4</b> Example: Local Model Fidelity</a></li>
<li class="chapter" data-level="4.5" data-path="ceterisParibus.html"><a href="ceterisParibus.html#pros-and-cons"><i class="fa fa-check"></i><b>4.5</b> Pros and cons</a></li>
<li class="chapter" data-level="4.6" data-path="ceterisParibus.html"><a href="ceterisParibus.html#code-snippets-for-r"><i class="fa fa-check"></i><b>4.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html"><i class="fa fa-check"></i><b>5</b> Variable attribution for linear models</a><ul>
<li class="chapter" data-level="5.1" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#intuition-1"><i class="fa fa-check"></i><b>5.2</b> Intuition</a></li>
<li class="chapter" data-level="5.3" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#method-1"><i class="fa fa-check"></i><b>5.3</b> Method</a></li>
<li class="chapter" data-level="5.4" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#example-wine-quality"><i class="fa fa-check"></i><b>5.4</b> Example: Wine quality</a></li>
<li class="chapter" data-level="5.5" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#pros-and-cons-1"><i class="fa fa-check"></i><b>5.5</b> Pros and Cons</a></li>
<li class="chapter" data-level="5.6" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#code-snippets-1"><i class="fa fa-check"></i><b>5.6</b> Code snippets</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="breakDown.html"><a href="breakDown.html"><i class="fa fa-check"></i><b>6</b> Variable attributions</a><ul>
<li class="chapter" data-level="6.1" data-path="breakDown.html"><a href="breakDown.html#intuition-2"><i class="fa fa-check"></i><b>6.1</b> Intuition</a></li>
<li class="chapter" data-level="6.2" data-path="breakDown.html"><a href="breakDown.html#method-2"><i class="fa fa-check"></i><b>6.2</b> Method</a></li>
<li class="chapter" data-level="6.3" data-path="breakDown.html"><a href="breakDown.html#example-hire-or-fire"><i class="fa fa-check"></i><b>6.3</b> Example: Hire or Fire?</a></li>
<li class="chapter" data-level="6.4" data-path="breakDown.html"><a href="breakDown.html#pros-and-cons-2"><i class="fa fa-check"></i><b>6.4</b> Pros and cons</a></li>
<li class="chapter" data-level="6.5" data-path="breakDown.html"><a href="breakDown.html#code-snippets-for-r-1"><i class="fa fa-check"></i><b>6.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html"><i class="fa fa-check"></i><b>7</b> Variable attribution with interactions</a><ul>
<li class="chapter" data-level="7.1" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#intuition-3"><i class="fa fa-check"></i><b>7.1</b> Intuition</a></li>
<li class="chapter" data-level="7.2" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#method-3"><i class="fa fa-check"></i><b>7.2</b> Method</a></li>
<li class="chapter" data-level="7.3" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#example-hire-or-fire-1"><i class="fa fa-check"></i><b>7.3</b> Example: Hire or Fire?</a></li>
<li class="chapter" data-level="7.4" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#break-down-plots"><i class="fa fa-check"></i><b>7.4</b> Break Down Plots</a></li>
<li class="chapter" data-level="7.5" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#pros-and-cons-3"><i class="fa fa-check"></i><b>7.5</b> Pros and cons</a></li>
<li class="chapter" data-level="7.6" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#code-snippets-for-r-2"><i class="fa fa-check"></i><b>7.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>8</b> Average variable attributions</a><ul>
<li class="chapter" data-level="8.1" data-path="shapley.html"><a href="shapley.html#intuition-4"><i class="fa fa-check"></i><b>8.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2" data-path="shapley.html"><a href="shapley.html#method-4"><i class="fa fa-check"></i><b>8.2</b> Method</a></li>
<li class="chapter" data-level="8.3" data-path="shapley.html"><a href="shapley.html#example-hire-or-fire-2"><i class="fa fa-check"></i><b>8.3</b> Example: Hire or Fire?</a></li>
<li class="chapter" data-level="8.4" data-path="shapley.html"><a href="shapley.html#pros-and-cons-4"><i class="fa fa-check"></i><b>8.4</b> Pros and cons</a></li>
<li class="chapter" data-level="8.5" data-path="shapley.html"><a href="shapley.html#code-snippets-for-r-3"><i class="fa fa-check"></i><b>8.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="LIME.html"><a href="LIME.html"><i class="fa fa-check"></i><b>9</b> Local approximations with white-box model</a><ul>
<li class="chapter" data-level="9.1" data-path="LIME.html"><a href="LIME.html#intuition-5"><i class="fa fa-check"></i><b>9.1</b> Intuition</a></li>
<li class="chapter" data-level="9.2" data-path="LIME.html"><a href="LIME.html#method-5"><i class="fa fa-check"></i><b>9.2</b> Method</a></li>
<li class="chapter" data-level="9.3" data-path="LIME.html"><a href="LIME.html#example-hire-or-fire-3"><i class="fa fa-check"></i><b>9.3</b> Example: Hire or Fire?</a></li>
<li class="chapter" data-level="9.4" data-path="LIME.html"><a href="LIME.html#pros-and-cons-5"><i class="fa fa-check"></i><b>9.4</b> Pros and cons</a></li>
<li class="chapter" data-level="9.5" data-path="LIME.html"><a href="LIME.html#code-snippets-for-r-4"><i class="fa fa-check"></i><b>9.5</b> Code snippets for R</a><ul>
<li class="chapter" data-level="9.5.1" data-path="LIME.html"><a href="LIME.html#the-lime-pacakge"><i class="fa fa-check"></i><b>9.5.1</b> <strong>The lime pacakge</strong></a></li>
<li class="chapter" data-level="9.5.2" data-path="LIME.html"><a href="LIME.html#the-live-package"><i class="fa fa-check"></i><b>9.5.2</b> <strong>The live package</strong></a></li>
<li class="chapter" data-level="9.5.3" data-path="LIME.html"><a href="LIME.html#the-iml-package"><i class="fa fa-check"></i><b>9.5.3</b> <strong>The iml package</strong></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="comparision-of-prediction-level-explainers.html"><a href="comparision-of-prediction-level-explainers.html"><i class="fa fa-check"></i><b>10</b> Comparision of prediction level explainers</a><ul>
<li class="chapter" data-level="10.1" data-path="comparision-of-prediction-level-explainers.html"><a href="comparision-of-prediction-level-explainers.html#when-to-use"><i class="fa fa-check"></i><b>10.1</b> When to use?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-level-explanations.html"><a href="model-level-explanations.html"><i class="fa fa-check"></i>Model level explanations</a></li>
<li class="chapter" data-level="11" data-path="introduction-3.html"><a href="introduction-3.html"><i class="fa fa-check"></i><b>11</b> Introduction</a><ul>
<li class="chapter" data-level="11.1" data-path="introduction-3.html"><a href="introduction-3.html#approaches-to-model-explanations"><i class="fa fa-check"></i><b>11.1</b> Approaches to model explanations</a></li>
<li class="chapter" data-level="11.2" data-path="introduction-3.html"><a href="introduction-3.html#a-bit-of-philosophy-three-laws-for-model-level-explanations"><i class="fa fa-check"></i><b>11.2</b> A bit of philosophy: Three Laws for Model Level Explanations</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="variableImportance.html"><a href="variableImportance.html"><i class="fa fa-check"></i><b>12</b> Feature Importance</a><ul>
<li class="chapter" data-level="12.1" data-path="variableImportance.html"><a href="variableImportance.html#permutation-based-feature-importance"><i class="fa fa-check"></i><b>12.1</b> Permutation Based Feature Importance</a></li>
<li class="chapter" data-level="12.2" data-path="variableImportance.html"><a href="variableImportance.html#example-titanic"><i class="fa fa-check"></i><b>12.2</b> Example: Titanic</a></li>
<li class="chapter" data-level="12.3" data-path="variableImportance.html"><a href="variableImportance.html#example-price-prediction"><i class="fa fa-check"></i><b>12.3</b> Example: Price prediction</a></li>
<li class="chapter" data-level="12.4" data-path="variableImportance.html"><a href="variableImportance.html#more-models"><i class="fa fa-check"></i><b>12.4</b> More models</a></li>
<li class="chapter" data-level="12.5" data-path="variableImportance.html"><a href="variableImportance.html#level-frequency"><i class="fa fa-check"></i><b>12.5</b> Level frequency</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="variableEngeneering.html"><a href="variableEngeneering.html"><i class="fa fa-check"></i><b>13</b> Feature effects</a><ul>
<li class="chapter" data-level="13.1" data-path="variableEngeneering.html"><a href="variableEngeneering.html#global-level-vs-instance-level-explanations"><i class="fa fa-check"></i><b>13.1</b> Global level vs instance level explanations</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html"><i class="fa fa-check"></i><b>14</b> Partial Dependency Profiles</a><ul>
<li class="chapter" data-level="14.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#definition"><i class="fa fa-check"></i><b>14.1</b> Definition</a></li>
<li class="chapter" data-level="14.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#estimation"><i class="fa fa-check"></i><b>14.2</b> Estimation</a><ul>
<li class="chapter" data-level="14.2.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#interactions-and-partial-dependency-profiles"><i class="fa fa-check"></i><b>14.2.1</b> Interactions and Partial Dependency profiles</a></li>
<li class="chapter" data-level="14.2.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#groups-of-partial-dependency-profiles"><i class="fa fa-check"></i><b>14.2.2</b> Groups of Partial Dependency profiles</a></li>
<li class="chapter" data-level="14.2.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#model-comparisons-with-partial-dependency-plots"><i class="fa fa-check"></i><b>14.2.3</b> Model comparisons with Partial Dependency Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="localProfiles.html"><a href="localProfiles.html"><i class="fa fa-check"></i><b>15</b> Conditional Dependency Profiles</a><ul>
<li class="chapter" data-level="15.1" data-path="localProfiles.html"><a href="localProfiles.html#definition-1"><i class="fa fa-check"></i><b>15.1</b> Definition</a></li>
<li class="chapter" data-level="15.2" data-path="localProfiles.html"><a href="localProfiles.html#estimation-1"><i class="fa fa-check"></i><b>15.2</b> Estimation</a></li>
<li class="chapter" data-level="15.3" data-path="localProfiles.html"><a href="localProfiles.html#conditional-marginal-profiles"><i class="fa fa-check"></i><b>15.3</b> Conditional / Marginal Profiles</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html"><i class="fa fa-check"></i><b>16</b> Accumulated Local Profiles</a><ul>
<li class="chapter" data-level="16.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#definition-2"><i class="fa fa-check"></i><b>16.1</b> Definition</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="how-pd-cd-and-al-profiles-are-different-and-which-to-choose.html"><a href="how-pd-cd-and-al-profiles-are-different-and-which-to-choose.html"><i class="fa fa-check"></i><b>17</b> How PD, CD and AL Profiles are different and which to choose</a></li>
<li class="chapter" data-level="18" data-path="factorMerger.html"><a href="factorMerger.html"><i class="fa fa-check"></i><b>18</b> Merging Path Plots and Others</a></li>
<li class="chapter" data-level="19" data-path="other-topics.html"><a href="other-topics.html"><i class="fa fa-check"></i><b>19</b> Other topics</a></li>
<li class="chapter" data-level="20" data-path="modelComparisons.html"><a href="modelComparisons.html"><i class="fa fa-check"></i><b>20</b> Performance Diagnostic</a></li>
<li class="chapter" data-level="21" data-path="modelAuditing.html"><a href="modelAuditing.html"><i class="fa fa-check"></i><b>21</b> Residual Diagnostic</a></li>
<li class="chapter" data-level="22" data-path="concept-drift.html"><a href="concept-drift.html"><i class="fa fa-check"></i><b>22</b> Concept Drift</a><ul>
<li class="chapter" data-level="22.1" data-path="concept-drift.html"><a href="concept-drift.html#introduction-4"><i class="fa fa-check"></i><b>22.1</b> Introduction</a></li>
<li class="chapter" data-level="22.2" data-path="concept-drift.html"><a href="concept-drift.html#covariate-drift"><i class="fa fa-check"></i><b>22.2</b> Covariate Drift</a></li>
<li class="chapter" data-level="22.3" data-path="concept-drift.html"><a href="concept-drift.html#code-snippets-2"><i class="fa fa-check"></i><b>22.3</b> Code snippets</a></li>
<li class="chapter" data-level="22.4" data-path="concept-drift.html"><a href="concept-drift.html#residual-drift"><i class="fa fa-check"></i><b>22.4</b> Residual Drift</a></li>
<li class="chapter" data-level="22.5" data-path="concept-drift.html"><a href="concept-drift.html#code-snippets-3"><i class="fa fa-check"></i><b>22.5</b> Code snippets</a></li>
<li class="chapter" data-level="22.6" data-path="concept-drift.html"><a href="concept-drift.html#model-drift"><i class="fa fa-check"></i><b>22.6</b> Model Drift</a></li>
<li class="chapter" data-level="22.7" data-path="concept-drift.html"><a href="concept-drift.html#code-snippets-4"><i class="fa fa-check"></i><b>22.7</b> Code snippets</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendixes.html"><a href="appendixes.html"><i class="fa fa-check"></i>Appendixes</a></li>
<li class="chapter" data-level="23" data-path="DataSets.html"><a href="DataSets.html"><i class="fa fa-check"></i><b>23</b> Data Sets</a><ul>
<li class="chapter" data-level="23.1" data-path="DataSets.html"><a href="DataSets.html#HRdataset"><i class="fa fa-check"></i><b>23.1</b> Hire or Fire? HR in Call Center</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="Packages.html"><a href="Packages.html"><i class="fa fa-check"></i><b>24</b> Packages</a><ul>
<li class="chapter" data-level="24.1" data-path="Packages.html"><a href="Packages.html#arguments"><i class="fa fa-check"></i><b>24.1</b> Arguments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/pbiecek/DALEX" target="blank">DALEX website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Models: Visualisal Exploration, Explanation and Debugging</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ceterisParibus" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> What-If analysis with the Ceteris-paribus Profiles</h1>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<p><em>Ceteris paribus</em> is a Latin phrase meaning “other things held constant” or “all else unchanged.” In this chapter, we introduce a technique for model exploration based on the Ceteris paribus principle. In particular, we examine the influence of each explanatory variable, asumming that effects of all other variables are unchanged. The main goal is to understand how changes in a single explanatory variable affects model predictions.</p>
<p>The presented tools (explainers) are linked to the second law introduced in Section <a href="index.html#three-single-laws">1.2</a>, i.e. the law of “Prediction’s speculation.” This is why the tools are also known as <em>What-If model analysis</em> or <em>Individual Conditional EXpectations</em> <span class="citation">(Goldstein et al. <a href="#ref-ICEbox">2015</a>)</span>. It turns out that it is easier to understand how a black-box model is working if we can explore the model by investigating the influence of explanatory variables separately, changing one at a time.</p>
</div>
<div id="intuition" class="section level2">
<h2><span class="header-section-number">4.2</span> Intuition</h2>
<p>Panel A of Figure <a href="ceterisParibus.html#fig:modelResponseCurveLine">4.1</a> presents a response surface for a model with two explanatory variables, <em>floor</em> and <em>construction.year</em>, from the <em>Apartment prices</em> dataset (see Section <a href="DataSetsIntro.html#ApartmentDataset">2.2</a>). We are interested in the change of the model prediction induced by each of the variables. Toward this end, we may want to explore the curvature of the response surface around a single point that is marked on the plot by a black dot. Ceteris-paribus (CP) profiles are one-dimensional profiles that examine the curvature across each dimension, i.e., for each variable. Panel B of Figure <a href="ceterisParibus.html#fig:modelResponseCurveLine">4.1</a> presents the profiles corresponding to <em>floor</em> and <em>construction.year</em>. In essence, a CP profile shows a conditional expectation of the dependent variable (response) for the particular explanatory variable.</p>
<div class="figure" style="text-align: center"><span id="fig:modelResponseCurveLine"></span>
<img src="figure/model_response_line.png" alt="(fig:modelResponseCurveLine) A) Model response (prediction) surface. Ceteris-paribus (CP) profiles marked with black curves help to understand the curvature of the surface while changing only a single explanatory variable. B) CP profiles for individual variables." width="70%" />
<p class="caption">
Figure 4.1: (fig:modelResponseCurveLine) A) Model response (prediction) surface. Ceteris-paribus (CP) profiles marked with black curves help to understand the curvature of the surface while changing only a single explanatory variable. B) CP profiles for individual variables.
</p>
</div>
<p>CP technique is similar to the LIME method (see Section <a href="LIME.html#LIME">9</a>). LIME and CP profiles examine the curvature of a model response-surface. The difference between these two methods lies in the fact that LIME approximates the black-box model of interest locally with a simpler white-box model. Usually, the LIME model is sparse, i.e., contains fewer variables, and thus we have got to graphically investigate a smaller number of dimensions. On the other hand, the CP profiles present conditional predictions for every variable and, in most cases, are easier to intepret.</p>
</div>
<div id="method" class="section level2">
<h2><span class="header-section-number">4.3</span> Method</h2>
<p>In this section we introduce more formally 1- and 2-dimensional CP profiles. <!-- We consider the case of a continuous dependent variable, but the profiles can be easily generalized for other types. --></p>
<div id="ceterisParibus1d" class="section level3">
<h3><span class="header-section-number">4.3.1</span> One-dimensional (1D) Ceteris-paribus Profiles</h3>
<p>Assume that <span class="math inline">\(E_Y(Y | x^*) \approx f(x^*)\)</span>, where <span class="math inline">\(f(x^*)\)</span> is the value of the model at <span class="math inline">\(x^*\)</span>. Note that <span class="math inline">\(x^*\)</span> is a vector containing values for explanatory covariates. We will use subscript <span class="math inline">\(x^*_i\)</span> to refer to the vector corresponding to the <span class="math inline">\(i\)</span>-th observation in a dataset. We will use superscript <span class="math inline">\(x^{*j}\)</span> to refer to the <span class="math inline">\(j\)</span>-th element of <span class="math inline">\(x^*\)</span>, i.e., the <span class="math inline">\(j\)</span>-th variable. Additionally, let <span class="math inline">\(x^{*-j}\)</span> denote a vector resultinig from removing the <span class="math inline">\(j\)</span>-th element from vector <span class="math inline">\(x^{*}\)</span>. Moreover, let <span class="math inline">\(x^{*|j}=z\)</span> denotes a vector in which the <span class="math inline">\(j\)</span>-th element is equal to <span class="math inline">\(z\)</span> (a scalar).</p>
<p>We define a one-dimensional CP profile for the model <span class="math inline">\(f()\)</span>, <span class="math inline">\(j\)</span>-th explanatory variable, and point <span class="math inline">\(x^*\)</span> as follows:</p>
<p><span class="math display">\[
CP^{f, j, x^*}(z) \equiv f(x^{*|j} = z).
\]</span>
That is, CP profile is a function that provides the dependence of the (approximate) expected value (prediction) of the model for <span class="math inline">\(Y\)</span> on the value of <span class="math inline">\(j\)</span>-th explanatory variable <span class="math inline">\(z\)</span>, where <span class="math inline">\(z\)</span> is taken to go through the range of values typical for the variable and values of all other variables in <span class="math inline">\(x^*\)</span> are kept fixed at the values present in <span class="math inline">\(x^*\)</span>.</p>
<p>[TOMASZ: HOW ABOUT CP PROFILES FOR FACTORS?]</p>
<p>A natural way to represent the function is to use a profile plot similar to the one presented in Figure <a href="ceterisParibus.html#fig:HRCPFiredHours">4.2</a>. In the figure, the black dot presents an instance prediction, i.e., prediction <span class="math inline">\(f(x^*)\)</span> for a single observation decribed by <span class="math inline">\(x^*\)</span>. The red curve shows how the prediction would change if the value of a particular explanatory variable (in this case, “hours”; see Section <a href="DataSetsIntro.html#HFDataset">2.3</a>) changed. It is worth oberving that the profile is not smooth and it expresses quite some variabilit. Moreovery, for this instance (observation), the prediction would drop substantially if the value of the explanatory variable became higher than 45.</p>
<div class="figure" style="text-align: center"><span id="fig:HRCPFiredHours"></span>
<img src="figure/HR_cp_fired_hours.png" alt="(fig:HRCPHiredHours) Ceteris-paribus profile for a Random Forest model that assesses the probability of being fired in a call center as a function of the average number of working hours" width="50%" />
<p class="caption">
Figure 4.2: (fig:HRCPHiredHours) Ceteris-paribus profile for a Random Forest model that assesses the probability of being fired in a call center as a function of the average number of working hours
</p>
</div>
<p>Note that in the <code>HR</code> dataset the dependent variable is categorical with three classes. Thus, it is a good idea to plot the CP profile for each class in the same panel, as shown in Figure <a href="ceterisParibus.html#fig:HRCPAllHours">4.3</a>. In this way we can simultaneously compare the effect of “hours” on the probability of each of the three classes. Of course, for a categorical dependent variable the sum of the probabilities over all classes is equal 1. Thus, one of the profiles is redundant. However, plotting all profiles helps to understand how changes in a single explanatory variable affect the model predictions.</p>
<div class="figure" style="text-align: center"><span id="fig:HRCPAllHours"></span>
<img src="figure/HR_cp_all_hours.png" alt="(fig:HRCPAllHours) Ceteris-paribus profiles for the probability of each of three classess predicted by the Random Forest model as a function of the average number of working hours" width="60%" />
<p class="caption">
Figure 4.3: (fig:HRCPAllHours) Ceteris-paribus profiles for the probability of each of three classess predicted by the Random Forest model as a function of the average number of working hours
</p>
</div>
<p>Usually, black-box models contain a large number of explanatory variables. However, CP profiles are legible even for tiny subplots, created with techniques like sparklines or small multiples <span class="citation">(Tufte <a href="#ref-Tufte1986">1986</a>)</span>. In this way we can display a large number of profiles at the same time keeping profiles for consecutive variables in separate panels, as shown in Figure <a href="ceterisParibus.html#fig:HRCPFiredAll">4.4</a>. It helps if these panels are ordered so that the most important profiles are listed first. We discuss a method to assess importance of CP profiles in the next subsection.</p>
<div class="figure" style="text-align: center"><span id="fig:HRCPFiredAll"></span>
<img src="figure/HR_cp_fired_all.png" alt="(fig:HRCPFiredAll) Ceteris-paribus profiles for all continuous explanatory variables in the `HR' dataset for the Random Forest model" width="70%" />
<p class="caption">
Figure 4.4: (fig:HRCPFiredAll) Ceteris-paribus profiles for all continuous explanatory variables in the `HR’ dataset for the Random Forest model
</p>
</div>
</div>
<div id="oscillations" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Profile Oscillations</h3>
<p>Visual examination of CP profiles is insightful, but for a model with a large number of explanatory variables we may end up with a large number of plots which may be overwhelming. To prioritize between profiles we need a measure that would summarize the impact of a selected variable on model’s predictions. We will propose now a solution closely linked with CP profiles, but the issue is also discussed also in the next chapter.</p>
<p>To assign importance to CP profiles, we can use the concept of profile oscillations. In particular, the larger influence of an explanatory variable on prediction at a particular instance, the larger the fluctuations along the corresponding CP profile. For a variable that exercises little or no influence on model prediction, the profile will be flat or will barely change. Figure <a href="ceterisParibus.html#fig:CPVIPprofiles">4.5</a> illustrates the idea behind measuring oscillations. The larger the highlighted area, the more important is the variable.</p>
<p>Let us formalize this concept now. Denote by <span class="math inline">\(g^j(z)\)</span> the probability density function of the distribution of the <span class="math inline">\(j\)</span>-th explanatory variable. The summary measure of the variable’s importance for model prediction at point <span class="math inline">\(x\)</span>, <span class="math inline">\(vip^{CP}_j(x)\)</span>, computed based on the variable’s CP profile, is defined as follows:</p>
<p><span class="math display">\[
vip^{CP}_j(x^*) = \int_{\mathcal R} |CP^{f,j,x^*}(z) - f(x^*)| g^j(z)dz=E_{X_j}[|CP^{f,j,x^*}(X_j) - f(x^*)|].
\]</span>
Thus, <span class="math inline">\(vip^{CP}_j(x^*)\)</span> is the expected absolute deviation of the CP profile from the model prediction for <span class="math inline">\(x^*\)</span> over the distribution of the <span class="math inline">\(j\)</span>-th explanatory variable. A straightforward estimator of <span class="math inline">\(vip^{CP}_j(x^*)\)</span> is</p>
<p><span class="math display">\[
\widehat{ vip^{CP}_j(x^*)} = \frac 1n \sum_{i=1}^n |CP^{f,j,x^*}(x^{*j}_i) - f(x^*)|,
\]</span>
where index <span class="math inline">\(i\)</span> goes through all observations in a dataset.</p>
<div class="figure" style="text-align: center"><span id="fig:CPVIPprofiles"></span>
<img src="figure/CP_VIP_profiles.png" alt="(fig:CPVIPprofiles) The value of the red are area summarizes CP oscillations and provides the average absolute deviations between the CP profile and the instance prediction. This example is for the random forest model and the 'apartments' dataset" width="50%" />
<p class="caption">
Figure 4.5: (fig:CPVIPprofiles) The value of the red are area summarizes CP oscillations and provides the average absolute deviations between the CP profile and the instance prediction. This example is for the random forest model and the ‘apartments’ dataset
</p>
</div>
<p>Figure <a href="ceterisParibus.html#fig:CPVIP1">4.6</a> provides a plot of variable importance measures for different variables for the random forest model and the ‘apartments’ dataset. The wider the interval, the larger the CP-profile oscillations for a particular explanatory variable. Thus, Figure <a href="ceterisParibus.html#fig:CPVIP1">4.6</a> indicates that the most important variable for prediction for observation with ID equal to 1001 is “surface”, followed by “floor”.</p>
<div class="figure" style="text-align: center"><span id="fig:CPVIP1"></span>
<img src="figure/cp_vip_1.png" alt="(fig:CPVIP1) Variable-importance measures calculated for Ceteris-paribus profiles for observation ID: 1001. This example is for the random forest model and the 'apartments' dataset" width="40%" />
<p class="caption">
Figure 4.6: (fig:CPVIP1) Variable-importance measures calculated for Ceteris-paribus profiles for observation ID: 1001. This example is for the random forest model and the ‘apartments’ dataset
</p>
</div>
<p>Note that the importance of an explanatory variable for instance prediction may be very different for different points <span class="math inline">\(x^*\)</span>. For example, consider model
<span class="math display">\[
f(x_1, x_2) = x_1 * x_2,
\]</span>
where <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> take values in <span class="math inline">\([0,1]\)</span>. Consider prediction for an observation described by vector <span class="math inline">\(x^* = (0,1)\)</span>. In that case, the importance of <span class="math inline">\(X_1\)</span> is larger than <span class="math inline">\(X_2\)</span>. This is because the CP profile for the first variable, given by the values of function <span class="math inline">\(f(z,1)=z\)</span>, will have oscillations, while the profile for the second variable will show no oscillation, because it is given by function <span class="math inline">\(f(0,z)=0\)</span>. Obviously, the situation is reversed for <span class="math inline">\(x^*=(1,0)\)</span>.</p>
</div>
<div id="ceterisParibus2d" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Two-dimensional (2D) Ceteris-paribus Profiles</h3>
<p>The definition of CP profiles, given in Section <a href="ceterisParibus.html#ceterisParibus1d">4.3.1</a>, may be easily extended to two or more explanatory variables. Also, the definition of the variance importance measure <span class="math inline">\(vip^{CP}_j(x^*)\)</span> have a straightforward extension for a larger number of variables. For instance, a two-dimensional (2D) CP profile for model <span class="math inline">\(f\)</span>, explanatory variables <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span>, and point <span class="math inline">\(x^*\)</span> is defined as follows:</p>
<p><span class="math display">\[
CP^{f, (j,k), x^*}(z_1, z_2) \equiv f(x^*|^{(j,k)} = (z_1,z_2)).
\]</span>
Thus, 2D CP profile is a function that provides the dependence of the (approximate) expected value (prediction) of the model for <span class="math inline">\(Y\)</span> on the values of <span class="math inline">\(j\)</span>-th and <span class="math inline">\(k\)</span>-th explanatory variables <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span>, respectively, where <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> are taken to go through the range of values typical for the variables, and values of all other variables in <span class="math inline">\(x^*\)</span> are kept fixed at the values present in <span class="math inline">\(x^*\)</span>.</p>
<p>The corresponding variance importance measure would be defined as follows:
<span class="math display">\[
vip^{CP}_{j,k}(x^*) = \int_{\mathcal R}\int_{\mathcal R} |CP^{f,(j,k),x^*}(z_1,z_2) - f(x^*)| g^{j,k}(z_1,z_2)dz_1dz_2=E_{X_j,X_k}[|CP^{f,j,x^*}(X_j,X_k) - f(x^*)|],
\]</span>
where the expected value is taken over the joint distribution of the <span class="math inline">\(j\)</span>-th and <span class="math inline">\(k\)</span>-th explanatory variable.</p>
<p>Such multi-dimensional extensions are useful to check if, for instance, the model involves interactions. In particular, presence of pairwise interactions may be detected with two-dimensional (2D) CP profiles.</p>
<p>A natural way to visualise 2D CP profiles is to use a heatmap, as in Figure <a href="ceterisParibus.html#fig:CP2Dsurflor">4.7</a>. [TOMASZ: WHAT DOES THE PLOT SUGGEST?]</p>
<div class="figure" style="text-align: center"><span id="fig:CP2Dsurflor"></span>
<img src="figure/cp_2d_surf_floor.png" alt="(fig:CP2Dsurflor) Ceteris-paribus profile for a pair of explanatory variables (floor and surface) for a model for the price of an apartment. Black cross marks the coordinates of the point of interest." width="60%" />
<p class="caption">
Figure 4.7: (fig:CP2Dsurflor) Ceteris-paribus profile for a pair of explanatory variables (floor and surface) for a model for the price of an apartment. Black cross marks the coordinates of the point of interest.
</p>
</div>
<p>If the number of pairs of explanatory variables is small or moderate, then it is possible to present 2D CP profiles for all pairs of variables, as illustrated in Figure <a href="ceterisParibus.html#fig:CP2Dall">4.8</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:CP2Dall"></span>
<img src="figure/cp_2d_all.png" alt="(fig:CP2Dall) Ceteris-paribus profile for all pairs of explanatory variables  for a model for the price of an apartment." width="90%" />
<p class="caption">
Figure 4.8: (fig:CP2Dall) Ceteris-paribus profile for all pairs of explanatory variables for a model for the price of an apartment.
</p>
</div>
<p>If the number of pairs is large, we can use the variable importance measure to order the pairs based on their importance and select the most important pairs for purposes of illustration.</p>
</div>
</div>
<div id="example-local-model-fidelity" class="section level2">
<h2><span class="header-section-number">4.4</span> Example: Local Model Fidelity</h2>
<p>It may happen that global performance of the model is good, while for some particular observations the fit is very bad. Local fidelity helps to understand how good is the model fit at a particular observation. In this section we show how to use CP profiles to validate local model fidelity.</p>
<p>The idea behind fidelity plots is to select a number of observatons (“neighbors”) from the validation dataset that are closest to the observation of interest. Then for the selected observations we plot CP profiles and check how stable they are. Additionally, if we know true values of the dependent variable for the selected neighbours, we may add residuals to the plot to evaluate the local fit of the model.</p>
<p>An example of a fidelity plot is presented in Figure <a href="ceterisParibus.html#fig:CPfidelity1">4.9</a>. [TOMASZ: WHICH DATA? WHICH MODEL? WHICH OBSERVATION WAS SELECTED? HOW THE NEIGHBORS WERE SELECTED?] The dark black line shows the CP profile for the observation of interest. Grey lines show CP profiles for the neihgbors. The red intervals indicate the residuals, i.e., the difference between the model prediction and the observed value of the dependent variable (indicated by the red dot). [TOMASZ: WHY ARE THERE INTERVALS THAT DO NOT START AT THE CP PROFILE?] In this example the intervals show that the predicted values are larger than the observed values of the dependent variable for all neighbors, i.e., all residuals are negative. Thus, the model may provide biased predictions around the observation of interest. [TOMASZ: IF WE FOCUS ON THE RESIDUALS, WHY DO WE NEED THE PROFILES?]</p>
<div class="figure" style="text-align: center"><span id="fig:CPfidelity1"></span>
<img src="figure/cp_fidelity_1.png" alt="(fig:CPfidelity1) Local fidelity plot [TOMASZ: WHICH DATA? WHICH OBSERVATION? WHICH MODEL?]. The black line shows the Cetris-paribus (CP) profile for the observation of interest. Grey lines show CP profiles for the neighbors of the observation from a validation dataset. Red intervals correspond to residuals - each interval extends from the model prediction for a selected neighbor to the observed value of the dependent variable (indicated by the red dot)." width="70%" />
<p class="caption">
Figure 4.9: (fig:CPfidelity1) Local fidelity plot [TOMASZ: WHICH DATA? WHICH OBSERVATION? WHICH MODEL?]. The black line shows the Cetris-paribus (CP) profile for the observation of interest. Grey lines show CP profiles for the neighbors of the observation from a validation dataset. Red intervals correspond to residuals - each interval extends from the model prediction for a selected neighbor to the observed value of the dependent variable (indicated by the red dot).
</p>
</div>
<p>This observation is confirmed by Figure <a href="ceterisParibus.html#fig:CPfidelityBoxplot">4.10</a>, which compares the distribution of all residuals from the validation dataset and the distribution of residuals for the neighbors. The plot shows that the residuals for neighbors are shifted towards higher values. [TOMASZ: THIS MAKES NO SENSE. THE PLOT SUGGESTS POSITIVE RESIDUALS, WHEREAS ABOVE IT IS ARGUED THAT THE RESIDUALS ARE NEGATIVE.] This suggests that the model predictions around the observation of interest are biased.</p>
<div class="figure" style="text-align: center"><span id="fig:CPfidelityBoxplot"></span>
<img src="figure/cp_fidelity_boxplot.png" alt="(fig:CPfidelityBoxplot) Distribution of residuals for the entire  validation dataset (grey boxplot) and for 15 selected neighbors to the observation of interest (red boxplot)." width="70%" />
<p class="caption">
Figure 4.10: (fig:CPfidelityBoxplot) Distribution of residuals for the entire validation dataset (grey boxplot) and for 15 selected neighbors to the observation of interest (red boxplot).
</p>
</div>
</div>
<div id="pros-and-cons" class="section level2">
<h2><span class="header-section-number">4.5</span> Pros and cons</h2>
<p>CP profiles offer a uniform, easy to comunicate and extendable approach to model exploration. Their graphical representation is easy to understand and explain. It is possible to present profiles for many variables or models in a single plot. By using the expected oscillations of CP profiles it is possible to select the most important variables for instane prediction. Two-dimensional CP profiles can be used to, for instance, identify the presence of pairwise interactions in a model.</p>
<p>There are several issues related to the use of the CP profiles. If explanatory variables are correlated, then changing one variable implies a change in the other. In such case, the application of the <em>Ceteris paribus</em> principle may lead to unrealistic settings, as it is not possible to keep one variable fixed while changing the other one. A special case are interactions, which require the use of 2D CP profiles that are more complex than the 1D ones. Also, in case of a model with hundreds or thousands of variables, the number of plots to inspect may be daunting. Finally, visualization of CP profiles for factors (categorical explanatory variables) is not trivial, especially for factors with many nominal (unordered) categories (like, for example, a ZIP-code).</p>
</div>
<div id="code-snippets-for-r" class="section level2">
<h2><span class="header-section-number">4.6</span> Code snippets for R</h2>
<p>In this section we present key features of the R package <code>ingredients</code> <span class="citation">(<span class="citeproc-not-found" data-reference-id="R-ingredients"><strong>???</strong></span>)</span> which is a part of <code>DALEXverse</code> and covers all methods presented in this chapter. More details and examples can be found at <code>https://modeloriented.github.io/ingredients/</code>.</p>
<p>There are also other R packages that offer similar methods. For example, an interesting approach to model exploration based on similar principles [TOMASZ: CETERIS-PARIBUS? IF NOT, WHY MENTION IT HERE?] is implemented in the <code>condvis</code> package <span class="citation">(O’Connell, Hurley, and Domijan <a href="#ref-JSSv081i05">2017</a>)</span>.</p>
<p>In this section we use a random forest <span class="citation">(Breiman et al. <a href="#ref-R-randomForest">2018</a>)</span> model <code>model_titanic_rf</code> developed for the Titanic dataset (see Section @ref{model_titanic_rf}). In particular, we deal with a binary classification problem - we want to predict the probability of survival for a selected passenger.</p>
<p>CP profiles are calculated in four steps with the <code>ingredients</code> package.</p>
<p><strong>1. Create an explainer - wrapper around model and validation data.</strong></p>
<p>Model-objects created with different libraries may have different internal structures. Thus, first, we have got to create a wrapper around the model. Toward this end we use the <code>explain()</code> function from the <code>DALEX</code> package <span class="citation">(Biecek <a href="#ref-R-DALEX">2018</a><a href="#ref-R-DALEX">b</a>)</span>. The function requires at least four arguments: a model-object, a validation data frame, observed values of the dependent variable for the validation data, and a function that returns prediction scores. In the example below we use the training data as the validation dataset. Moreover, we do not specify the fourth argument, because the prediction-score function for <code>randomForest</code> objects is pre-implemented. [TOMASZ: SO, THE STATEMENT OF THE REQUIREMENT OF FOUR ARGUMENTS IS NOT PRECISE.]</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;DALEX&quot;</span>)</a>
<a class="sourceLine" id="cb33-2" data-line-number="2">explainer_titanic_rf &lt;-<span class="st"> </span><span class="kw">explain</span>(model_titanic_rf, <span class="dt">data =</span> titanic_small, <span class="dt">y =</span> titanic_small<span class="op">$</span>Survived)</a></code></pre></div>
<p><strong>2. Define the instance (observation) of interest.</strong></p>
<p>CP profiles explore model around a single observation. Thus, in the exampe below, we define data frame <code>johny_d</code> with a single row. It describes an 8-years old boy that travels in the first class without parents and siblings. Then, we obtain the model prediction for this instance with the help of the `predict()’ function. In particular, we compute the probability for each category of the dependent binary variable.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1">johny_d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb34-2" data-line-number="2">  <span class="dt">Pclass =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb34-3" data-line-number="3">  <span class="dt">Sex =</span> <span class="kw">factor</span>(<span class="st">&quot;male&quot;</span>, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;female&quot;</span>, <span class="st">&quot;male&quot;</span>)),</a>
<a class="sourceLine" id="cb34-4" data-line-number="4">  <span class="dt">Age =</span> <span class="dv">8</span>,</a>
<a class="sourceLine" id="cb34-5" data-line-number="5">  <span class="dt">SibSp =</span> <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb34-6" data-line-number="6">  <span class="dt">Parch =</span> <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb34-7" data-line-number="7">  <span class="dt">Fare =</span> <span class="dv">72</span>,</a>
<a class="sourceLine" id="cb34-8" data-line-number="8">  <span class="dt">Embarked =</span> <span class="kw">factor</span>(<span class="st">&quot;C&quot;</span>, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;N&quot;</span>,<span class="st">&quot;C&quot;</span>,<span class="st">&quot;Q&quot;</span>,<span class="st">&quot;S&quot;</span>))</a>
<a class="sourceLine" id="cb34-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb34-10" data-line-number="10"></a>
<a class="sourceLine" id="cb34-11" data-line-number="11"><span class="kw">predict</span>(model_titanic_rf, johny_d, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)</a></code></pre></div>
<pre><code>##      0    1
## 1 0.46 0.54
## attr(,&quot;class&quot;)
## [1] &quot;matrix&quot; &quot;votes&quot;</code></pre>
<p><strong>3. Calculate CP profiles</strong></p>
<p>To obtain CP profiles, we use the <code>ceteris_paribus()</code> function. It requires the explainer-object and the instance data frame as arguments. By default, CP profiles are calculated for all numerical variables. To select a subset of variables, the <code>variables</code> argument can be used. As a result, the function yields a data frame with model predictions around the instance of interest.</p>
<p>[TOMASZ: WHAT IS THE DEFAULT OUTPUT WHEN PRINTING THE RESULTING DATA FRAME?]</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1">devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;ModelOriented/ingredients&quot;</span>)</a>
<a class="sourceLine" id="cb36-2" data-line-number="2"><span class="kw">library</span>(<span class="st">&quot;ingredients&quot;</span>)</a>
<a class="sourceLine" id="cb36-3" data-line-number="3">cp_titanic_rf &lt;-<span class="st"> </span><span class="kw">ceteris_paribus</span>(explainer_titanic_rf, johny_d, </a>
<a class="sourceLine" id="cb36-4" data-line-number="4">                            <span class="dt">variables =</span> <span class="kw">c</span>(<span class="st">&quot;Age&quot;</span>, <span class="st">&quot;Fare&quot;</span>, <span class="st">&quot;Pclass&quot;</span>, <span class="st">&quot;SibSp&quot;</span>))</a>
<a class="sourceLine" id="cb36-5" data-line-number="5">cp_titanic_rf</a></code></pre></div>
<pre><code>## Top profiles    : 
##     Pclass  Sex  Age SibSp Parch Fare Embarked _yhat_ _vname_ _ids_
## 1        1 male 0.42     0     0   72        C  0.578     Age     1
## 1.1      1 male 1.00     0     0   72        C  0.574     Age     1
## 1.2      1 male 2.00     0     0   72        C  0.564     Age     1
## 1.3      1 male 3.00     0     0   72        C  0.568     Age     1
## 1.4      1 male 4.00     0     0   72        C  0.554     Age     1
## 1.5      1 male 5.00     0     0   72        C  0.552     Age     1
##          _label_
## 1   randomForest
## 1.1 randomForest
## 1.2 randomForest
## 1.3 randomForest
## 1.4 randomForest
## 1.5 randomForest
## 
## 
## Top observations:
##   Pclass  Sex Age SibSp Parch Fare Embarked _yhat_      _label_ _ids_
## 1      1 male   8     0     0   72        C   0.54 randomForest     1</code></pre>
<p><strong>4. Plot CP profiles.</strong></p>
<p>To obtain a graphical represenation of CP profiles, the generic <code>plot()</code> function can be applied to the data frame returend by the <code>ceteris_paribus()</code> function. It returns a <code>ggplot2</code> object that can be processed if needed.</p>
<p>The resulting plot can be enriched with additional data by adding functions <code>show_rugs</code> (adds rugs for the selected points), <code>show_observations</code> (adds observations), or <code>show_aggreagated_profiles</code> (see Section @ref{variableEngeneering}). All these functions can take additional arguments to modify size, color or linetype.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">plot</span>(cp_titanic_rf) <span class="op">+</span></a>
<a class="sourceLine" id="cb38-2" data-line-number="2"><span class="st">  </span><span class="kw">show_observations</span>(cp_titanic_rf, </a>
<a class="sourceLine" id="cb38-3" data-line-number="3">        <span class="dt">variables =</span> <span class="kw">c</span>(<span class="st">&quot;Age&quot;</span>, <span class="st">&quot;Fare&quot;</span>, <span class="st">&quot;Pclass&quot;</span>, <span class="st">&quot;SibSp&quot;</span>)) </a></code></pre></div>
<p><img src="PM_VEE_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>A very useful feature of the CP profiles is that profiles for two or more models may be superimposed in a single plot. Such an approach is sometimes called “contrastive explanations” and is very helpful for model comparisons. We will illustrate this approach by comparing the profiles for the random forest model with those obtained for a logistic regression model (see Section @ref{model_titanic_lmr}).</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;rms&quot;</span>)</a>
<a class="sourceLine" id="cb39-2" data-line-number="2">model_titanic_lmr &lt;-<span class="st"> </span><span class="kw">lrm</span>(Survived <span class="op">==</span><span class="st"> &quot;1&quot;</span> <span class="op">~</span><span class="st"> </span>Pclass <span class="op">+</span><span class="st"> </span>Sex <span class="op">+</span><span class="st"> </span><span class="kw">rcs</span>(Age) <span class="op">+</span><span class="st"> </span>SibSp <span class="op">+</span></a>
<a class="sourceLine" id="cb39-3" data-line-number="3"><span class="st">                   </span>Parch <span class="op">+</span><span class="st"> </span>Fare <span class="op">+</span><span class="st"> </span>Embarked, titanic_small)</a>
<a class="sourceLine" id="cb39-4" data-line-number="4">explainer_titanic_lmr &lt;-<span class="st"> </span><span class="kw">explain</span>(model_titanic_lmr, <span class="dt">data =</span> titanic_small, <span class="dt">y =</span> titanic_small<span class="op">$</span>Survived,</a>
<a class="sourceLine" id="cb39-5" data-line-number="5">                                 <span class="dt">predict_function =</span> <span class="cf">function</span>(m,x) <span class="kw">predict</span>(m,x,<span class="dt">type =</span> <span class="st">&quot;fitted&quot;</span>))</a>
<a class="sourceLine" id="cb39-6" data-line-number="6">cp_titanic_lmr &lt;-<span class="st"> </span><span class="kw">ceteris_paribus</span>(explainer_titanic_lmr, johny_d, </a>
<a class="sourceLine" id="cb39-7" data-line-number="7">        <span class="dt">variables =</span> <span class="kw">c</span>(<span class="st">&quot;Age&quot;</span>, <span class="st">&quot;Fare&quot;</span>, <span class="st">&quot;Pclass&quot;</span>, <span class="st">&quot;SibSp&quot;</span>))</a>
<a class="sourceLine" id="cb39-8" data-line-number="8">cp_titanic_lmr</a></code></pre></div>
<pre><code>## Top profiles    : 
##     Pclass  Sex  Age SibSp Parch Fare Embarked    _yhat_ _vname_ _ids_
## 1        1 male 0.42     0     0   72        C 0.9784112     Age     1
## 1.1      1 male 1.00     0     0   72        C 0.9752918     Age     1
## 1.2      1 male 2.00     0     0   72        C 0.9688533     Age     1
## 1.3      1 male 3.00     0     0   72        C 0.9608045     Age     1
## 1.4      1 male 4.00     0     0   72        C 0.9507814     Age     1
## 1.5      1 male 5.00     0     0   72        C 0.9383767     Age     1
##     _label_
## 1       glm
## 1.1     glm
## 1.2     glm
## 1.3     glm
## 1.4     glm
## 1.5     glm
## 
## 
## Top observations:
##   Pclass  Sex Age SibSp Parch Fare Embarked    _yhat_ _label_ _ids_
## 1      1 male   8     0     0   72        C 0.8836101     glm     1</code></pre>
<p>After obtaining the CP profiles for the logistic regression model, we can use function <code>plot()</code> to compare profiles for both models in a single chart. Toward this aim, we use both data frames as arguments for the function. The additional argument <code>color = &quot;_label_&quot;</code> sets color as a key for model.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1"><span class="kw">plot</span>(cp_titanic_rf, cp_titanic_lmr, <span class="dt">color =</span> <span class="st">&quot;_label_&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb41-2" data-line-number="2"><span class="st">  </span><span class="kw">show_observations</span>(cp_titanic_rf, cp_titanic_lmr,</a>
<a class="sourceLine" id="cb41-3" data-line-number="3">        <span class="dt">variables =</span> <span class="kw">c</span>(<span class="st">&quot;Age&quot;</span>, <span class="st">&quot;Fare&quot;</span>, <span class="st">&quot;Pclass&quot;</span>, <span class="st">&quot;SibSp&quot;</span>)) </a></code></pre></div>
<p><img src="PM_VEE_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>[TOMASZ: WE SHOULD PROVIDE COMMENTS FOR THE RESULTING PLOT.]</p>
<p><strong>Oscillations</strong></p>
<p>To calculate the variable importance measures based on the oscillations of CP profiles, we use the <code>calculate_oscillations()</code> function. We apply the function to the data frame returned by the <code>ceteris_paribus()</code> function. The result for the prediction of the Random Forest model for <code>johny_d</code> indicates that the most important variable is the age. To obtain a graphical representaton of the result, we can apply function <code>plot()</code> to the object returned by the <code>calculate_oscillations()</code> function.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1">co_titanic_rf &lt;-<span class="st"> </span><span class="kw">calculate_oscillations</span>(cp_titanic_rf)</a>
<a class="sourceLine" id="cb42-2" data-line-number="2">co_titanic_rf</a></code></pre></div>
<pre><code>##   _vname_ _ids_ oscillations
## 1     Age     1   0.11161905
## 2    Fare     1   0.06315000
## 4   SibSp     1   0.03971429
## 3  Pclass     1   0.03000000</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1"><span class="kw">plot</span>(co_titanic_rf)</a></code></pre></div>
<p><img src="PM_VEE_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p><strong>Two-dimensional Ceteris-paribus Profiles</strong></p>
<p>To create 2D CP profiles, we use the <code>ceteris_paribus_2d()</code> function. Its syntax resembles the one of the <code>ceteris_paribus()</code> function.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1">cp2d_titanic_rf &lt;-<span class="st"> </span><span class="kw">ceteris_paribus_2d</span>(explainer_titanic_rf, <span class="dt">observation =</span> johny_d, </a>
<a class="sourceLine" id="cb45-2" data-line-number="2">                 <span class="dt">variables =</span> <span class="kw">c</span>(<span class="st">&quot;Age&quot;</span>,<span class="st">&quot;Fare&quot;</span>, <span class="st">&quot;Pclass&quot;</span>))</a>
<a class="sourceLine" id="cb45-3" data-line-number="3"><span class="kw">plot</span>(cp2d_titanic_rf, <span class="dt">split_ncol =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="PM_VEE_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>[TOMASZ: WE SHOULD PROVIDE COMMENTS FOR THE RESULTING PLOT.]</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ICEbox">
<p>Goldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. “Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.” <em>Journal of Computational and Graphical Statistics</em> 24 (1): 44–65. <a href="https://doi.org/10.1080/10618600.2014.907095">https://doi.org/10.1080/10618600.2014.907095</a>.</p>
</div>
<div id="ref-Tufte1986">
<p>Tufte, Edward R. 1986. <em>The Visual Display of Quantitative Information</em>. Cheshire, CT, USA: Graphics Press.</p>
</div>
<div id="ref-JSSv081i05">
<p>O’Connell, Mark, Catherine Hurley, and Katarina Domijan. 2017. “Conditional Visualization for Statistical Models: An Introduction to the Condvis Package in R.” <em>Journal of Statistical Software, Articles</em> 81 (5): 1–20. <a href="https://doi.org/10.18637/jss.v081.i05">https://doi.org/10.18637/jss.v081.i05</a>.</p>
</div>
<div id="ref-R-randomForest">
<p>Breiman, Leo, Adele Cutler, Andy Liaw, and Matthew Wiener. 2018. <em>RandomForest: Breiman and Cutler’s Random Forests for Classification and Regression</em>. <a href="https://CRAN.R-project.org/package=randomForest">https://CRAN.R-project.org/package=randomForest</a>.</p>
</div>
<div id="ref-R-DALEX">
<p>Biecek, Przemyslaw. 2018b. <em>DALEX: Descriptive mAchine Learning Explanations</em>. <a href="https://pbiecek.github.io/DALEX/">https://pbiecek.github.io/DALEX/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="PredictionExplainers.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="variableAttributionMethods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["PM_VEE.pdf", "PM_VEE.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
