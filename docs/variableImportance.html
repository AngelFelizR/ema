<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Predictive Models: Visualisal Exploration, Explanation and Debugging</title>
  <meta name="description" content="This book introduces key concepts for exploration, explanation and visualization of complex predictive models.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Predictive Models: Visualisal Exploration, Explanation and Debugging" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces key concepts for exploration, explanation and visualization of complex predictive models." />
  <meta name="github-repo" content="pbiecek/PM_VEE" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Predictive Models: Visualisal Exploration, Explanation and Debugging" />
  
  <meta name="twitter:description" content="This book introduces key concepts for exploration, explanation and visualization of complex predictive models." />
  

<meta name="author" content="Przemyslaw Biecek and Tomasz Burzykowski">


<meta name="date" content="2019-03-31">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction-3.html">
<link rel="next" href="variableEngeneering.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Predictive Models:<br/> Visualisation, Exploration and Explanation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#the-aim-of-the-book"><i class="fa fa-check"></i><b>1.1</b> The aim of the book</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#three-single-laws"><i class="fa fa-check"></i><b>1.2</b> A bit of philosophy: three laws of model explanation</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#terminology"><i class="fa fa-check"></i><b>1.3</b> Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#white-box-models-vs.black-box-models"><i class="fa fa-check"></i><b>1.4</b> White-box models vs. black-box models</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#model-visualization-exploration-and-explanation"><i class="fa fa-check"></i><b>1.5</b> Model visualization, exploration, and explanation</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#model-agnostic-vs.model-specific-approach"><i class="fa fa-check"></i><b>1.6</b> Model-agnostic vs. model-specific approach</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#code-snippets"><i class="fa fa-check"></i><b>1.7</b> Code snippets</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#the-structure-of-the-book"><i class="fa fa-check"></i><b>1.8</b> The structure of the book</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#thanksto"><i class="fa fa-check"></i><b>1.9</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html"><i class="fa fa-check"></i><b>2</b> Data Sets</a><ul>
<li class="chapter" data-level="2.1" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#TitanicDataset"><i class="fa fa-check"></i><b>2.1</b> Sinking of the RMS Titanic</a><ul>
<li class="chapter" data-level="2.1.1" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#data-cleaning"><i class="fa fa-check"></i><b>2.1.1</b> Data cleaning</a></li>
<li class="chapter" data-level="2.1.2" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#data-exploration"><i class="fa fa-check"></i><b>2.1.2</b> Data exploration</a></li>
<li class="chapter" data-level="2.1.3" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#model_titanic_lmr"><i class="fa fa-check"></i><b>2.1.3</b> Logistic regression is always a good choice</a></li>
<li class="chapter" data-level="2.1.4" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#model_titanic_rf"><i class="fa fa-check"></i><b>2.1.4</b> Random Forest to the rescue</a></li>
<li class="chapter" data-level="2.1.5" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#gradient-boosting-for-interactions"><i class="fa fa-check"></i><b>2.1.5</b> Gradient boosting for interactions</a></li>
<li class="chapter" data-level="2.1.6" data-path="DataSetsIntro.html"><a href="DataSetsIntro.html#model-predictions"><i class="fa fa-check"></i><b>2.1.6</b> Model predictions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="instance-level-explanation.html"><a href="instance-level-explanation.html"><i class="fa fa-check"></i>Instance-level explanation</a></li>
<li class="chapter" data-level="3" data-path="PredictionExplainers.html"><a href="PredictionExplainers.html"><i class="fa fa-check"></i><b>3</b> Introduction</a></li>
<li class="chapter" data-level="4" data-path="ceterisParibus.html"><a href="ceterisParibus.html"><i class="fa fa-check"></i><b>4</b> What-If analysis with the Ceteris Paribus Profiles</a><ul>
<li class="chapter" data-level="4.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#intuition"><i class="fa fa-check"></i><b>4.2</b> Intuition</a></li>
<li class="chapter" data-level="4.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#the-method"><i class="fa fa-check"></i><b>4.3</b> The Method</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#ceterisParibus1d"><i class="fa fa-check"></i><b>4.3.1</b> 1D Ceteris Paribus Profiles</a></li>
<li class="chapter" data-level="4.3.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#oscillations"><i class="fa fa-check"></i><b>4.3.2</b> Profile oscillations</a></li>
<li class="chapter" data-level="4.3.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#d-profiles"><i class="fa fa-check"></i><b>4.3.3</b> 2D profiles</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ceterisParibus.html"><a href="ceterisParibus.html#local-model-fidelity"><i class="fa fa-check"></i><b>4.4</b> Local model fidelity</a></li>
<li class="chapter" data-level="4.5" data-path="ceterisParibus.html"><a href="ceterisParibus.html#pros-and-cons"><i class="fa fa-check"></i><b>4.5</b> Pros and cons</a></li>
<li class="chapter" data-level="4.6" data-path="ceterisParibus.html"><a href="ceterisParibus.html#code-snippets-for-r"><i class="fa fa-check"></i><b>4.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html"><i class="fa fa-check"></i><b>5</b> Variable attribution for linear models</a><ul>
<li class="chapter" data-level="5.1" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#intuition-1"><i class="fa fa-check"></i><b>5.2</b> Intuition</a></li>
<li class="chapter" data-level="5.3" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#method"><i class="fa fa-check"></i><b>5.3</b> Method</a></li>
<li class="chapter" data-level="5.4" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#example-wine-quality"><i class="fa fa-check"></i><b>5.4</b> Example: Wine quality</a></li>
<li class="chapter" data-level="5.5" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#pros-and-cons-1"><i class="fa fa-check"></i><b>5.5</b> Pros and Cons</a></li>
<li class="chapter" data-level="5.6" data-path="variableAttributionMethods.html"><a href="variableAttributionMethods.html#code-snippets-1"><i class="fa fa-check"></i><b>5.6</b> Code snippets</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="breakDown.html"><a href="breakDown.html"><i class="fa fa-check"></i><b>6</b> Variable attributions</a><ul>
<li class="chapter" data-level="6.1" data-path="breakDown.html"><a href="breakDown.html#intuition-2"><i class="fa fa-check"></i><b>6.1</b> Intuition</a></li>
<li class="chapter" data-level="6.2" data-path="breakDown.html"><a href="breakDown.html#method-1"><i class="fa fa-check"></i><b>6.2</b> Method</a></li>
<li class="chapter" data-level="6.3" data-path="breakDown.html"><a href="breakDown.html#example-hire-or-fire"><i class="fa fa-check"></i><b>6.3</b> Example: Hire or Fire?</a></li>
<li class="chapter" data-level="6.4" data-path="breakDown.html"><a href="breakDown.html#pros-and-cons-2"><i class="fa fa-check"></i><b>6.4</b> Pros and cons</a></li>
<li class="chapter" data-level="6.5" data-path="breakDown.html"><a href="breakDown.html#code-snippets-for-r-1"><i class="fa fa-check"></i><b>6.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html"><i class="fa fa-check"></i><b>7</b> Variable attribution with interactions</a><ul>
<li class="chapter" data-level="7.1" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#intuition-3"><i class="fa fa-check"></i><b>7.1</b> Intuition</a></li>
<li class="chapter" data-level="7.2" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#method-2"><i class="fa fa-check"></i><b>7.2</b> Method</a></li>
<li class="chapter" data-level="7.3" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#example-hire-or-fire-1"><i class="fa fa-check"></i><b>7.3</b> Example: Hire or Fire?</a></li>
<li class="chapter" data-level="7.4" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#break-down-plots"><i class="fa fa-check"></i><b>7.4</b> Break Down Plots</a></li>
<li class="chapter" data-level="7.5" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#pros-and-cons-3"><i class="fa fa-check"></i><b>7.5</b> Pros and cons</a></li>
<li class="chapter" data-level="7.6" data-path="variable-attribution-with-interactions.html"><a href="variable-attribution-with-interactions.html#code-snippets-for-r-2"><i class="fa fa-check"></i><b>7.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>8</b> Average variable attributions</a><ul>
<li class="chapter" data-level="8.1" data-path="shapley.html"><a href="shapley.html#intuition-4"><i class="fa fa-check"></i><b>8.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2" data-path="shapley.html"><a href="shapley.html#method-3"><i class="fa fa-check"></i><b>8.2</b> Method</a></li>
<li class="chapter" data-level="8.3" data-path="shapley.html"><a href="shapley.html#example-hire-or-fire-2"><i class="fa fa-check"></i><b>8.3</b> Example: Hire or Fire?</a></li>
<li class="chapter" data-level="8.4" data-path="shapley.html"><a href="shapley.html#pros-and-cons-4"><i class="fa fa-check"></i><b>8.4</b> Pros and cons</a></li>
<li class="chapter" data-level="8.5" data-path="shapley.html"><a href="shapley.html#code-snippets-for-r-3"><i class="fa fa-check"></i><b>8.5</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="LIME.html"><a href="LIME.html"><i class="fa fa-check"></i><b>9</b> Local approximations with white-box model</a><ul>
<li class="chapter" data-level="9.1" data-path="LIME.html"><a href="LIME.html#intuition-5"><i class="fa fa-check"></i><b>9.1</b> Intuition</a></li>
<li class="chapter" data-level="9.2" data-path="LIME.html"><a href="LIME.html#method-4"><i class="fa fa-check"></i><b>9.2</b> Method</a></li>
<li class="chapter" data-level="9.3" data-path="LIME.html"><a href="LIME.html#example-hire-or-fire-3"><i class="fa fa-check"></i><b>9.3</b> Example: Hire or Fire?</a></li>
<li class="chapter" data-level="9.4" data-path="LIME.html"><a href="LIME.html#pros-and-cons-5"><i class="fa fa-check"></i><b>9.4</b> Pros and cons</a></li>
<li class="chapter" data-level="9.5" data-path="LIME.html"><a href="LIME.html#code-snippets-for-r-4"><i class="fa fa-check"></i><b>9.5</b> Code snippets for R</a><ul>
<li class="chapter" data-level="9.5.1" data-path="LIME.html"><a href="LIME.html#the-lime-pacakge"><i class="fa fa-check"></i><b>9.5.1</b> <strong>The lime pacakge</strong></a></li>
<li class="chapter" data-level="9.5.2" data-path="LIME.html"><a href="LIME.html#the-live-package"><i class="fa fa-check"></i><b>9.5.2</b> <strong>The live package</strong></a></li>
<li class="chapter" data-level="9.5.3" data-path="LIME.html"><a href="LIME.html#the-iml-package"><i class="fa fa-check"></i><b>9.5.3</b> <strong>The iml package</strong></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="comparision-of-prediction-level-explainers.html"><a href="comparision-of-prediction-level-explainers.html"><i class="fa fa-check"></i><b>10</b> Comparision of prediction level explainers</a><ul>
<li class="chapter" data-level="10.1" data-path="comparision-of-prediction-level-explainers.html"><a href="comparision-of-prediction-level-explainers.html#when-to-use"><i class="fa fa-check"></i><b>10.1</b> When to use?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-level-explanations.html"><a href="model-level-explanations.html"><i class="fa fa-check"></i>Model level explanations</a></li>
<li class="chapter" data-level="11" data-path="introduction-3.html"><a href="introduction-3.html"><i class="fa fa-check"></i><b>11</b> Introduction</a><ul>
<li class="chapter" data-level="11.1" data-path="introduction-3.html"><a href="introduction-3.html#approaches-to-model-explanations"><i class="fa fa-check"></i><b>11.1</b> Approaches to model explanations</a></li>
<li class="chapter" data-level="11.2" data-path="introduction-3.html"><a href="introduction-3.html#a-bit-of-philosophy-three-laws-for-model-level-explanations"><i class="fa fa-check"></i><b>11.2</b> A bit of philosophy: Three Laws for Model Level Explanations</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="variableImportance.html"><a href="variableImportance.html"><i class="fa fa-check"></i><b>12</b> Feature Importance</a><ul>
<li class="chapter" data-level="12.1" data-path="variableImportance.html"><a href="variableImportance.html#permutation-based-feature-importance"><i class="fa fa-check"></i><b>12.1</b> Permutation Based Feature Importance</a></li>
<li class="chapter" data-level="12.2" data-path="variableImportance.html"><a href="variableImportance.html#example-titanic"><i class="fa fa-check"></i><b>12.2</b> Example: Titanic</a></li>
<li class="chapter" data-level="12.3" data-path="variableImportance.html"><a href="variableImportance.html#example-price-prediction"><i class="fa fa-check"></i><b>12.3</b> Example: Price prediction</a></li>
<li class="chapter" data-level="12.4" data-path="variableImportance.html"><a href="variableImportance.html#more-models"><i class="fa fa-check"></i><b>12.4</b> More models</a></li>
<li class="chapter" data-level="12.5" data-path="variableImportance.html"><a href="variableImportance.html#level-frequency"><i class="fa fa-check"></i><b>12.5</b> Level frequency</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="variableEngeneering.html"><a href="variableEngeneering.html"><i class="fa fa-check"></i><b>13</b> Feature effects</a></li>
<li class="chapter" data-level="14" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html"><i class="fa fa-check"></i><b>14</b> Partial Dependency Profiles</a><ul>
<li class="chapter" data-level="14.0.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#interactions-and-partial-dependency-profiles"><i class="fa fa-check"></i><b>14.0.1</b> Interactions and Partial Dependency profiles</a></li>
<li class="chapter" data-level="14.0.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#groups-of-partial-dependency-profiles"><i class="fa fa-check"></i><b>14.0.2</b> Groups of Partial Dependency profiles</a></li>
<li class="chapter" data-level="14.0.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#model-comparisons-with-partial-dependency-plots"><i class="fa fa-check"></i><b>14.0.3</b> Model comparisons with Partial Dependency Plots</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="localProfiles.html"><a href="localProfiles.html"><i class="fa fa-check"></i><b>15</b> Local Dependency Profiles</a><ul>
<li class="chapter" data-level="15.1" data-path="localProfiles.html"><a href="localProfiles.html#conditional-marginal-profiles"><i class="fa fa-check"></i><b>15.1</b> Conditional / Marginal Profiles</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html"><i class="fa fa-check"></i><b>16</b> Accumulated Local Profiles</a></li>
<li class="chapter" data-level="17" data-path="factorMerger.html"><a href="factorMerger.html"><i class="fa fa-check"></i><b>17</b> Merging Path Plots and Others</a></li>
<li class="chapter" data-level="18" data-path="other-topics.html"><a href="other-topics.html"><i class="fa fa-check"></i><b>18</b> Other topics</a></li>
<li class="chapter" data-level="19" data-path="modelComparisons.html"><a href="modelComparisons.html"><i class="fa fa-check"></i><b>19</b> Performance Diagnostic</a></li>
<li class="chapter" data-level="20" data-path="modelAuditing.html"><a href="modelAuditing.html"><i class="fa fa-check"></i><b>20</b> Residual Diagnostic</a></li>
<li class="chapter" data-level="21" data-path="concept-drift.html"><a href="concept-drift.html"><i class="fa fa-check"></i><b>21</b> Concept Drift</a><ul>
<li class="chapter" data-level="21.1" data-path="concept-drift.html"><a href="concept-drift.html#introduction-4"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="concept-drift.html"><a href="concept-drift.html#covariate-drift"><i class="fa fa-check"></i><b>21.2</b> Covariate Drift</a></li>
<li class="chapter" data-level="21.3" data-path="concept-drift.html"><a href="concept-drift.html#code-snippets-2"><i class="fa fa-check"></i><b>21.3</b> Code snippets</a></li>
<li class="chapter" data-level="21.4" data-path="concept-drift.html"><a href="concept-drift.html#residual-drift"><i class="fa fa-check"></i><b>21.4</b> Residual Drift</a></li>
<li class="chapter" data-level="21.5" data-path="concept-drift.html"><a href="concept-drift.html#code-snippets-3"><i class="fa fa-check"></i><b>21.5</b> Code snippets</a></li>
<li class="chapter" data-level="21.6" data-path="concept-drift.html"><a href="concept-drift.html#model-drift"><i class="fa fa-check"></i><b>21.6</b> Model Drift</a></li>
<li class="chapter" data-level="21.7" data-path="concept-drift.html"><a href="concept-drift.html#code-snippets-4"><i class="fa fa-check"></i><b>21.7</b> Code snippets</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendixes.html"><a href="appendixes.html"><i class="fa fa-check"></i>Appendixes</a></li>
<li class="chapter" data-level="22" data-path="DataSets.html"><a href="DataSets.html"><i class="fa fa-check"></i><b>22</b> Data Sets</a><ul>
<li class="chapter" data-level="22.1" data-path="DataSets.html"><a href="DataSets.html#HRdataset"><i class="fa fa-check"></i><b>22.1</b> Hire or Fire? HR in Call Center</a></li>
<li class="chapter" data-level="22.2" data-path="DataSets.html"><a href="DataSets.html#apartmentsDataset"><i class="fa fa-check"></i><b>22.2</b> How much does it cost? Price prediction for a square meter</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="Packages.html"><a href="Packages.html"><i class="fa fa-check"></i><b>23</b> Packages</a><ul>
<li class="chapter" data-level="23.1" data-path="Packages.html"><a href="Packages.html#arguments"><i class="fa fa-check"></i><b>23.1</b> Arguments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/pbiecek/DALEX" target="blank">DALEX website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Models: Visualisal Exploration, Explanation and Debugging</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variableImportance" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Feature Importance</h1>
<p>Methods presented in this chapter are useful for assessment of feature importance. There are many possible applications of such methods, for example:</p>
<ul>
<li>Feature importance scores may be used for feature filtering. Features that are not important may be removed from the model training procedure. Removal of the noise shall lead to better models.</li>
<li>Identification of the most important features may be used as a validation of a model against domain knowledge. Just to make sure that it’s not like a single random feature dominates model predictions.</li>
<li>Identification of the most important features may leads to new domain knowledge. Well, we have identified important features.</li>
<li>Comparison of feature importance between different models helps to understand how different models handle particular features.</li>
<li>Ranking of feature importance helps to decide in what order we shall perform further model exploration, in what order we shall examine particular feature effects.</li>
</ul>
<p>There are many methods for assessment of feature importance. In general we may divide them into two groups, methods that are model specific and methods that are model agnostic.</p>
<p>Some models like random forest, gradient boosting, linear models and many others have their own ways to assess feature importance. Such method are linked with the particular structure of the model. In terms of linear models such specific measures are linked with normalized regression coefficients of p-values. For tree based ensembles such measures may be based on utilization of particular features in particular trees, see <span class="citation">(Foster <a href="#ref-xgboostExplainer">2017</a>)</span> for gradient boosting or <span class="citation">(Paluszynska and Biecek <a href="#ref-randomForestExplainer">2017</a><a href="#ref-randomForestExplainer">a</a>)</span> for random forest.</p>
<p>But in this book we are focused on methods that are model agnostic. The may reason for that is</p>
<ul>
<li>First, be able to apply this method to any predictive model or ensemble of models.</li>
<li>Second, (which is maybe even more important) to be able to compare feature importance between models despite differences in their structure.</li>
</ul>
<p>Model agnostic methods cannot assume anything about the model structure and we do not want to refit a model. The method that is presented below is described in details in the <span class="citation">(Fisher, Rudin, and Dominici <a href="#ref-variableImportancePermutations">2018</a>)</span>.
The main idea is to measure how much the model fit will decrease if a selected feature or group of features will be cancelled out. Here cancellation means perturbations like resampling from empirical distribution of just permutation.</p>
<p>The method can be used to measure importance of single features, pairs of features or larger tuples For the simplicity below we describe algorithm for single features, but it is straight forward to use it for larger subsets of features.</p>
<div id="permutation-based-feature-importance" class="section level2">
<h2><span class="header-section-number">12.1</span> Permutation Based Feature Importance</h2>
<p>The idea behind is easy and in some sense borrowed from Random Forest <span class="citation">(Breiman et al. <a href="#ref-R-randomForest">2018</a>)</span>. If a feature is important then after permutation model performance shall drop. The larger drop the more important is the feature.</p>
<p>Let’s describe this idea in a bit more formal way. Let <span class="math inline">\(\mathcal L(f(x), y)\)</span> be a loss function that assess goodness of fit for a model <span class="math inline">\(f(x)\)</span> while let <span class="math inline">\(\mathcal X\)</span> be a set of features.</p>
<ol style="list-style-type: decimal">
<li>For each feature <span class="math inline">\(x_i \in \mathcal X\)</span> do steps 2-5</li>
<li>Create a new data <span class="math inline">\(x^{*,-i}\)</span> with feature <span class="math inline">\(x_i\)</span> resampled (or permutated).</li>
<li>Calculate model predictions for the new data <span class="math inline">\(x^{*,-i}\)</span>, they will be denoted as <span class="math inline">\(f(x^{*,-i})\)</span>.</li>
<li>Calculate loss function for models predictions on perturbed data
<span class="math display">\[
L^{*,-i} = \mathcal L(f(x^{*,-i}), y)
\]</span></li>
<li>Feature importance may be calculated as difference or ratio of the original loss and loss on perturbed data, i.e. <span class="math inline">\(vip(x_i) = L^{*,-i} - L\)</span> or <span class="math inline">\(vip(x_i) = L^{*,-i} / L\)</span>.</li>
</ol>
<p>Note that ranking of feature importance will be the same for the difference and the ratio since the loss <span class="math inline">\(L\)</span> is the same.</p>
<p>Note also, that the main advantage of the step 5 is that feature importance is kind of normalized. But in many cases such normalization is not needed and in fact it makes more sense to present raw <span class="math inline">\(L^{*,-i}\)</span> values.</p>
</div>
<div id="example-titanic" class="section level2">
<h2><span class="header-section-number">12.2</span> Example: Titanic</h2>
<pre><code>## Distribution not specified, assuming bernoulli ...</code></pre>
<p>Let’s use this approach to a random forest model created for the Titanic dataset. The goal is to predict passenger survival probability based on their sex, age, class, fare and some other features available in the <code>titanic</code> dataset.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1"><span class="kw">head</span>(titanic_small)</a></code></pre></div>
<pre><code>##   Survived Pclass    Sex Age SibSp Parch    Fare Embarked
## 1        0      3   male  22     1     0  7.2500        S
## 2        1      1 female  38     1     0 71.2833        C
## 3        1      3 female  26     0     0  7.9250        S
## 4        1      1 female  35     1     0 53.1000        S
## 5        0      3   male  35     0     0  8.0500        S
## 7        0      1   male  54     0     0 51.8625        S</code></pre>
<p>Permutation based feature importance can be calculated with the <code>feature_importance{ingredients}</code>. By default it permutes values feature by feature.</p>
<p>Instead of showing normalized feature importance we plot both original <span class="math inline">\(L\)</span> and loss after permutation <span class="math inline">\(L^{*,-i}\)</span>. This way we can read also how good was the model, and as we will see in next subsection it will be useful for model comparison.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;ingredients&quot;</span>)</a>
<a class="sourceLine" id="cb90-2" data-line-number="2">fi_rf &lt;-<span class="st"> </span><span class="kw">feature_importance</span>(explainer_rf) </a>
<a class="sourceLine" id="cb90-3" data-line-number="3"><span class="kw">plot</span>(fi_rf) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Permutation based feature importance&quot;</span>, <span class="st">&quot;For Random Forest model and Titanic data&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:titanic3"></span>
<img src="PM_VEE_files/figure-html/titanic3-1.png" alt="Feature importance. Each interval presents the difference between original model performance (left end) and the performance on a dataset with a single feature perturbed" width="480" />
<p class="caption">
Figure 12.1: Feature importance. Each interval presents the difference between original model performance (left end) and the performance on a dataset with a single feature perturbed
</p>
</div>
<p>It’s interesting that the most important variable for Titanic data is the Sex. So it have been ,,women first’’ after all. Then the three features of similar importance are passenger class (first class has higher survival), age (kids have higher survival) and fare (owners of more pricy tickets have higher survival).</p>
<p>Note that drawing permutations evolves some randomness. Thus to have higher repeatability of results you may either set a seed for random number generator or replicate the procedure few times. The second approach has additional advantage, that you will learn the uncertainty behind feature importance assessment.</p>
<p>Here we present scores for 10 repetition of the process.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1">fi_rf10 &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">10</span>, <span class="kw">feature_importance</span>(explainer_rf), <span class="dt">simplify =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb91-2" data-line-number="2"><span class="kw">do.call</span>(plot, fi_rf10) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Permutation based feature importance&quot;</span>, <span class="st">&quot;For Random Forest model and Titanic data&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:titanic4"></span>
<img src="PM_VEE_files/figure-html/titanic4-1.png" alt="Feature importance for 10 replication of feature importance assessment" width="480" />
<p class="caption">
Figure 12.2: Feature importance for 10 replication of feature importance assessment
</p>
</div>
<p>It is much easier to assess feature importance if they come with some assessment of the uncertainty. We can read from the plot that Age and passenger class are close to each other.</p>
<p>Note that intervals are useful for model comparisons. In the Figure @ref{titanic5} we can read feature importance for random forest, gradient boosting and logistic regression models. Best results are achieved by the random forest model and also this method consume more features than others. A good example is the <em>Fare</em> variable, not used in gradient boosting not logistic regression (as a feature highly correlated with passenger class) but consumed in the random forest model.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1">fi_rf &lt;-<span class="st"> </span><span class="kw">feature_importance</span>(explainer_rf)</a>
<a class="sourceLine" id="cb92-2" data-line-number="2">fi_gbm &lt;-<span class="st"> </span><span class="kw">feature_importance</span>(explainer_gbm)</a>
<a class="sourceLine" id="cb92-3" data-line-number="3">fi_glm &lt;-<span class="st"> </span><span class="kw">feature_importance</span>(explainer_glm)</a>
<a class="sourceLine" id="cb92-4" data-line-number="4"></a>
<a class="sourceLine" id="cb92-5" data-line-number="5"><span class="kw">plot</span>(fi_rf, fi_gbm, fi_glm)</a></code></pre></div>
<div class="figure"><span id="fig:titanic5"></span>
<img src="PM_VEE_files/figure-html/titanic5-1.png" alt="Feature importance for random forest, gradient boosting and logistic regression models" width="480" />
<p class="caption">
Figure 12.3: Feature importance for random forest, gradient boosting and logistic regression models
</p>
</div>
</div>
<div id="example-price-prediction" class="section level2">
<h2><span class="header-section-number">12.3</span> Example: Price prediction</h2>
<p>Let’s create a regression model for prediction of apartment prices.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;DALEX&quot;</span>)</a>
<a class="sourceLine" id="cb93-2" data-line-number="2"><span class="kw">library</span>(<span class="st">&quot;randomForest&quot;</span>)</a>
<a class="sourceLine" id="cb93-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">59</span>)</a>
<a class="sourceLine" id="cb93-4" data-line-number="4">model_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(m2.price <span class="op">~</span><span class="st"> </span>construction.year <span class="op">+</span><span class="st"> </span>surface <span class="op">+</span><span class="st"> </span>floor <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb93-5" data-line-number="5"><span class="st">                           </span>no.rooms <span class="op">+</span><span class="st"> </span>district, <span class="dt">data =</span> apartments)</a></code></pre></div>
<p>A popular loss function for regression model is the root mean square loss
<span class="math display">\[
  L(x, y) = \sqrt{\frac1n \sum_{i=1}^n (x_i - y_i)^2}
\]</span></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1"><span class="kw">loss_root_mean_square</span>(</a>
<a class="sourceLine" id="cb94-2" data-line-number="2">  <span class="kw">predict</span>(model_rf, apartments), </a>
<a class="sourceLine" id="cb94-3" data-line-number="3">  apartments<span class="op">$</span>m2.price</a>
<a class="sourceLine" id="cb94-4" data-line-number="4">)</a></code></pre></div>
<pre><code>## [1] 193.8477</code></pre>
<p>Let’s calculate feature importance</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1">explainer_rf &lt;-<span class="st"> </span><span class="kw">explain</span>(model_rf, </a>
<a class="sourceLine" id="cb96-2" data-line-number="2">            <span class="dt">data =</span> apartmentsTest[,<span class="dv">2</span><span class="op">:</span><span class="dv">6</span>], <span class="dt">y =</span> apartmentsTest<span class="op">$</span>m2.price)</a>
<a class="sourceLine" id="cb96-3" data-line-number="3">vip &lt;-<span class="st"> </span><span class="kw">variable_importance</span>(explainer_rf, </a>
<a class="sourceLine" id="cb96-4" data-line-number="4">            <span class="dt">loss_function =</span> loss_root_mean_square)</a>
<a class="sourceLine" id="cb96-5" data-line-number="5">vip</a></code></pre></div>
<pre><code>##            variable dropout_loss        label
## 1      _full_model_     285.1355 randomForest
## 2          no.rooms     391.0710 randomForest
## 3 construction.year     410.5866 randomForest
## 4             floor     445.2164 randomForest
## 5           surface     480.1431 randomForest
## 6          district     843.6519 randomForest
## 7        _baseline_    1081.3710 randomForest</code></pre>
<p>On a diagnostic plot is useful to present feature importance as an interval that start in a loss and ends in a loss of perturbed data.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1"><span class="kw">plot</span>(vip)</a></code></pre></div>
<p><img src="PM_VEE_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
</div>
<div id="more-models" class="section level2">
<h2><span class="header-section-number">12.4</span> More models</h2>
<p>Much more can be read from feature importance plots if we compare models of a different structure.
Let’s train three predictive models trained on <code>apartments</code> dataset from the <code>DALEX</code> package. Random Forest model <span class="citation">(Breiman et al. <a href="#ref-R-randomForest">2018</a>)</span> (elastic but biased), Support Vector Machines model <span class="citation">(Meyer et al. <a href="#ref-R-e1071">2017</a>)</span> (large variance on boundaries) and Linear Model (stable but not very elastic).
Presented examples are for regression (prediction of square meter price), but the CP profiles may be used in the same way for classification.</p>
<p>Let’s fit these three models.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;DALEX&quot;</span>)</a>
<a class="sourceLine" id="cb99-2" data-line-number="2">model_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(m2.price <span class="op">~</span><span class="st"> </span>construction.year <span class="op">+</span><span class="st"> </span>surface <span class="op">+</span><span class="st"> </span>floor <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb99-3" data-line-number="3"><span class="st">                      </span>no.rooms <span class="op">+</span><span class="st"> </span>district, <span class="dt">data =</span> apartments)</a>
<a class="sourceLine" id="cb99-4" data-line-number="4"></a>
<a class="sourceLine" id="cb99-5" data-line-number="5"><span class="kw">library</span>(<span class="st">&quot;randomForest&quot;</span>)</a>
<a class="sourceLine" id="cb99-6" data-line-number="6"><span class="kw">set.seed</span>(<span class="dv">59</span>)</a>
<a class="sourceLine" id="cb99-7" data-line-number="7">model_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(m2.price <span class="op">~</span><span class="st"> </span>construction.year <span class="op">+</span><span class="st"> </span>surface <span class="op">+</span><span class="st"> </span>floor <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb99-8" data-line-number="8"><span class="st">                      </span>no.rooms <span class="op">+</span><span class="st"> </span>district, <span class="dt">data =</span> apartments)</a>
<a class="sourceLine" id="cb99-9" data-line-number="9"></a>
<a class="sourceLine" id="cb99-10" data-line-number="10"><span class="kw">library</span>(<span class="st">&quot;e1071&quot;</span>)</a>
<a class="sourceLine" id="cb99-11" data-line-number="11">model_svm &lt;-<span class="st"> </span><span class="kw">svm</span>(m2.price <span class="op">~</span><span class="st"> </span>construction.year <span class="op">+</span><span class="st"> </span>surface <span class="op">+</span><span class="st"> </span>floor <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb99-12" data-line-number="12"><span class="st">                         </span>no.rooms <span class="op">+</span><span class="st"> </span>district, <span class="dt">data =</span> apartments)</a></code></pre></div>
<p>For these models we use <code>DALEX</code> explainers created with <code>explain()</code> function. These explainers wrap models, predict functions and validation data.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1">explainer_lm &lt;-<span class="st"> </span><span class="kw">explain</span>(model_lm, </a>
<a class="sourceLine" id="cb100-2" data-line-number="2">                       <span class="dt">data =</span> apartmentsTest[,<span class="dv">2</span><span class="op">:</span><span class="dv">6</span>], <span class="dt">y =</span> apartmentsTest<span class="op">$</span>m2.price)</a>
<a class="sourceLine" id="cb100-3" data-line-number="3">vip_lm &lt;-<span class="st"> </span><span class="kw">variable_importance</span>(explainer_lm, </a>
<a class="sourceLine" id="cb100-4" data-line-number="4">            <span class="dt">loss_function =</span> loss_root_mean_square)</a>
<a class="sourceLine" id="cb100-5" data-line-number="5">vip_lm</a></code></pre></div>
<pre><code>##            variable dropout_loss label
## 1      _full_model_     282.0062    lm
## 2 construction.year     281.9007    lm
## 3          no.rooms     292.8398    lm
## 4             floor     492.0857    lm
## 5           surface     614.9198    lm
## 6          district    1002.3487    lm
## 7        _baseline_    1193.6209    lm</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1">explainer_rf &lt;-<span class="st"> </span><span class="kw">explain</span>(model_rf, </a>
<a class="sourceLine" id="cb102-2" data-line-number="2">                       <span class="dt">data =</span> apartmentsTest[,<span class="dv">2</span><span class="op">:</span><span class="dv">6</span>], <span class="dt">y =</span> apartmentsTest<span class="op">$</span>m2.price)</a>
<a class="sourceLine" id="cb102-3" data-line-number="3">vip_rf &lt;-<span class="st"> </span><span class="kw">variable_importance</span>(explainer_rf, </a>
<a class="sourceLine" id="cb102-4" data-line-number="4">            <span class="dt">loss_function =</span> loss_root_mean_square)</a>
<a class="sourceLine" id="cb102-5" data-line-number="5">vip_rf</a></code></pre></div>
<pre><code>##            variable dropout_loss        label
## 1      _full_model_     293.2729 randomForest
## 2          no.rooms     389.4526 randomForest
## 3 construction.year     416.1154 randomForest
## 4             floor     453.9195 randomForest
## 5           surface     480.4062 randomForest
## 6          district     867.7050 randomForest
## 7        _baseline_    1116.2616 randomForest</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" data-line-number="1">explainer_svm &lt;-<span class="st"> </span><span class="kw">explain</span>(model_svm, </a>
<a class="sourceLine" id="cb104-2" data-line-number="2">                       <span class="dt">data =</span> apartmentsTest[,<span class="dv">2</span><span class="op">:</span><span class="dv">6</span>], <span class="dt">y =</span> apartmentsTest<span class="op">$</span>m2.price)</a>
<a class="sourceLine" id="cb104-3" data-line-number="3">vip_svm &lt;-<span class="st"> </span><span class="kw">variable_importance</span>(explainer_svm, </a>
<a class="sourceLine" id="cb104-4" data-line-number="4">            <span class="dt">loss_function =</span> loss_root_mean_square)</a>
<a class="sourceLine" id="cb104-5" data-line-number="5">vip_svm</a></code></pre></div>
<pre><code>##            variable dropout_loss label
## 1      _full_model_     157.7938   svm
## 2          no.rooms     221.4595   svm
## 3 construction.year     365.2600   svm
## 4             floor     439.8724   svm
## 5           surface     527.2598   svm
## 6          district     942.8512   svm
## 7        _baseline_    1203.7571   svm</code></pre>
<p>Let’s plot feature importance for all three models on a single plot.</p>
<p>Intervals start in a different values, thus we can read that loss for SVM model is the lowest.</p>
<p>When we compare other features it looks like in all models the <code>district</code> is the most important feature followed by <code>surface</code> and <code>floor</code>.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" data-line-number="1"><span class="kw">plot</span>(vip_rf, vip_svm, vip_lm)</a></code></pre></div>
<p><img src="PM_VEE_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>There is interesting difference between linear model and others in the way how important is the <code>construction.year</code>. For linear model this variable is not importance, while for remaining two models there is some importance.</p>
<p>In the next chapter we will see how this is possible.</p>
</div>
<div id="level-frequency" class="section level2">
<h2><span class="header-section-number">12.5</span> Level frequency</h2>
<p>What does the feature importance mean? How it is linked with a data distribution.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-xgboostExplainer">
<p>Foster, David. 2017. <em>XgboostExplainer: An R Package That Makes Xgboost Models Fully Interpretable</em>. <a href="https://github.com/AppliedDataSciencePartners/xgboostExplainer/">https://github.com/AppliedDataSciencePartners/xgboostExplainer/</a>.</p>
</div>
<div id="ref-randomForestExplainer">
<p>Paluszynska, Aleksandra, and Przemyslaw Biecek. 2017a. <em>RandomForestExplainer: A Set of Tools to Understand What Is Happening Inside a Random Forest</em>. <a href="https://github.com/MI2DataLab/randomForestExplainer">https://github.com/MI2DataLab/randomForestExplainer</a>.</p>
</div>
<div id="ref-variableImportancePermutations">
<p>Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2018. “Model Class Reliance: Variable Importance Measures for Any Machine Learning Model Class, from the ’Rashomon’ Perspective.” <em>Journal of Computational and Graphical Statistics</em>. <a href="http://arxiv.org/abs/1801.01489">http://arxiv.org/abs/1801.01489</a>.</p>
</div>
<div id="ref-R-randomForest">
<p>Breiman, Leo, Adele Cutler, Andy Liaw, and Matthew Wiener. 2018. <em>RandomForest: Breiman and Cutler’s Random Forests for Classification and Regression</em>. <a href="https://CRAN.R-project.org/package=randomForest">https://CRAN.R-project.org/package=randomForest</a>.</p>
</div>
<div id="ref-R-e1071">
<p>Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2017. <em>E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), Tu Wien</em>. <a href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="variableEngeneering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["PM_VEE.pdf", "PM_VEE.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
