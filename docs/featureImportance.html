<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>16 Variable-importance Measures | Explanatory Model Analysis</title>
  <meta name="description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="16 Variable-importance Measures | Explanatory Model Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="figure/front4.png" />
  <meta property="og:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="github-repo" content="pbiecek/ema" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="16 Variable-importance Measures | Explanatory Model Analysis" />
  
  <meta name="twitter:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="twitter:image" content="figure/front4.png" />

<meta name="author" content="Przemyslaw Biecek and Tomasz Burzykowski" />


<meta name="date" content="2020-12-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modelPerformance.html"/>
<link rel="next" href="partialDependenceProfiles.html"/>
<script src="libs/header-attrs-2.4/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-5650686-15', 'https://pbiecek.github.io/ema/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>
<style>
.figure {
   padding:40px 0px;
}
</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><span style="font-size: large">Explanatory Model Analysis</span><br/>Explore, Explain, and Examine<br/>Predictive Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#the-aim-of-the-book"><i class="fa fa-check"></i><b>1.1</b> The aim of the book</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#three-single-laws"><i class="fa fa-check"></i><b>1.2</b> A bit of philosophy: three laws of model explanation</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#teminology"><i class="fa fa-check"></i><b>1.3</b> Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#glassblack"><i class="fa fa-check"></i><b>1.4</b> Black-box models and glass-box models</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#agnosticspecific"><i class="fa fa-check"></i><b>1.5</b> Model-agnostic and model-specific approach</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#bookstructure"><i class="fa fa-check"></i><b>1.6</b> The structure of the book</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#whatisinthebook"><i class="fa fa-check"></i><b>1.7</b> What is included in this book and what is not</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#thanksto"><i class="fa fa-check"></i><b>1.8</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html"><i class="fa fa-check"></i><b>2</b> Model Development</a>
<ul>
<li class="chapter" data-level="2.1" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPIntro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPprocess"><i class="fa fa-check"></i><b>2.2</b> Model-development process</a></li>
<li class="chapter" data-level="2.3" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#notation"><i class="fa fa-check"></i><b>2.3</b> Notation</a></li>
<li class="chapter" data-level="2.4" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#dataunderstanding"><i class="fa fa-check"></i><b>2.4</b> Data understanding</a></li>
<li class="chapter" data-level="2.5" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#fitting"><i class="fa fa-check"></i><b>2.5</b> Model assembly (fitting)</a></li>
<li class="chapter" data-level="2.6" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#validation"><i class="fa fa-check"></i><b>2.6</b> Model audit</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="do-it-yourself.html"><a href="do-it-yourself.html"><i class="fa fa-check"></i><b>3</b> Do-it-yourself</a>
<ul>
<li class="chapter" data-level="3.1" data-path="do-it-yourself.html"><a href="do-it-yourself.html#doItYourselfWithR"><i class="fa fa-check"></i><b>3.1</b> Do-it-yourself with R</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="do-it-yourself.html"><a href="do-it-yourself.html#what-to-install"><i class="fa fa-check"></i><b>3.1.1</b> What to install?</a></li>
<li class="chapter" data-level="3.1.2" data-path="do-it-yourself.html"><a href="do-it-yourself.html#infoDALEX"><i class="fa fa-check"></i><b>3.1.2</b> How to work with <code>DALEX</code>?</a></li>
<li class="chapter" data-level="3.1.3" data-path="do-it-yourself.html"><a href="do-it-yourself.html#how-to-work-with-archivist"><i class="fa fa-check"></i><b>3.1.3</b> How to work with <code>archivist</code>?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="do-it-yourself.html"><a href="do-it-yourself.html#doItYourselfWithPython"><i class="fa fa-check"></i><b>3.2</b> Do-it-yourself with Python</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="do-it-yourself.html"><a href="do-it-yourself.html#what-to-install-1"><i class="fa fa-check"></i><b>3.2.1</b> What to install?</a></li>
<li class="chapter" data-level="3.2.2" data-path="do-it-yourself.html"><a href="do-it-yourself.html#infoDALEXpy"><i class="fa fa-check"></i><b>3.2.2</b> How to work with <code>dalex</code>?</a></li>
<li class="chapter" data-level="3.2.3" data-path="do-it-yourself.html"><a href="do-it-yourself.html#code-snippets-for-python"><i class="fa fa-check"></i><b>3.2.3</b> Code snippets for Python</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html"><i class="fa fa-check"></i><b>4</b> Datasets and Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#TitanicDataset"><i class="fa fa-check"></i><b>4.1</b> Sinking of the RMS Titanic</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-titanic"><i class="fa fa-check"></i><b>4.1.1</b> Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#models-for-rms-titanic-snippets-for-r"><i class="fa fa-check"></i><b>4.2</b> Models for RMS Titanic, snippets for R</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-lmr"><i class="fa fa-check"></i><b>4.2.1</b> Logistic regression model</a></li>
<li class="chapter" data-level="4.2.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-rf"><i class="fa fa-check"></i><b>4.2.2</b> Random forest model</a></li>
<li class="chapter" data-level="4.2.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-gbm"><i class="fa fa-check"></i><b>4.2.3</b> Gradient boosting model</a></li>
<li class="chapter" data-level="4.2.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-svm"><i class="fa fa-check"></i><b>4.2.4</b> Support vector machine model</a></li>
<li class="chapter" data-level="4.2.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic"><i class="fa fa-check"></i><b>4.2.5</b> Models’ predictions</a></li>
<li class="chapter" data-level="4.2.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicRCode"><i class="fa fa-check"></i><b>4.2.6</b> Models’ explainers</a></li>
<li class="chapter" data-level="4.2.7" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsTitanic"><i class="fa fa-check"></i><b>4.2.7</b> List of model-objects</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#models-for-rms-titanic-snippets-for-python"><i class="fa fa-check"></i><b>4.3</b> Models for RMS Titanic, snippets for Python</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-lr"><i class="fa fa-check"></i><b>4.3.1</b> Logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-rf"><i class="fa fa-check"></i><b>4.3.2</b> Random forest model</a></li>
<li class="chapter" data-level="4.3.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-gbm"><i class="fa fa-check"></i><b>4.3.3</b> Gradient boosting model</a></li>
<li class="chapter" data-level="4.3.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-svm"><i class="fa fa-check"></i><b>4.3.4</b> Support vector machine model</a></li>
<li class="chapter" data-level="4.3.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic-python"><i class="fa fa-check"></i><b>4.3.5</b> Models’ predictions</a></li>
<li class="chapter" data-level="4.3.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicPythonCode"><i class="fa fa-check"></i><b>4.3.6</b> Models’ explainers</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ApartmentDataset"><i class="fa fa-check"></i><b>4.4</b> Apartment prices</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-apartments"><i class="fa fa-check"></i><b>4.4.1</b> Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#models-for-apartment-prices-snippets-for-r"><i class="fa fa-check"></i><b>4.5</b> Models for apartment prices, snippets for R</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-lr"><i class="fa fa-check"></i><b>4.5.1</b> Linear regression model</a></li>
<li class="chapter" data-level="4.5.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-rf"><i class="fa fa-check"></i><b>4.5.2</b> Random forest model</a></li>
<li class="chapter" data-level="4.5.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-svm"><i class="fa fa-check"></i><b>4.5.3</b> Support vector machine model</a></li>
<li class="chapter" data-level="4.5.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictionsApartments"><i class="fa fa-check"></i><b>4.5.4</b> Models’ predictions</a></li>
<li class="chapter" data-level="4.5.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsRCode"><i class="fa fa-check"></i><b>4.5.5</b> Models’ explainers</a></li>
<li class="chapter" data-level="4.5.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsApartments"><i class="fa fa-check"></i><b>4.5.6</b> List of model-objects</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#models-for-apartment-prices-snippets-for-python"><i class="fa fa-check"></i><b>4.6</b> Models for apartment prices, snippets for Python</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-lr"><i class="fa fa-check"></i><b>4.6.1</b> Linear regression model</a></li>
<li class="chapter" data-level="4.6.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-rf"><i class="fa fa-check"></i><b>4.6.2</b> Random forest model</a></li>
<li class="chapter" data-level="4.6.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-svm"><i class="fa fa-check"></i><b>4.6.3</b> Support vector machine model</a></li>
<li class="chapter" data-level="4.6.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-apartments-python"><i class="fa fa-check"></i><b>4.6.4</b> Models’ predictions</a></li>
<li class="chapter" data-level="4.6.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsPythonCode"><i class="fa fa-check"></i><b>4.6.5</b> Models’ explainers</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Instance Level</b></span></li>
<li class="chapter" data-level="5" data-path="InstanceLevelExploration.html"><a href="InstanceLevelExploration.html"><i class="fa fa-check"></i><b>5</b> Introduction to Instance-level Exploration</a></li>
<li class="chapter" data-level="6" data-path="breakDown.html"><a href="breakDown.html"><i class="fa fa-check"></i><b>6</b> Break-down Plots for Additive Attributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="breakDown.html"><a href="breakDown.html#BDIntroduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="breakDown.html"><a href="breakDown.html#BDIntuition"><i class="fa fa-check"></i><b>6.2</b> Intuition</a></li>
<li class="chapter" data-level="6.3" data-path="breakDown.html"><a href="breakDown.html#BDMethod"><i class="fa fa-check"></i><b>6.3</b> Method</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="breakDown.html"><a href="breakDown.html#BDMethodLin"><i class="fa fa-check"></i><b>6.3.1</b> Break-down for linear models</a></li>
<li class="chapter" data-level="6.3.2" data-path="breakDown.html"><a href="breakDown.html#BDMethodGen"><i class="fa fa-check"></i><b>6.3.2</b> Break-down for a general case</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="breakDown.html"><a href="breakDown.html#BDExample"><i class="fa fa-check"></i><b>6.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="6.5" data-path="breakDown.html"><a href="breakDown.html#BDProsCons"><i class="fa fa-check"></i><b>6.5</b> Pros and cons</a></li>
<li class="chapter" data-level="6.6" data-path="breakDown.html"><a href="breakDown.html#BDR"><i class="fa fa-check"></i><b>6.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="breakDown.html"><a href="breakDown.html#basic-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>6.6.1</b> Basic use of the <code>predict_parts()</code> function</a></li>
<li class="chapter" data-level="6.6.2" data-path="breakDown.html"><a href="breakDown.html#advanced-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>6.6.2</b> Advanced use of the <code>predict_parts()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="breakDown.html"><a href="breakDown.html#BDPython"><i class="fa fa-check"></i><b>6.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="iBreakDown.html"><a href="iBreakDown.html"><i class="fa fa-check"></i><b>7</b> Break-down Plots for Interactions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDIntuition"><i class="fa fa-check"></i><b>7.1</b> Intuition</a></li>
<li class="chapter" data-level="7.2" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDMethod"><i class="fa fa-check"></i><b>7.2</b> Method</a></li>
<li class="chapter" data-level="7.3" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDExample"><i class="fa fa-check"></i><b>7.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="7.4" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDProsCons"><i class="fa fa-check"></i><b>7.4</b> Pros and cons</a></li>
<li class="chapter" data-level="7.5" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDRcode"><i class="fa fa-check"></i><b>7.5</b> Code snippets for R</a></li>
<li class="chapter" data-level="7.6" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDPythonCode"><i class="fa fa-check"></i><b>7.6</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>8</b> Shapley Additive Explanations (SHAP) for Average Attributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="shapley.html"><a href="shapley.html#SHAPIntuition"><i class="fa fa-check"></i><b>8.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2" data-path="shapley.html"><a href="shapley.html#SHAPMethod"><i class="fa fa-check"></i><b>8.2</b> Method</a></li>
<li class="chapter" data-level="8.3" data-path="shapley.html"><a href="shapley.html#SHAPExample"><i class="fa fa-check"></i><b>8.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="8.4" data-path="shapley.html"><a href="shapley.html#SHAProsCons"><i class="fa fa-check"></i><b>8.4</b> Pros and cons</a></li>
<li class="chapter" data-level="8.5" data-path="shapley.html"><a href="shapley.html#SHAPRcode"><i class="fa fa-check"></i><b>8.5</b> Code snippets for R</a></li>
<li class="chapter" data-level="8.6" data-path="shapley.html"><a href="shapley.html#SHAPPythonCode"><i class="fa fa-check"></i><b>8.6</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="LIME.html"><a href="LIME.html"><i class="fa fa-check"></i><b>9</b> Local Interpretable Model-agnostic Explanations (LIME)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="LIME.html"><a href="LIME.html#LIMEIntroduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="LIME.html"><a href="LIME.html#LIMEIntuition"><i class="fa fa-check"></i><b>9.2</b> Intuition</a></li>
<li class="chapter" data-level="9.3" data-path="LIME.html"><a href="LIME.html#LIMEMethod"><i class="fa fa-check"></i><b>9.3</b> Method</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="LIME.html"><a href="LIME.html#LIMErepr"><i class="fa fa-check"></i><b>9.3.1</b> Interpretable data representation</a></li>
<li class="chapter" data-level="9.3.2" data-path="LIME.html"><a href="LIME.html#LIMEsample"><i class="fa fa-check"></i><b>9.3.2</b> Sampling around the instance of interest</a></li>
<li class="chapter" data-level="9.3.3" data-path="LIME.html"><a href="LIME.html#LIMEglas"><i class="fa fa-check"></i><b>9.3.3</b> Fitting the glass-box model</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="LIME.html"><a href="LIME.html#LIMEExample"><i class="fa fa-check"></i><b>9.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="9.5" data-path="LIME.html"><a href="LIME.html#LIMEProsCons"><i class="fa fa-check"></i><b>9.5</b> Pros and cons</a></li>
<li class="chapter" data-level="9.6" data-path="LIME.html"><a href="LIME.html#LIMERcode"><i class="fa fa-check"></i><b>9.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="LIME.html"><a href="LIME.html#LIMERcodelime"><i class="fa fa-check"></i><b>9.6.1</b> The <code>lime</code> package</a></li>
<li class="chapter" data-level="9.6.2" data-path="LIME.html"><a href="LIME.html#LIMERcodelocMod"><i class="fa fa-check"></i><b>9.6.2</b> The <code>localModel</code> package</a></li>
<li class="chapter" data-level="9.6.3" data-path="LIME.html"><a href="LIME.html#LIMERcodeiml"><i class="fa fa-check"></i><b>9.6.3</b> The <code>iml</code> package</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="LIME.html"><a href="LIME.html#LIMEPythoncode"><i class="fa fa-check"></i><b>9.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ceterisParibus.html"><a href="ceterisParibus.html"><i class="fa fa-check"></i><b>10</b> Ceteris-paribus Profiles</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntro"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntuition"><i class="fa fa-check"></i><b>10.2</b> Intuition</a></li>
<li class="chapter" data-level="10.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPMethod"><i class="fa fa-check"></i><b>10.3</b> Method</a></li>
<li class="chapter" data-level="10.4" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPExample"><i class="fa fa-check"></i><b>10.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="10.5" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPProsCons"><i class="fa fa-check"></i><b>10.5</b> Pros and cons</a></li>
<li class="chapter" data-level="10.6" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPR"><i class="fa fa-check"></i><b>10.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#basic-use-of-the-predict_profile-function"><i class="fa fa-check"></i><b>10.6.1</b> Basic use of the <code>predict_profile()</code> function</a></li>
<li class="chapter" data-level="10.6.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#advanced-use-of-the-predict_profile-function"><i class="fa fa-check"></i><b>10.6.2</b> Advanced use of the <code>predict_profile()</code> function</a></li>
<li class="chapter" data-level="10.6.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#comparison-of-models-champion-challenger"><i class="fa fa-check"></i><b>10.6.3</b> Comparison of models (champion-challenger)</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPPython"><i class="fa fa-check"></i><b>10.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html"><i class="fa fa-check"></i><b>11</b> Ceteris-paribus Oscillations</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntro"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntuition"><i class="fa fa-check"></i><b>11.2</b> Intuition</a></li>
<li class="chapter" data-level="11.3" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscMethod"><i class="fa fa-check"></i><b>11.3</b> Method</a></li>
<li class="chapter" data-level="11.4" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscExample"><i class="fa fa-check"></i><b>11.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="11.5" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscProsCons"><i class="fa fa-check"></i><b>11.5</b> Pros and cons</a></li>
<li class="chapter" data-level="11.6" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscR"><i class="fa fa-check"></i><b>11.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#basic-use-of-the-predict_parts-function-1"><i class="fa fa-check"></i><b>11.6.1</b> Basic use of the <code>predict_parts()</code> function</a></li>
<li class="chapter" data-level="11.6.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#advanced-use-of-the-predict_parts-function-1"><i class="fa fa-check"></i><b>11.6.2</b> Advanced use of the <code>predict_parts()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscPython"><i class="fa fa-check"></i><b>11.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="localDiagnostics.html"><a href="localDiagnostics.html"><i class="fa fa-check"></i><b>12</b> Local-diagnostics Plots</a>
<ul>
<li class="chapter" data-level="12.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntuition"><i class="fa fa-check"></i><b>12.2</b> Intuition</a></li>
<li class="chapter" data-level="12.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagMethod"><i class="fa fa-check"></i><b>12.3</b> Method</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagneighbours"><i class="fa fa-check"></i><b>12.3.1</b> Nearest neighbours</a></li>
<li class="chapter" data-level="12.3.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagLFplot"><i class="fa fa-check"></i><b>12.3.2</b> Local-fidelity plot</a></li>
<li class="chapter" data-level="12.3.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProfiles"><i class="fa fa-check"></i><b>12.3.3</b> Local-stability plot</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagExample"><i class="fa fa-check"></i><b>12.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProsCons"><i class="fa fa-check"></i><b>12.5</b> Pros and cons</a></li>
<li class="chapter" data-level="12.6" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagR"><i class="fa fa-check"></i><b>12.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="12.7" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagPython"><i class="fa fa-check"></i><b>12.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html"><i class="fa fa-check"></i><b>13</b> Summary of Instance-level Exploration</a>
<ul>
<li class="chapter" data-level="13.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#summaryInstanceLevelIntro"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#number-of-explanatory-variables-in-the-model"><i class="fa fa-check"></i><b>13.2</b> Number of explanatory variables in the model</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#low-to-medium-number-of-explanatory-variables"><i class="fa fa-check"></i><b>13.2.1</b> Low to medium number of explanatory variables</a></li>
<li class="chapter" data-level="13.2.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#medium-to-a-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>13.2.2</b> Medium to a large number of explanatory variables</a></li>
<li class="chapter" data-level="13.2.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#very-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>13.2.3</b> Very large number of explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#correlated-explanatory-variables"><i class="fa fa-check"></i><b>13.3</b> Correlated explanatory variables</a></li>
<li class="chapter" data-level="13.4" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#models-with-interactions"><i class="fa fa-check"></i><b>13.4</b> Models with interactions</a></li>
<li class="chapter" data-level="13.5" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#sparse-explanations"><i class="fa fa-check"></i><b>13.5</b> Sparse explanations</a></li>
<li class="chapter" data-level="13.6" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#additional-uses-of-model-exploration-and-explanation"><i class="fa fa-check"></i><b>13.6</b> Additional uses of model exploration and explanation</a></li>
<li class="chapter" data-level="13.7" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#comparison-of-models-champion-challenger-analysis"><i class="fa fa-check"></i><b>13.7</b> Comparison of models (champion-challenger analysis)</a></li>
</ul></li>
<li class="part"><span><b>III Dataset Level</b></span></li>
<li class="chapter" data-level="14" data-path="modelLevelExploration.html"><a href="modelLevelExploration.html"><i class="fa fa-check"></i><b>14</b> Introduction to Dataset-level Exploration</a></li>
<li class="chapter" data-level="15" data-path="modelPerformance.html"><a href="modelPerformance.html"><i class="fa fa-check"></i><b>15</b> Model-performance Measures</a>
<ul>
<li class="chapter" data-level="15.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntro"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntuition"><i class="fa fa-check"></i><b>15.2</b> Intuition</a></li>
<li class="chapter" data-level="15.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethod"><i class="fa fa-check"></i><b>15.3</b> Method</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCont"><i class="fa fa-check"></i><b>15.3.1</b> Continuous dependent variable</a></li>
<li class="chapter" data-level="15.3.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodBin"><i class="fa fa-check"></i><b>15.3.2</b> Binary dependent variable</a></li>
<li class="chapter" data-level="15.3.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCateg"><i class="fa fa-check"></i><b>15.3.3</b> Categorical dependent variable</a></li>
<li class="chapter" data-level="15.3.4" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCount"><i class="fa fa-check"></i><b>15.3.4</b> Count dependent variable</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="modelPerformance.html"><a href="modelPerformance.html#example"><i class="fa fa-check"></i><b>15.4</b> Example</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceApartments"><i class="fa fa-check"></i><b>15.4.1</b> Apartment prices</a></li>
<li class="chapter" data-level="15.4.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceTitanic"><i class="fa fa-check"></i><b>15.4.2</b> Titanic data</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceProsCons"><i class="fa fa-check"></i><b>15.5</b> Pros and cons</a></li>
<li class="chapter" data-level="15.6" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceR"><i class="fa fa-check"></i><b>15.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="15.7" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformancePython"><i class="fa fa-check"></i><b>15.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="featureImportance.html"><a href="featureImportance.html"><i class="fa fa-check"></i><b>16</b> Variable-importance Measures</a>
<ul>
<li class="chapter" data-level="16.1" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntro"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntuition"><i class="fa fa-check"></i><b>16.2</b> Intuition</a></li>
<li class="chapter" data-level="16.3" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceMethod"><i class="fa fa-check"></i><b>16.3</b> Method</a></li>
<li class="chapter" data-level="16.4" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceTitanic"><i class="fa fa-check"></i><b>16.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="16.5" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceProsCons"><i class="fa fa-check"></i><b>16.5</b> Pros and cons</a></li>
<li class="chapter" data-level="16.6" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceR"><i class="fa fa-check"></i><b>16.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="16.7" data-path="featureImportance.html"><a href="featureImportance.html#featureImportancePython"><i class="fa fa-check"></i><b>16.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html"><i class="fa fa-check"></i><b>17</b> Partial-dependence Profiles</a>
<ul>
<li class="chapter" data-level="17.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntro"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntuition"><i class="fa fa-check"></i><b>17.2</b> Intuition</a></li>
<li class="chapter" data-level="17.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPMethod"><i class="fa fa-check"></i><b>17.3</b> Method</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPs"><i class="fa fa-check"></i><b>17.3.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="17.3.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clusteredPDPs"><i class="fa fa-check"></i><b>17.3.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="17.3.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#groupedPDPs"><i class="fa fa-check"></i><b>17.3.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="17.3.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastivePDPs"><i class="fa fa-check"></i><b>17.3.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPExample"><i class="fa fa-check"></i><b>17.4</b> Example: apartment-prices data</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependence-profiles"><i class="fa fa-check"></i><b>17.4.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="17.4.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles"><i class="fa fa-check"></i><b>17.4.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="17.4.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles"><i class="fa fa-check"></i><b>17.4.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="17.4.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles"><i class="fa fa-check"></i><b>17.4.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPProsCons"><i class="fa fa-check"></i><b>17.5</b> Pros and cons</a></li>
<li class="chapter" data-level="17.6" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPR"><i class="fa fa-check"></i><b>17.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependence-profiles-1"><i class="fa fa-check"></i><b>17.6.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="17.6.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>17.6.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="17.6.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>17.6.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="17.6.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>17.6.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPPython"><i class="fa fa-check"></i><b>17.7</b> Code snippets for Python</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles-2"><i class="fa fa-check"></i><b>17.7.1</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="17.7.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles-2"><i class="fa fa-check"></i><b>17.7.2</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html"><i class="fa fa-check"></i><b>18</b> Local-dependence and Accumulated-local Profiles</a>
<ul>
<li class="chapter" data-level="18.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntro"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntuition"><i class="fa fa-check"></i><b>18.2</b> Intuition</a></li>
<li class="chapter" data-level="18.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPMethod"><i class="fa fa-check"></i><b>18.3</b> Method</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#local-dependence-profile"><i class="fa fa-check"></i><b>18.3.1</b> Local-dependence profile</a></li>
<li class="chapter" data-level="18.3.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#accumulated-local-profile"><i class="fa fa-check"></i><b>18.3.2</b> Accumulated-local profile</a></li>
<li class="chapter" data-level="18.3.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#dependence-profiles-for-a-model-with-interaction-and-correlated-explanatory-variables-an-example"><i class="fa fa-check"></i><b>18.3.3</b> Dependence profiles for a model with interaction and correlated explanatory variables: an example</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#CDPExample"><i class="fa fa-check"></i><b>18.4</b> Example: apartment-prices data</a></li>
<li class="chapter" data-level="18.5" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPProsCons"><i class="fa fa-check"></i><b>18.5</b> Pros and cons</a></li>
<li class="chapter" data-level="18.6" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPR"><i class="fa fa-check"></i><b>18.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="18.7" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPPython"><i class="fa fa-check"></i><b>18.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html"><i class="fa fa-check"></i><b>19</b> Residual-diagnostics Plots</a>
<ul>
<li class="chapter" data-level="19.1" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntroResidualDiagnostic"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntuitionResidualDiagnostic"><i class="fa fa-check"></i><b>19.2</b> Intuition</a></li>
<li class="chapter" data-level="19.3" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#MethodResidualDiagnostic"><i class="fa fa-check"></i><b>19.3</b> Method</a></li>
<li class="chapter" data-level="19.4" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ExampleResidualDiagnostic"><i class="fa fa-check"></i><b>19.4</b> Example: apartment-prices data</a></li>
<li class="chapter" data-level="19.5" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ProsConsResidualDiagnostic"><i class="fa fa-check"></i><b>19.5</b> Pros and cons</a></li>
<li class="chapter" data-level="19.6" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#RcodeResidualDiagnostic"><i class="fa fa-check"></i><b>19.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="19.7" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#PythoncodeResidualDiagnostic"><i class="fa fa-check"></i><b>19.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html"><i class="fa fa-check"></i><b>20</b> Summary of Dataset-level Exploration</a>
<ul>
<li class="chapter" data-level="20.1" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html#summaryModelLevelIntro"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
<li class="chapter" data-level="20.2" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html#exploration-on-trainingtesting-data"><i class="fa fa-check"></i><b>20.2</b> Exploration on training/testing data</a></li>
<li class="chapter" data-level="20.3" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html#correlated-explanatory-variables-1"><i class="fa fa-check"></i><b>20.3</b> Correlated explanatory variables</a></li>
<li class="chapter" data-level="20.4" data-path="summaryModelLevel.html"><a href="summaryModelLevel.html#comparison-of-models-champion-challenger-analysis-1"><i class="fa fa-check"></i><b>20.4</b> Comparison of models (champion-challenger analysis)</a></li>
</ul></li>
<li class="part"><span><b>IV Use-cases</b></span></li>
<li class="chapter" data-level="21" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html"><i class="fa fa-check"></i><b>21</b> FIFA 19</a>
<ul>
<li class="chapter" data-level="21.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAintro"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAdataprep"><i class="fa fa-check"></i><b>21.2</b> Data preparation</a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r"><i class="fa fa-check"></i><b>21.2.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.2.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-1"><i class="fa fa-check"></i><b>21.2.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAdataunderst"><i class="fa fa-check"></i><b>21.3</b> Data understanding</a></li>
<li class="chapter" data-level="21.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelassembly"><i class="fa fa-check"></i><b>21.4</b> Model assembly</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-1"><i class="fa fa-check"></i><b>21.4.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.4.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-2"><i class="fa fa-check"></i><b>21.4.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelaudit"><i class="fa fa-check"></i><b>21.5</b> Model audit</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-2"><i class="fa fa-check"></i><b>21.5.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.5.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-3"><i class="fa fa-check"></i><b>21.5.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelunderst"><i class="fa fa-check"></i><b>21.6</b> Model understanding (dataset-level explanations)</a>
<ul>
<li class="chapter" data-level="21.6.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-3"><i class="fa fa-check"></i><b>21.6.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.6.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-4"><i class="fa fa-check"></i><b>21.6.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.7" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAinstanceunderst"><i class="fa fa-check"></i><b>21.7</b> Instance-level explanations</a>
<ul>
<li class="chapter" data-level="21.7.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFALewy"><i class="fa fa-check"></i><b>21.7.1</b> Robert Lewandowski</a></li>
<li class="chapter" data-level="21.7.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-4"><i class="fa fa-check"></i><b>21.7.2</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.7.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-5"><i class="fa fa-check"></i><b>21.7.3</b> Code snippets for Python</a></li>
<li class="chapter" data-level="21.7.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFACR7"><i class="fa fa-check"></i><b>21.7.4</b> CR7</a></li>
<li class="chapter" data-level="21.7.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFASzczesny"><i class="fa fa-check"></i><b>21.7.5</b> Wojciech Szczęsny</a></li>
<li class="chapter" data-level="21.7.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAMessi"><i class="fa fa-check"></i><b>21.7.6</b> Lionel Messi</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="reproducibility.html"><a href="reproducibility.html"><i class="fa fa-check"></i><b>22</b> Reproducibility</a>
<ul>
<li class="chapter" data-level="22.1" data-path="reproducibility.html"><a href="reproducibility.html#package-versions-for-r"><i class="fa fa-check"></i><b>22.1</b> Package versions for R</a></li>
<li class="chapter" data-level="22.2" data-path="reproducibility.html"><a href="reproducibility.html#package-versions-for-python"><i class="fa fa-check"></i><b>22.2</b> Package versions for Python</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/ModelOriented/DALEX" target="blank">DALEX website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Explanatory Model Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="featureImportance" class="section level1" number="16">
<h1><span class="header-section-number">16</span> Variable-importance Measures</h1>
<div id="featureImportanceIntro" class="section level2" number="16.1">
<h2><span class="header-section-number">16.1</span> Introduction</h2>
<p>In this chapter, we present a method that is useful for the evaluation of the importance of an explanatory variable. The method may be applied for several purposes.</p>
<ul>
<li>Model simplification: variables that do not influence a model’s predictions may be excluded from the model. </li>
<li>Model exploration: comparison of variables’ importance in different models may help in discovering interrelations between the variables. Also, the ordering of variables in the function of their importance is helpful in deciding in which order should we perform further model exploration.</li>
<li>Domain-knowledge-based model validation: identification of the most important variables may be helpful in assessing the validity of the model based on domain knowledge.</li>
<li>Knowledge generation: identification of the most important variables may lead to the discovery of new factors involved in a particular mechanism.</li>
</ul>
<p>The methods for assessment of variable importance can be divided, in general, into two groups: model-specific and model-agnostic.</p>
<p>For linear models and many other types of models, there are methods of assessing explanatory variable’s importance that exploit particular elements of the structure of the model. These are model-specific methods. For instance, for linear models, one can use the value of the normalized regression coefficient or its corresponding p-value as the variable-importance measure. For tree-based ensembles, such a measure may be based on the use of a particular variable in particular trees. A great example in this respect is the variable-importance measure based on out-of-bag data for a random forest model <span class="citation">(Leo Breiman <a href="#ref-randomForestBreiman" role="doc-biblioref">2001</a><a href="#ref-randomForestBreiman" role="doc-biblioref">a</a>)</span>, but there are also other approaches like methods implemented in the <code>XgboostExplainer</code> package <span class="citation">(Foster <a href="#ref-xgboostExplainer" role="doc-biblioref">2017</a>)</span> for gradient boosting and <code>randomForestExplainer</code> <span class="citation">(Paluszynska and Biecek <a href="#ref-randomForestExplainer" role="doc-biblioref">2017</a>)</span> for random forest.</p>
<p>In this book, we focus on a model-agnostic method that does not assume anything about the model structure. Therefore, it can be applied to any predictive model or ensemble of models. Moreover, and perhaps even more importantly, it allows comparing an explanatory-variable’s importance between models with different structures.</p>
</div>
<div id="featureImportanceIntuition" class="section level2" number="16.2">
<h2><span class="header-section-number">16.2</span> Intuition</h2>
<p>We focus on the method described in more detail by <span class="citation">Fisher, Rudin, and Dominici (<a href="#ref-variableImportancePermutations" role="doc-biblioref">2019</a>)</span>. The main idea is to measure how much does a model’s performance change if the effect of a selected explanatory variable, or of a group of variables, is removed? To remove the effect, we use perturbations, like resampling from an empirical distribution or permutation of the values of the variable.</p>
<p>The idea is borrowed from the variable-importance measure proposed by <span class="citation">Leo Breiman (<a href="#ref-randomForestBreiman" role="doc-biblioref">2001</a><a href="#ref-randomForestBreiman" role="doc-biblioref">a</a>)</span> for random forest. If a variable is important, then we expect that, after permuting the values of the variable, the model’s performance (as captured by one of the measures discussed in Chapter <a href="modelPerformance.html#modelPerformance">15</a>) will worsen. The larger the change in the performance, the more important is the variable.</p>
<p>Despite the simplicity of the idea, the permutation-based approach to measuring an explanatory-variable’s importance is a very powerful model-agnostic tool for model exploration. Variable-importance measures obtained in this way may be compared between different models. This property is discussed in detail in Section <a href="featureImportance.html#featureImportanceProsCons">16.5</a>.</p>
</div>
<div id="featureImportanceMethod" class="section level2" number="16.3">
<h2><span class="header-section-number">16.3</span> Method</h2>
<p>Consider a set of <span class="math inline">\(n\)</span> observations for a set of <span class="math inline">\(p\)</span> explanatory variables and dependent variable <span class="math inline">\(Y\)</span>. Let <span class="math inline">\(\underline{X}\)</span> denote the matrix containing, in rows, the (transposed column-vectors of) observed values of the explanatory variables for all observations. Denote by <span class="math inline">\(\underline{y}\)</span> the column vector of the observed values of <span class="math inline">\(Y\)</span>. Let <span class="math inline">\(\underline{\hat{y}}=(f(\underline{x}_1),\ldots,f(\underline{x}_n))&#39;\)</span> denote the corresponding vector of predictions for <span class="math inline">\(\underline{y}\)</span> for model <span class="math inline">\(f()\)</span>. </p>
<p>Let <span class="math inline">\(\mathcal L(\underline{\hat{y}}, \underline X, \underline{y})\)</span> be a loss function that quantifies goodness-of-fit of model <span class="math inline">\(f()\)</span>. For instance, <span class="math inline">\(\mathcal L()\)</span> may be the value of log-likelihood (see Chapter <a href="modelPerformance.html#modelPerformance">15</a>) or any other model performance measure discussed in previous chapter. Consider the following algorithm:</p>
<ol style="list-style-type: decimal">
<li>Compute <span class="math inline">\(L^0 = \mathcal L(\underline{\hat{y}}, \underline X, \underline{y})\)</span>, i.e., the value of the loss function for the original data. Then, for each explanatory variable <span class="math inline">\(X^j\)</span> included in the model, do steps 2-5.</li>
<li>Create matrix <span class="math inline">\(\underline{X}^{*j}\)</span> by permuting the <span class="math inline">\(j\)</span>-th column of <span class="math inline">\(\underline{X}\)</span>, i.e., by permuting the vector of observed values of <span class="math inline">\(X^j\)</span>.</li>
<li>Compute model predictions <span class="math inline">\(\underline{\hat{y}}^{*j}\)</span> based on the modified data <span class="math inline">\(\underline{X}^{*j}\)</span>.</li>
<li>Compute the value of the loss function for the modified data:
<span class="math display">\[
L^{*j} = \mathcal L(\underline{\hat{y}}^{*j}, \underline{X}^{*j}, \underline{y}).
\]</span></li>
<li>Quantify the importance of <span class="math inline">\(X^j\)</span> by calculating <span class="math inline">\(vip_{Diff}^j = L^{*j} - L^0\)</span> or <span class="math inline">\(vip_{Ratio}^j = L^{*j} / L^0\)</span>.</li>
</ol>
<p>Note that the use of resampling or permuting data in Step 2 involves randomness. Thus, the results of the procedure may depend on the obtained configuration of resampled/permuted values. Hence, it is advisable to repeat the procedure several (many) times. In this way, the uncertainty associated with the calculated variable-importance values can be assessed.</p>
<p>The calculations in Step 5 “normalize” the value of the variable-importance measure with respect to <span class="math inline">\(L^0\)</span>. However, given that <span class="math inline">\(L^0\)</span> is a constant, the normalization has no effect on the ranking of explanatory variables according to <span class="math inline">\(vip_{Diff}^j\)</span> nor <span class="math inline">\(vip_{Ratio}^j\)</span>. Thus, in practice, often the values of <span class="math inline">\(L^{*j}\)</span> are simply used to quantify a variable’s importance.</p>
</div>
<div id="featureImportanceTitanic" class="section level2" number="16.4">
<h2><span class="header-section-number">16.4</span> Example: Titanic data</h2>
<p>In this section, we illustrate the use of the permutation-based variable-importance evaluation by applying it to the random forest model for the Titanic data (see Section <a href="dataSetsIntro.html#model-titanic-rf">4.2.2</a>). Recall that the goal is to predict survival probability of passengers based on their gender, age, class in which they travelled, ticket fare, the number of persons they travelled with, and the harbour they embarked the ship on.</p>
<p>We use the area under the ROC curve (AUC, see Section <a href="modelPerformance.html#modelPerformanceMethodBinGOP">15.3.2.2</a>) as the model-performance measure. Figure <a href="featureImportance.html#fig:TitanicRFFeatImp">16.1</a> shows, for each explanatory variable included in the model, the values of <span class="math inline">\(1-AUC^{*j}\)</span> obtained by the algorithm described in the previous section. Additionally, the plot indicates the value of <span class="math inline">\(L^0\)</span> by the vertical dashed-line at the left-hand-side of the plot. The lengths of the bars correspond to <span class="math inline">\(vip_{Diff}^j\)</span> and provide the variable-importance measures.</p>

<div class="figure" style="text-align: center"><span id="fig:TitanicRFFeatImp"></span>
<img src="ema_files/figure-html/TitanicRFFeatImp-1.png" alt="Single-permutation-based variable-importance measures for the explanatory variables included in the random forest model for the Titanic data using 1-AUC as the loss function." width="70%" />
<p class="caption">
Figure 16.1: Single-permutation-based variable-importance measures for the explanatory variables included in the random forest model for the Titanic data using 1-AUC as the loss function.
</p>
</div>
<p>The plot in Figure <a href="featureImportance.html#fig:TitanicRFFeatImp">16.1</a> suggests that the most important variable in the model is <em>gender</em>. This agrees with the conclusions drawn in the exploratory analysis presented in Section <a href="dataSetsIntro.html#exploration-titanic">4.1.1</a>. The next three important variables are <em>class</em> (passengers travelling in the first class had a higher chance of survival), <em>age</em> (children had a higher chance of survival), and <em>fare</em> (owners of more expensive tickets had a higher chance of survival).</p>
<p>To take into account the uncertainty related to the use of permutations, we can consider computing the mean values of <span class="math inline">\(L^{*j}\)</span> over a set of, say, 10 permutations. The plot in Figure <a href="featureImportance.html#fig:TitanicRFFeatImp10">16.2</a> presents the mean values. The only remarkable difference, as compared to Figure <a href="featureImportance.html#fig:TitanicRFFeatImp">16.1</a>, is the change in the ordering of the <em>sibsp</em> and <em>parch</em> variables.</p>

<div class="figure" style="text-align: center"><span id="fig:TitanicRFFeatImp10"></span>
<img src="ema_files/figure-html/TitanicRFFeatImp10-1.png" alt="Means (over 10 permutations) of permutation-based variable-importance measures for the explanatory variables included in the random forest model for the Titanic data using 1-AUC as the loss function." width="70%" />
<p class="caption">
Figure 16.2: Means (over 10 permutations) of permutation-based variable-importance measures for the explanatory variables included in the random forest model for the Titanic data using 1-AUC as the loss function.
</p>
</div>
<p>Plots similar to those presented in Figures <a href="featureImportance.html#fig:TitanicRFFeatImp">16.1</a> and <a href="featureImportance.html#fig:TitanicRFFeatImp10">16.2</a> are useful for comparisons of a variable’s importance in different models. Figure <a href="featureImportance.html#fig:TitanicFeatImp">16.3</a> presents single-permutation results for the random forest, logistic regression (see Section <a href="dataSetsIntro.html#model-titanic-lmr">4.2.1</a>), and gradient boosting (see Section <a href="dataSetsIntro.html#model-titanic-gbm">4.2.3</a>) models. The best result, in terms of the smallest value of <span class="math inline">\(L^0\)</span>, is obtained for the random forest model (as indicated by the location of the dashed lines in the plots). Note that the indicated <span class="math inline">\(L^0\)</span> value for the model is different from the one indicated in Figure <a href="featureImportance.html#fig:TitanicRFFeatImp">16.1</a>. This is due to the difference in the set of (random) permutations used to compute the two values.</p>
<!---For less important variables such as embarked we see that even after its permutation the quality of the random forest model is better than the output of the additive linear regression models.--->

<div class="figure" style="text-align: center"><span id="fig:TitanicFeatImp"></span>
<img src="ema_files/figure-html/TitanicFeatImp-1.png" alt="Single-permutation-based variable-importance measures for the random forest, gradient boosting, and logistic regression models for the Titanic data with 1-AUC as the loss function. Note the different starting locations for the bars, due to differences in the AUC value obtained for the original data for different models." width="70%" />
<p class="caption">
Figure 16.3: Single-permutation-based variable-importance measures for the random forest, gradient boosting, and logistic regression models for the Titanic data with 1-AUC as the loss function. Note the different starting locations for the bars, due to differences in the AUC value obtained for the original data for different models.
</p>
</div>
<p>The plots in Figure <a href="featureImportance.html#fig:TitanicFeatImp">16.3</a> indicate that <em>gender</em> is the most important explanatory variable in all three models, followed by <em>class</em> and <em>age</em>. Variable <em>fare</em>, which is highly correlated with <em>class</em>, is important in the random forest and SVM models, but not in the logistic regression model. On the other hand, variable <em>parch</em> is, essentially, not important, neither in the gradient boosting nor in the logistic regression model, but it has some importance in the random forest model. <em>Country</em> is not important in any of the models. Overall, Figure <a href="featureImportance.html#fig:TitanicFeatImp">16.3</a> indicates that, in the random forest model, all variables (except of <em>country</em>) have got some importance, while in the other two models the effect is mainly limited to <em>gender</em>, <em>class</em>, and <em>age</em> (and <em>fare</em> for the gradient boosting model).</p>
</div>
<div id="featureImportanceProsCons" class="section level2" number="16.5">
<h2><span class="header-section-number">16.5</span> Pros and cons</h2>
<p>Permutation-based variable importance offers several advantages. It is a model-agnostic approach to the assessment of the influence of an explanatory variable on a model’s performance. The plots of variable-importance measures are easy to understand, as they are compact and present the most important variables in a single graph. The measures can be compared between models and may lead to interesting insights. For example, if variables are correlated, then models like random forest are expected to spread importance across many variables, while in regularized-regression models the effect of one variable may dominate the effect of other correlated variables.</p>
<p>The same approach can be used to measure the importance of a single explanatory variable or a group of variables. The latter is useful for “aspects,” i.e., groups of variables that are complementary to each other or are related to a similar concept. For example, in the Titanic example, the <em>fare</em> and <em>class</em> variables are related to the financial status of a passenger. Instead of assessing the importance of each of these variables separately, we may be interested in their joint importance. Toward this aim, we may compute the permutation-based measure by permuting the values of both variables at the same time.</p>
<p>The main disadvantage of the permutation-based variable-importance measure is its dependence on the random nature of the permutations. As a result, for different permutations, we will, in general, get different results. Also, the value of the measure depends on the choice of the loss function <span class="math inline">\(\mathcal L()\)</span>. Thus, there is no single, “absolute” measure.</p>
</div>
<div id="featureImportanceR" class="section level2" number="16.6">
<h2><span class="header-section-number">16.6</span> Code snippets for R</h2>
<p>In this section, we present the implementation of the permutation-based variable-importance measure in the <code>DALEX</code> package for R. The key function is <code>model_parts()</code> that allows computation of the measure. For the purposes of the computation, one can choose among several loss fuctions that include <code>loss_sum_of_squares()</code>, <code>loss_root_mean_square()</code>, <code>loss_accuracy()</code>, <code>loss_cross_entropy()</code>, and <code>loss_one_minus_auc()</code>. For the definitions of the loss functions, see Chapter <a href="modelPerformance.html#modelPerformance">15</a>.</p>
<p>For illustration purposes, we use the random forest model <code>apartments_rf</code> for the apartment-prices data (see Section <a href="dataSetsIntro.html#model-Apartments-rf">4.5.2</a>).</p>
<p>We first load the model-object via the <code>archivist</code> hook, as listed in Section <a href="dataSetsIntro.html#ListOfModelsApartments">4.5.6</a>. We also load the <code>randomForest</code> package, as the model was fitted by using function <code>randomForest()</code> from this package (see Section <a href="dataSetsIntro.html#model-Apartments-rf">4.5.2</a>) and it is important to have the corresponding <code>predict()</code> function available.<br />
Then we construct the explainer for the model by using the function <code>explain()</code> from the <code>DALEX</code> package (see Section <a href="dataSetsIntro.html#ExplainersTitanicRCode">4.2.6</a>). Note that we use the <code>apartments_test</code> data frame without the first column, i.e., the <em>m2.price</em> variable, in the <code>data</code> argument. This will be the dataset to which the model will be applied (see Section <a href="dataSetsIntro.html#ExplainersApartmentsRCode">4.5.5</a>). The <em>m2.price</em> variable is explicitly specified as the dependent variable in the <code>y</code> argument.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="featureImportance.html#cb182-1"></a><span class="kw">library</span>(<span class="st">&quot;DALEX&quot;</span>)</span>
<span id="cb182-2"><a href="featureImportance.html#cb182-2"></a><span class="kw">library</span>(<span class="st">&quot;randomForest&quot;</span>)</span>
<span id="cb182-3"><a href="featureImportance.html#cb182-3"></a>apartments_rf &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/fe7a5&quot;</span>)</span>
<span id="cb182-4"><a href="featureImportance.html#cb182-4"></a>explainer_rf &lt;-<span class="st"> </span>DALEX<span class="op">::</span><span class="kw">explain</span>(<span class="dt">model =</span> apartments_rf, </span>
<span id="cb182-5"><a href="featureImportance.html#cb182-5"></a>                               <span class="dt">data =</span> apartments_test[,<span class="op">-</span><span class="dv">1</span>], </span>
<span id="cb182-6"><a href="featureImportance.html#cb182-6"></a>                               <span class="dt">y =</span> apartments_test<span class="op">$</span>m2.price, </span>
<span id="cb182-7"><a href="featureImportance.html#cb182-7"></a>                               <span class="dt">label =</span> <span class="st">&quot;Random Forest&quot;</span>)</span></code></pre></div>
<p>A popular loss function is the root-mean-square-error (RMSE) function <a href="modelPerformance.html#eq:RMSE">(15.2)</a>. It is implemented in the <code>DALEX</code> package as the <code>loss_root_mean_square()</code> function. The latter requires two arguments: <code>observed</code>, which indicates the vector of observed values of the dependent variable, and <code>predicted</code>, which specifies the object (either vector or a matrix, as returned from the model-specific <code>predict()</code> function) with the predicted values. The original-testing-data value <span class="math inline">\(L^0\)</span> of RMSE for the random forest model can be obtained by applying the <code>loss_root_mean_square()</code> in the form given below. </p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="featureImportance.html#cb183-1"></a><span class="kw">loss_root_mean_square</span>(<span class="dt">observed =</span> apartments_test<span class="op">$</span>m2.price, </span>
<span id="cb183-2"><a href="featureImportance.html#cb183-2"></a>                   <span class="dt">predicted =</span> <span class="kw">predict</span>(apartments_rf, apartments_test))</span></code></pre></div>
<pre><code>## [1] 282.9519</code></pre>
<p>To compute the permutation-based variable-importance measure, we apply the <code>model_parts()</code> function. Note that it is a wrapper for function <code>feature_importance()</code> from the <code>ingredients</code> package. The only required argument is <code>explainer</code>, which indicates the explainer-object (obtained with the help of the <code>explain()</code> function, see Section <a href="dataSetsIntro.html#ExplainersTitanicRCode">4.2.6</a>) for the model to be explained. The other (optional) arguments are:</p>
<ul>
<li><code>loss_function</code>, the loss function to be used (by default, it is the <code>loss_root_mean_square</code> function).</li>
<li><code>type</code>, the form of the variable-importance measure, with values <code>"raw"</code> resulting in the computation of <span class="math inline">\(\mathcal L()\)</span>, <code>"difference"</code> yielding <span class="math inline">\(vip_{Diff}^j\)</span>, and <code>"ratio"</code> providing <span class="math inline">\(vip_{Ratio}^j\)</span> (see Section <a href="featureImportance.html#featureImportanceMethod">16.3</a>).</li>
<li><code>variables</code>, a character vector providing the names of the explanatory variables, for which the variable-importance measure is to be computed. By default, <code>variables = NULL</code>, in which case computations are performed for all variables in the dataset.</li>
<li><code>variable_groups</code>, a list of character vectors of names of explanatory variables. For each vector, a single variable-importance measure is computed for the joint effect of the variables which names are provided in the vector. By default, <code>variable_groups = NULL</code>, in which case variable-importance measures are computed separately for all variables indicated in the <code>variables</code> argument.</li>
<li><code>B</code>, the number of permutations to be used for the purpose of calculation of the (mean) variable-importance measures, with <code>B = 10</code> used by default. To get a single-permutation-based measure, use <code>B = 1</code>.</li>
<li><code>N</code>, the number of observations that are to be sampled from the data available in the explainer-object for the purpose of calculation of the variable-importance measure; by default, <code>N = 1000</code> is used; if <code>N = NULL</code>, the entire dataset is used.</li>
</ul>
<p>To compute a single-permutation-based value of the RMSE for all the explanatory variables included in the random forest model <code>apartments_rf</code>, we apply the <code>model_parts()</code> function to the model’s explainer-object as shown below. We use the <code>set.seed()</code> function to make the process of random selection of the permutation repeatable.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="featureImportance.html#cb185-1"></a><span class="kw">set.seed</span>(<span class="dv">1980</span>)</span>
<span id="cb185-2"><a href="featureImportance.html#cb185-2"></a><span class="kw">model_parts</span>(<span class="dt">explainer =</span> explainer_rf, </span>
<span id="cb185-3"><a href="featureImportance.html#cb185-3"></a>        <span class="dt">loss_function =</span> loss_root_mean_square,</span>
<span id="cb185-4"><a href="featureImportance.html#cb185-4"></a>                    <span class="dt">B =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##            variable mean_dropout_loss         label
## 1      _full_model_          271.9089 Random Forest
## 2 construction.year          389.4840 Random Forest
## 3          no.rooms          396.0281 Random Forest
## 4             floor          436.6190 Random Forest
## 5           surface          462.7374 Random Forest
## 6          district          794.7619 Random Forest
## 7        _baseline_         1095.4724 Random Forest</code></pre>
<p>Note that the outcome is identical to the following call below (results not shown).</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="featureImportance.html#cb187-1"></a><span class="kw">set.seed</span>(<span class="dv">1980</span>)</span>
<span id="cb187-2"><a href="featureImportance.html#cb187-2"></a><span class="kw">model_parts</span>(<span class="dt">explainer =</span> explainer_rf, </span>
<span id="cb187-3"><a href="featureImportance.html#cb187-3"></a>        <span class="dt">loss_function =</span> loss_root_mean_square,</span>
<span id="cb187-4"><a href="featureImportance.html#cb187-4"></a>                    <span class="dt">B =</span> <span class="dv">1</span>,</span>
<span id="cb187-5"><a href="featureImportance.html#cb187-5"></a>            <span class="dt">variables =</span> <span class="kw">colnames</span>(explainer_rf<span class="op">$</span>data))</span></code></pre></div>
<p>However, if we use a different ordering of the variables in the <code>variables</code> argument, the result is slightly different:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="featureImportance.html#cb188-1"></a><span class="kw">set.seed</span>(<span class="dv">1980</span>)</span>
<span id="cb188-2"><a href="featureImportance.html#cb188-2"></a>vars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;surface&quot;</span>,<span class="st">&quot;floor&quot;</span>,<span class="st">&quot;construction.year&quot;</span>,<span class="st">&quot;no.rooms&quot;</span>,<span class="st">&quot;district&quot;</span>)</span>
<span id="cb188-3"><a href="featureImportance.html#cb188-3"></a><span class="kw">model_parts</span>(<span class="dt">explainer =</span> explainer_rf, </span>
<span id="cb188-4"><a href="featureImportance.html#cb188-4"></a>        <span class="dt">loss_function =</span> loss_root_mean_square,</span>
<span id="cb188-5"><a href="featureImportance.html#cb188-5"></a>                    <span class="dt">B =</span> <span class="dv">1</span>,</span>
<span id="cb188-6"><a href="featureImportance.html#cb188-6"></a>            <span class="dt">variables =</span> vars)</span></code></pre></div>
<pre><code>##            variable mean_dropout_loss         label
## 1      _full_model_          271.9089 Random Forest
## 2 construction.year          393.1586 Random Forest
## 3          no.rooms          396.0281 Random Forest
## 4             floor          440.9293 Random Forest
## 5           surface          483.1104 Random Forest
## 6          district          794.7619 Random Forest
## 7        _baseline_         1095.4724 Random Forest</code></pre>
<p>This is due to the fact that, despite the same seed, the first permutation is now selected for the <em>surface</em> variable, while in the previous code the same permutation was applied to the values of the <em>floor</em> variable.</p>
<p>To compute the mean variable-importance measure based on 50 permutations and using the RMSE difference <span class="math inline">\(vip_{Diff}^j\)</span> (see Section <a href="featureImportance.html#featureImportanceMethod">16.3</a>), we have got to specify the appropriate values of the <code>B</code> and <code>type</code> arguments.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="featureImportance.html#cb190-1"></a><span class="kw">set.seed</span>(<span class="dv">1980</span>)</span>
<span id="cb190-2"><a href="featureImportance.html#cb190-2"></a>(vip<span class="fl">.50</span> &lt;-<span class="st"> </span><span class="kw">model_parts</span>(<span class="dt">explainer =</span> explainer_rf, </span>
<span id="cb190-3"><a href="featureImportance.html#cb190-3"></a>                   <span class="dt">loss_function =</span> loss_root_mean_square,</span>
<span id="cb190-4"><a href="featureImportance.html#cb190-4"></a>                               <span class="dt">B =</span> <span class="dv">50</span>,</span>
<span id="cb190-5"><a href="featureImportance.html#cb190-5"></a>                            <span class="dt">type =</span> <span class="st">&quot;difference&quot;</span>))</span></code></pre></div>
<pre><code>##            variable mean_dropout_loss         label
## 1      _full_model_            0.0000 Random Forest
## 2          no.rooms          117.4678 Random Forest
## 3 construction.year          122.4445 Random Forest
## 4             floor          162.4554 Random Forest
## 5           surface          182.4368 Random Forest
## 6          district          563.7343 Random Forest
## 7        _baseline_          843.0472 Random Forest</code></pre>
<p>To obtain a graphical illustration, we apply the <code>plot()</code> function to the <code>vip.50</code> object.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="featureImportance.html#cb192-1"></a><span class="kw">library</span>(<span class="st">&quot;ggplot2&quot;</span>)</span>
<span id="cb192-2"><a href="featureImportance.html#cb192-2"></a><span class="kw">plot</span>(vip<span class="fl">.50</span>) <span class="op">+</span></span>
<span id="cb192-3"><a href="featureImportance.html#cb192-3"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Mean variable-importance over 50 permutations&quot;</span>, <span class="st">&quot;&quot;</span>) </span></code></pre></div>
<p>The resulting graph is presented in Figure <a href="featureImportance.html#fig:featureImportanceUnoPlot">16.4</a>. The bars in the plot indicate the mean values of the variable-importance measures for all explanatory variables. Box plots are added to the bars to provide an idea about the distribution of the values of the measure across the permutations.</p>

<div class="figure" style="text-align: center"><span id="fig:featureImportanceUnoPlot"></span>
<img src="ema_files/figure-html/featureImportanceUnoPlot-1.png" alt="Mean variable-importance calculated by using 50 permutations and the root-mean-squared-error loss-function for the random forest model apartments_rf for the apartment-prices data. Plot obtained by using the generic plot() function in R." width="70%" />
<p class="caption">
Figure 16.4: Mean variable-importance calculated by using 50 permutations and the root-mean-squared-error loss-function for the random forest model <code>apartments_rf</code> for the apartment-prices data. Plot obtained by using the generic <code>plot()</code> function in R.
</p>
</div>
<p>Variable-importance measures are a very useful tool for model comparison. We will illustrate this application by considering the random forest model, linear-regression model (Section <a href="dataSetsIntro.html#model-Apartments-lr">4.5.1</a>), and support-vector-machine (SVM) model (Section <a href="dataSetsIntro.html#model-Apartments-svm">4.5.3</a>) for the apartment prices dataset. The models differ in their flexibility and structure; hence, it may be of interest to compare them.</p>
<p>We first load the necessary model-objects via the <code>archivist</code> hooks, as listed in Section <a href="dataSetsIntro.html#ListOfModelsApartments">4.5.6</a>.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="featureImportance.html#cb193-1"></a>apartments_lm  &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/55f19&quot;</span>)</span>
<span id="cb193-2"><a href="featureImportance.html#cb193-2"></a>apartments_svm &lt;-<span class="st"> </span>archivist<span class="op">::</span><span class="kw">aread</span>(<span class="st">&quot;pbiecek/models/d2ca0&quot;</span>)</span></code></pre></div>
<p>Then we construct the corresponding explainer-objects. We also load the <code>e1071</code> package, as it is important to have a suitable <code>predict()</code> function available for the SVM model.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="featureImportance.html#cb194-1"></a>explainer_lm &lt;-<span class="st"> </span>DALEX<span class="op">::</span><span class="kw">explain</span>(<span class="dt">model =</span> apartments_lm, </span>
<span id="cb194-2"><a href="featureImportance.html#cb194-2"></a>                               <span class="dt">data =</span> apartments_test[,<span class="op">-</span><span class="dv">1</span>], </span>
<span id="cb194-3"><a href="featureImportance.html#cb194-3"></a>                               <span class="dt">y =</span> apartments_test<span class="op">$</span>m2.price, </span>
<span id="cb194-4"><a href="featureImportance.html#cb194-4"></a>                               <span class="dt">label =</span> <span class="st">&quot;Linear Regression&quot;</span>)</span>
<span id="cb194-5"><a href="featureImportance.html#cb194-5"></a></span>
<span id="cb194-6"><a href="featureImportance.html#cb194-6"></a><span class="kw">library</span>(<span class="st">&quot;e1071&quot;</span>)</span>
<span id="cb194-7"><a href="featureImportance.html#cb194-7"></a>explainer_svm &lt;-<span class="st"> </span>DALEX<span class="op">::</span><span class="kw">explain</span>(<span class="dt">model =</span> apartments_svm, </span>
<span id="cb194-8"><a href="featureImportance.html#cb194-8"></a>                                <span class="dt">data =</span> apartments_test[,<span class="op">-</span><span class="dv">1</span>], </span>
<span id="cb194-9"><a href="featureImportance.html#cb194-9"></a>                                <span class="dt">y =</span> apartments_test<span class="op">$</span>m2.price, </span>
<span id="cb194-10"><a href="featureImportance.html#cb194-10"></a>                                <span class="dt">label =</span> <span class="st">&quot;Support Vector Machine&quot;</span>)</span></code></pre></div>
<p>Subsequently, we compute mean values of the permutation-based variable-importance measure for 50 permutations and the RMSE loss function. Note that we use the <code>set.seed()</code> function to make the process of random selection of the permutation repeatable. By specifying <code>N = NULL</code> we include all the data from the apartments dataset in the calculations.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="featureImportance.html#cb195-1"></a>vip_lm  &lt;-<span class="st"> </span><span class="kw">model_parts</span>(<span class="dt">explainer =</span> explainer_lm,  <span class="dt">B =</span> <span class="dv">50</span>, <span class="dt">N =</span> <span class="ot">NULL</span>)</span>
<span id="cb195-2"><a href="featureImportance.html#cb195-2"></a>vip_rf  &lt;-<span class="st"> </span><span class="kw">model_parts</span>(<span class="dt">explainer =</span> explainer_rf,  <span class="dt">B =</span> <span class="dv">50</span>, <span class="dt">N =</span> <span class="ot">NULL</span>)</span>
<span id="cb195-3"><a href="featureImportance.html#cb195-3"></a>vip_svm &lt;-<span class="st"> </span><span class="kw">model_parts</span>(<span class="dt">explainer =</span> explainer_svm, <span class="dt">B =</span> <span class="dv">50</span>, <span class="dt">N =</span> <span class="ot">NULL</span>)</span></code></pre></div>
<p>Finally, we apply the <code>plot()</code> function to the created objects to obtain a single plot with the variable-importance measures for all three models.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="featureImportance.html#cb196-1"></a><span class="kw">library</span>(<span class="st">&quot;ggplot2&quot;</span>)</span>
<span id="cb196-2"><a href="featureImportance.html#cb196-2"></a><span class="kw">plot</span>(vip_rf, vip_svm, vip_lm) <span class="op">+</span></span>
<span id="cb196-3"><a href="featureImportance.html#cb196-3"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Mean variable-importance over 50 permutations&quot;</span>, <span class="st">&quot;&quot;</span>) </span></code></pre></div>
<p>The resulting graph is presented in Figure <a href="featureImportance.html#fig:featureImportanceTriPlot">16.5</a>. The plots suggest that the best result, in terms of the smallest value of <span class="math inline">\(L^0\)</span>, is obtained for the SVM model (as indicated by the location of the dashed lines in the plots). The length of bars indicates that <em>district</em> is the most important explanatory variable in all three models, followed by <em>surface</em> and <em>floor</em>. <em>Construction year</em> is the fourth most important variable for the random forest and SVM models, but it is not important in the linear-regression model at all. We will investigate the reason for this difference in the next chapter.</p>

<div class="figure" style="text-align: center"><span id="fig:featureImportanceTriPlot"></span>
<img src="ema_files/figure-html/featureImportanceTriPlot-1.png" alt="Mean variable-importance calculated using 50 permutations and the root-mean-squared-error loss for the random forest, support-vector-machine, and linear-regression models for the apartment-prices data." width="90%" />
<p class="caption">
Figure 16.5: Mean variable-importance calculated using 50 permutations and the root-mean-squared-error loss for the random forest, support-vector-machine, and linear-regression models for the apartment-prices data.
</p>
</div>
</div>
<div id="featureImportancePython" class="section level2" number="16.7">
<h2><span class="header-section-number">16.7</span> Code snippets for Python</h2>
<p>In this section, we use the <code>dalex</code> library for Python. The package covers all methods presented in this chapter. It is available on <code>pip</code> and <code>GitHub</code>.</p>
<p>For illustration purposes, we use the <code>titanic_rf</code> random forest model for the Titanic data developed in Section <a href="dataSetsIntro.html#model-titanic-python-rf">4.3.2</a>. Recall that the model is developed to predict the probability of survival for passengers of Titanic.</p>
<p>In the first step, we create an explainer-object that will provide a uniform interface for the predictive model. We use the <code>Explainer()</code> constructor for this purpose.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb197-1"><a href="featureImportance.html#cb197-1"></a><span class="im">import</span> dalex <span class="im">as</span> dx</span>
<span id="cb197-2"><a href="featureImportance.html#cb197-2"></a>titanic_rf_exp <span class="op">=</span> dx.Explainer(titanic_rf, X, y, </span>
<span id="cb197-3"><a href="featureImportance.html#cb197-3"></a>                  label <span class="op">=</span> <span class="st">&quot;Titanic RF Pipeline&quot;</span>)</span></code></pre></div>
<p>To calculate the variable-importance measure, we use the <code>model_parts()</code> method. By default it performs <code>B = 10</code> permutations of variable importance calculated on <code>N = 1000</code> observations.</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb198-1"><a href="featureImportance.html#cb198-1"></a>mp_rf <span class="op">=</span> titanic_rf_exp.model_parts()</span>
<span id="cb198-2"><a href="featureImportance.html#cb198-2"></a>mp_rf.result</span></code></pre></div>
<p><img src="figure/python_model_parts_2.png" width="60%" /></p>
<p>The obtained results can be visualised by using the <code>plot()</code> method. Results are presented in Figure <a href="featureImportance.html#fig:examplePythonFIM2">16.6</a>.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb199-1"><a href="featureImportance.html#cb199-1"></a>mp_rf.plot()</span></code></pre></div>

<div class="figure" style="text-align: center"><span id="fig:examplePythonFIM2"></span>
<img src="figure/python_model_parts_1.png" alt="Mean variable-importance calculated by using 10 permutations and the root-mean-squared-error loss-function for the random forest model for the Titanic data." width="90%" />
<p class="caption">
Figure 16.6: Mean variable-importance calculated by using 10 permutations and the root-mean-squared-error loss-function for the random forest model for the Titanic data.
</p>
</div>
<p>The <code>model_parts()</code> method in Python allows similar arguments as the corresponding function in the <code>DALEX</code> package in R (see Section <a href="featureImportance.html#featureImportanceR">16.6</a>). These include, for example, the <code>loss_function</code> argument (with values like, e.g. ,<code>'rmse'</code> or <code>'1-auc'</code>); the <code>type</code> argument, (with values <code>'variable_importance'</code>, <code>'ratio'</code>, <code>'difference'</code>); and the <code>variable_groups</code> argument that allows specifying groups of explanatory variables, for which a single variable-importance measure should be computed.</p>
<p>In the code below, we illustrate the use of the <code>variable_groups</code> argument to specify two groups of variables. The resulting plot is presented in Figure <a href="featureImportance.html#fig:examplePythonFIM5">16.7</a>.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb200-1"><a href="featureImportance.html#cb200-1"></a>vi_grouped <span class="op">=</span> titanic_rf_exp.model_parts(</span>
<span id="cb200-2"><a href="featureImportance.html#cb200-2"></a>                variable_groups<span class="op">=</span>{<span class="st">&#39;personal&#39;</span>: [<span class="st">&#39;gender&#39;</span>, <span class="st">&#39;age&#39;</span>, </span>
<span id="cb200-3"><a href="featureImportance.html#cb200-3"></a>                                              <span class="st">&#39;sibsp&#39;</span>, <span class="st">&#39;parch&#39;</span>],</span>
<span id="cb200-4"><a href="featureImportance.html#cb200-4"></a>                                   <span class="st">&#39;wealth&#39;</span>: [<span class="st">&#39;class&#39;</span>, <span class="st">&#39;fare&#39;</span>]})</span>
<span id="cb200-5"><a href="featureImportance.html#cb200-5"></a>vi_grouped.result</span></code></pre></div>
<p><img src="figure/python_model_parts_6.png" width="60%" /></p>
<div class="sourceCode" id="cb201"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb201-1"><a href="featureImportance.html#cb201-1"></a>vi_grouped.plot()</span></code></pre></div>

<div class="figure" style="text-align: center"><span id="fig:examplePythonFIM5"></span>
<img src="figure/python_model_parts_5.png" alt="Mean variable-importance calculated for two groups of variables for the random forest model for the Titanic data." width="90%" />
<p class="caption">
Figure 16.7: Mean variable-importance calculated for two groups of variables for the random forest model for the Titanic data.
</p>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-randomForestBreiman">
<p>Breiman, Leo. 2001a. “Random Forests.” <em>Machine Learning</em> 45: 5–32. <a href="https://doi.org/10.1023/a:1010933404324">https://doi.org/10.1023/a:1010933404324</a>.</p>
</div>
<div id="ref-variableImportancePermutations">
<p>Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2019. “All Models Are Wrong, but Many Are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously.” <em>Journal of Machine Learning Research</em> 20 (177): 1–81. <a href="http://jmlr.org/papers/v20/18-760.html">http://jmlr.org/papers/v20/18-760.html</a>.</p>
</div>
<div id="ref-xgboostExplainer">
<p>Foster, David. 2017. <em>XgboostExplainer: An R Package That Makes Xgboost Models Fully Interpretable</em>. <a href="https://github.com/AppliedDataSciencePartners/xgboostExplainer/">https://github.com/AppliedDataSciencePartners/xgboostExplainer/</a>.</p>
</div>
<div id="ref-randomForestExplainer">
<p>Paluszynska, Aleksandra, and Przemyslaw Biecek. 2017. <em>RandomForestExplainer: A Set of Tools to Understand What Is Happening Inside a Random Forest</em>. <a href="https://github.com/MI2DataLab/randomForestExplainer">https://github.com/MI2DataLab/randomForestExplainer</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modelPerformance.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="partialDependenceProfiles.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ema.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
