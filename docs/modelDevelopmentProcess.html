<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Model Development | Explanatory Model Analysis</title>
  <meta name="description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Model Development | Explanatory Model Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  <meta name="github-repo" content="pbiecek/ema" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Model Development | Explanatory Model Analysis" />
  
  <meta name="twitter:description" content="This book introduces unified language for exploration, explanation and examination of predictive machine learning models." />
  

<meta name="author" content="Przemyslaw Biecek and Tomasz Burzykowski" />


<meta name="date" content="2020-07-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ChapIntroduction.html"/>
<link rel="next" href="doItYourselfWithR.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-5650686-15', 'https://pbiecek.github.io/ema/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>
<style>
.figure {
   padding:40px 0px;
}
</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><span style="font-size: large">Explanatory Model Analysis</span><br/>Explore, Explain and Examine<br/>Predictive Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#notes-to-readers"><i class="fa fa-check"></i><b>1.1</b> Notes to readers</a></li>
<li class="chapter" data-level="1.2" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#the-aim-of-the-book"><i class="fa fa-check"></i><b>1.2</b> The aim of the book</a></li>
<li class="chapter" data-level="1.3" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#three-single-laws"><i class="fa fa-check"></i><b>1.3</b> A bit of philosophy: three laws of model explanation</a></li>
<li class="chapter" data-level="1.4" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#teminology"><i class="fa fa-check"></i><b>1.4</b> Terminology</a></li>
<li class="chapter" data-level="1.5" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#glassblack"><i class="fa fa-check"></i><b>1.5</b> Black-box models and glass-box models</a></li>
<li class="chapter" data-level="1.6" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#agnosticspecific"><i class="fa fa-check"></i><b>1.6</b> Model-agnostic and model-specific approach</a></li>
<li class="chapter" data-level="1.7" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#bookstructure"><i class="fa fa-check"></i><b>1.7</b> The structure of the book</a></li>
<li class="chapter" data-level="1.8" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#whatisinthebook"><i class="fa fa-check"></i><b>1.8</b> What is included in this book and what is not</a></li>
<li class="chapter" data-level="1.9" data-path="ChapIntroduction.html"><a href="ChapIntroduction.html#thanksto"><i class="fa fa-check"></i><b>1.9</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html"><i class="fa fa-check"></i><b>2</b> Model Development</a>
<ul>
<li class="chapter" data-level="2.1" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPIntro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#MDPprocess"><i class="fa fa-check"></i><b>2.2</b> Model-development process</a></li>
<li class="chapter" data-level="2.3" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#notation"><i class="fa fa-check"></i><b>2.3</b> Notation</a></li>
<li class="chapter" data-level="2.4" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#dataunderstanding"><i class="fa fa-check"></i><b>2.4</b> Data understanding</a></li>
<li class="chapter" data-level="2.5" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#fitting"><i class="fa fa-check"></i><b>2.5</b> Model assembly (fitting)</a></li>
<li class="chapter" data-level="2.6" data-path="modelDevelopmentProcess.html"><a href="modelDevelopmentProcess.html#validation"><i class="fa fa-check"></i><b>2.6</b> Model audit</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html"><i class="fa fa-check"></i><b>3</b> Do-it-yourself with R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#what-to-install"><i class="fa fa-check"></i><b>3.1</b> What to install?</a></li>
<li class="chapter" data-level="3.2" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#infoDALEX"><i class="fa fa-check"></i><b>3.2</b> How to work with <code>DALEX</code>?</a></li>
<li class="chapter" data-level="3.3" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#how-to-work-with-archivist"><i class="fa fa-check"></i><b>3.3</b> How to work with <code>archivist</code>?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html"><i class="fa fa-check"></i><b>4</b> Do-it-yourself with Python</a>
<ul>
<li class="chapter" data-level="4.1" data-path="doItYourselfWithR.html"><a href="doItYourselfWithR.html#what-to-install"><i class="fa fa-check"></i><b>4.1</b> What to install?</a></li>
<li class="chapter" data-level="4.2" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html#infoDALEXpy"><i class="fa fa-check"></i><b>4.2</b> How to work with <code>dalex</code>?</a></li>
<li class="chapter" data-level="4.3" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html#code-snippets-for-python"><i class="fa fa-check"></i><b>4.3</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html"><i class="fa fa-check"></i><b>5</b> Datasets and models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#TitanicDataset"><i class="fa fa-check"></i><b>5.1</b> Sinking of the RMS Titanic</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-titanic"><i class="fa fa-check"></i><b>5.1.1</b> Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#r-classification-models-for-titanic"><i class="fa fa-check"></i><b>5.2</b> R classification models for Titanic</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-lmr"><i class="fa fa-check"></i><b>5.2.1</b> Logistic-regression model</a></li>
<li class="chapter" data-level="5.2.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-rf"><i class="fa fa-check"></i><b>5.2.2</b> Random-forest model</a></li>
<li class="chapter" data-level="5.2.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-gbm"><i class="fa fa-check"></i><b>5.2.3</b> Gradient-boosting model</a></li>
<li class="chapter" data-level="5.2.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-svm"><i class="fa fa-check"></i><b>5.2.4</b> Support Vector Machine model for Classification</a></li>
<li class="chapter" data-level="5.2.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic"><i class="fa fa-check"></i><b>5.2.5</b> Models’ predictions</a></li>
<li class="chapter" data-level="5.2.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicRCode"><i class="fa fa-check"></i><b>5.2.6</b> Models’ explainers</a></li>
<li class="chapter" data-level="5.2.7" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsTitanic"><i class="fa fa-check"></i><b>5.2.7</b> List of objects for the Titanic example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#python-classification-models-for-titanic"><i class="fa fa-check"></i><b>5.3</b> Python classification models for Titanic</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-lr"><i class="fa fa-check"></i><b>5.3.1</b> Logistic-regression model</a></li>
<li class="chapter" data-level="5.3.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-rf"><i class="fa fa-check"></i><b>5.3.2</b> Random-forest model</a></li>
<li class="chapter" data-level="5.3.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-gbm"><i class="fa fa-check"></i><b>5.3.3</b> Gradient-boosting model</a></li>
<li class="chapter" data-level="5.3.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-titanic-python-svm"><i class="fa fa-check"></i><b>5.3.4</b> Support Vector Machine model for Classification</a></li>
<li class="chapter" data-level="5.3.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-titanic-python"><i class="fa fa-check"></i><b>5.3.5</b> Models’ predictions</a></li>
<li class="chapter" data-level="5.3.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersTitanicPythonCode"><i class="fa fa-check"></i><b>5.3.6</b> Models’ explainers</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ApartmentDataset"><i class="fa fa-check"></i><b>5.4</b> Apartment prices</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#exploration-apartments"><i class="fa fa-check"></i><b>5.4.1</b> Data exploration</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#r-regression-model-for-apartment-prices"><i class="fa fa-check"></i><b>5.5</b> R regression model for Apartment prices</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-lr"><i class="fa fa-check"></i><b>5.5.1</b> Linear-regression model</a></li>
<li class="chapter" data-level="5.5.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-rf"><i class="fa fa-check"></i><b>5.5.2</b> Random-forest model</a></li>
<li class="chapter" data-level="5.5.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-svm"><i class="fa fa-check"></i><b>5.5.3</b> Support Vector Machine model for Regression</a></li>
<li class="chapter" data-level="5.5.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictionsApartments"><i class="fa fa-check"></i><b>5.5.4</b> Models’ predictions</a></li>
<li class="chapter" data-level="5.5.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsRCode"><i class="fa fa-check"></i><b>5.5.5</b> Models’ explainers</a></li>
<li class="chapter" data-level="5.5.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ListOfModelsApartments"><i class="fa fa-check"></i><b>5.5.6</b> List of objects for the Apartment prices example</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#python-regression-models-for-apartment-prices"><i class="fa fa-check"></i><b>5.6</b> Python regression models for Apartment prices</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-lr"><i class="fa fa-check"></i><b>5.6.1</b> Linear-regression model</a></li>
<li class="chapter" data-level="5.6.2" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-rf"><i class="fa fa-check"></i><b>5.6.2</b> Random-forest model</a></li>
<li class="chapter" data-level="5.6.3" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#model-Apartments-python-svm"><i class="fa fa-check"></i><b>5.6.3</b> Support Vector Machine model for Regression</a></li>
<li class="chapter" data-level="5.6.4" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#predictions-apartments-python"><i class="fa fa-check"></i><b>5.6.4</b> Models’ predictions</a></li>
<li class="chapter" data-level="5.6.5" data-path="dataSetsIntro.html"><a href="dataSetsIntro.html#ExplainersApartmentsPythonCode"><i class="fa fa-check"></i><b>5.6.5</b> Models’ explainers</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Instance Level</b></span></li>
<li class="chapter" data-level="6" data-path="InstanceLevelExploration.html"><a href="InstanceLevelExploration.html"><i class="fa fa-check"></i><b>6</b> Introduction to Instance-level Exploration</a></li>
<li class="chapter" data-level="7" data-path="breakDown.html"><a href="breakDown.html"><i class="fa fa-check"></i><b>7</b> Break-down Plots for Additive Attributions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="breakDown.html"><a href="breakDown.html#BDIntroduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="breakDown.html"><a href="breakDown.html#BDIntuition"><i class="fa fa-check"></i><b>7.2</b> Intuition</a></li>
<li class="chapter" data-level="7.3" data-path="breakDown.html"><a href="breakDown.html#BDMethod"><i class="fa fa-check"></i><b>7.3</b> Method</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="breakDown.html"><a href="breakDown.html#BDMethodLin"><i class="fa fa-check"></i><b>7.3.1</b> Break-down for linear models</a></li>
<li class="chapter" data-level="7.3.2" data-path="breakDown.html"><a href="breakDown.html#BDMethodGen"><i class="fa fa-check"></i><b>7.3.2</b> Break-down for a general case</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="breakDown.html"><a href="breakDown.html#BDExample"><i class="fa fa-check"></i><b>7.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="7.5" data-path="breakDown.html"><a href="breakDown.html#BDProsCons"><i class="fa fa-check"></i><b>7.5</b> Pros and cons</a></li>
<li class="chapter" data-level="7.6" data-path="breakDown.html"><a href="breakDown.html#BDR"><i class="fa fa-check"></i><b>7.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="breakDown.html"><a href="breakDown.html#basic-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>7.6.1</b> Basic use of the <code>predict_parts()</code> function</a></li>
<li class="chapter" data-level="7.6.2" data-path="breakDown.html"><a href="breakDown.html#advanced-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>7.6.2</b> Advanced use of the <code>predict_parts()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="breakDown.html"><a href="breakDown.html#BDPython"><i class="fa fa-check"></i><b>7.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="iBreakDown.html"><a href="iBreakDown.html"><i class="fa fa-check"></i><b>8</b> Break-down Plots for Interactions (iBreak-down Plots)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDIntuition"><i class="fa fa-check"></i><b>8.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDMethod"><i class="fa fa-check"></i><b>8.2</b> Method</a></li>
<li class="chapter" data-level="8.3" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDExample"><i class="fa fa-check"></i><b>8.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="8.4" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDProsCons"><i class="fa fa-check"></i><b>8.4</b> Pros and cons</a></li>
<li class="chapter" data-level="8.5" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDRcode"><i class="fa fa-check"></i><b>8.5</b> Code snippets for R</a></li>
<li class="chapter" data-level="8.6" data-path="iBreakDown.html"><a href="iBreakDown.html#iBDPythonCode"><i class="fa fa-check"></i><b>8.6</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>9</b> Shapley Additive Explanations (SHAP) and Average Variable Attributions</a>
<ul>
<li class="chapter" data-level="9.1" data-path="shapley.html"><a href="shapley.html#SHAPIntuition"><i class="fa fa-check"></i><b>9.1</b> Intuition</a></li>
<li class="chapter" data-level="9.2" data-path="shapley.html"><a href="shapley.html#SHAPMethod"><i class="fa fa-check"></i><b>9.2</b> Method</a></li>
<li class="chapter" data-level="9.3" data-path="shapley.html"><a href="shapley.html#SHAPExample"><i class="fa fa-check"></i><b>9.3</b> Example: Titanic data</a></li>
<li class="chapter" data-level="9.4" data-path="shapley.html"><a href="shapley.html#SHAProsCons"><i class="fa fa-check"></i><b>9.4</b> Pros and cons</a></li>
<li class="chapter" data-level="9.5" data-path="shapley.html"><a href="shapley.html#SHAPRcode"><i class="fa fa-check"></i><b>9.5</b> Code snippets for R</a></li>
<li class="chapter" data-level="9.6" data-path="shapley.html"><a href="shapley.html#SHAPPythonCode"><i class="fa fa-check"></i><b>9.6</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="LIME.html"><a href="LIME.html"><i class="fa fa-check"></i><b>10</b> Local Interpretable Model-agnostic Explanations (LIME)</a>
<ul>
<li class="chapter" data-level="10.1" data-path="LIME.html"><a href="LIME.html#LIMEIntroduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="LIME.html"><a href="LIME.html#LIMEIntuition"><i class="fa fa-check"></i><b>10.2</b> Intuition</a></li>
<li class="chapter" data-level="10.3" data-path="LIME.html"><a href="LIME.html#LIMEMethod"><i class="fa fa-check"></i><b>10.3</b> Method</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="LIME.html"><a href="LIME.html#LIMErepr"><i class="fa fa-check"></i><b>10.3.1</b> Interpretable data representation</a></li>
<li class="chapter" data-level="10.3.2" data-path="LIME.html"><a href="LIME.html#LIMEsample"><i class="fa fa-check"></i><b>10.3.2</b> Sampling around the instance of interest</a></li>
<li class="chapter" data-level="10.3.3" data-path="LIME.html"><a href="LIME.html#LIMEglas"><i class="fa fa-check"></i><b>10.3.3</b> Fitting the glass-box model</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="LIME.html"><a href="LIME.html#LIMEExample"><i class="fa fa-check"></i><b>10.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="10.5" data-path="LIME.html"><a href="LIME.html#LIMEProsCons"><i class="fa fa-check"></i><b>10.5</b> Pros and cons</a></li>
<li class="chapter" data-level="10.6" data-path="LIME.html"><a href="LIME.html#LIMERcode"><i class="fa fa-check"></i><b>10.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="LIME.html"><a href="LIME.html#the-lime-package"><i class="fa fa-check"></i><b>10.6.1</b> The <code>lime</code> package</a></li>
<li class="chapter" data-level="10.6.2" data-path="LIME.html"><a href="LIME.html#the-localmodel-package"><i class="fa fa-check"></i><b>10.6.2</b> The <code>localModel</code> package</a></li>
<li class="chapter" data-level="10.6.3" data-path="LIME.html"><a href="LIME.html#the-iml-package"><i class="fa fa-check"></i><b>10.6.3</b> The <code>iml</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ceterisParibus.html"><a href="ceterisParibus.html"><i class="fa fa-check"></i><b>11</b> Ceteris-paribus Profiles</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntro"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPIntuition"><i class="fa fa-check"></i><b>11.2</b> Intuition</a></li>
<li class="chapter" data-level="11.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPMethod"><i class="fa fa-check"></i><b>11.3</b> Method</a></li>
<li class="chapter" data-level="11.4" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPExample"><i class="fa fa-check"></i><b>11.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="11.5" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPProsCons"><i class="fa fa-check"></i><b>11.5</b> Pros and cons</a></li>
<li class="chapter" data-level="11.6" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPR"><i class="fa fa-check"></i><b>11.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ceterisParibus.html"><a href="ceterisParibus.html#basic-use-of-the-predict_profile-function"><i class="fa fa-check"></i><b>11.6.1</b> Basic use of the <code>predict_profile()</code> function</a></li>
<li class="chapter" data-level="11.6.2" data-path="ceterisParibus.html"><a href="ceterisParibus.html#advanced-use-of-the-predict_profile-function"><i class="fa fa-check"></i><b>11.6.2</b> Advanced use of the <code>predict_profile()</code> function</a></li>
<li class="chapter" data-level="11.6.3" data-path="ceterisParibus.html"><a href="ceterisParibus.html#comparison-of-models-challenger-champion-analysis"><i class="fa fa-check"></i><b>11.6.3</b> Comparison of models (challenger-champion analysis)</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ceterisParibus.html"><a href="ceterisParibus.html#CPPython"><i class="fa fa-check"></i><b>11.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html"><i class="fa fa-check"></i><b>12</b> Ceteris-paribus Oscillations</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscIntuition"><i class="fa fa-check"></i><b>12.2</b> Intuition</a></li>
<li class="chapter" data-level="12.3" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscMethod"><i class="fa fa-check"></i><b>12.3</b> Method</a></li>
<li class="chapter" data-level="12.4" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscExample"><i class="fa fa-check"></i><b>12.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="12.5" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscProsCons"><i class="fa fa-check"></i><b>12.5</b> Pros and cons</a></li>
<li class="chapter" data-level="12.6" data-path="ceterisParibusOscillations.html"><a href="ceterisParibusOscillations.html#CPOscR"><i class="fa fa-check"></i><b>12.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="breakDown.html"><a href="breakDown.html#basic-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>12.6.1</b> Basic use of the <code>predict_parts()</code> function</a></li>
<li class="chapter" data-level="12.6.2" data-path="breakDown.html"><a href="breakDown.html#advanced-use-of-the-predict_parts-function"><i class="fa fa-check"></i><b>12.6.2</b> Advanced use of the <code>predict_parts()</code> function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="localDiagnostics.html"><a href="localDiagnostics.html"><i class="fa fa-check"></i><b>13</b> Local-diagnostics Plots</a>
<ul>
<li class="chapter" data-level="13.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntro"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagIntuition"><i class="fa fa-check"></i><b>13.2</b> Intuition</a></li>
<li class="chapter" data-level="13.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagMethod"><i class="fa fa-check"></i><b>13.3</b> Method</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagNeighbors"><i class="fa fa-check"></i><b>13.3.1</b> Nearest neighbors</a></li>
<li class="chapter" data-level="13.3.2" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagLFplot"><i class="fa fa-check"></i><b>13.3.2</b> Local-fidelity plot</a></li>
<li class="chapter" data-level="13.3.3" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProfiles"><i class="fa fa-check"></i><b>13.3.3</b> Local-stability plot</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagExample"><i class="fa fa-check"></i><b>13.4</b> Example: Titanic</a></li>
<li class="chapter" data-level="13.5" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagProsCons"><i class="fa fa-check"></i><b>13.5</b> Pros and cons</a></li>
<li class="chapter" data-level="13.6" data-path="localDiagnostics.html"><a href="localDiagnostics.html#cPLocDiagR"><i class="fa fa-check"></i><b>13.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html"><i class="fa fa-check"></i><b>14</b> Summary of Instance-level Exploration</a>
<ul>
<li class="chapter" data-level="14.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#summaryInstanceLevelIntro"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#number-of-explanatory-variables-in-the-model"><i class="fa fa-check"></i><b>14.2</b> Number of explanatory variables in the model</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#low-to-medium-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.2.1</b> Low to medium number of explanatory variables</a></li>
<li class="chapter" data-level="14.2.2" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#medium-to-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.2.2</b> Medium to large number of explanatory variables</a></li>
<li class="chapter" data-level="14.2.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#very-large-number-of-explanatory-variables"><i class="fa fa-check"></i><b>14.2.3</b> Very large number of explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#correlated-explanatory-variables"><i class="fa fa-check"></i><b>14.3</b> Correlated explanatory variables</a></li>
<li class="chapter" data-level="14.4" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#models-with-interactions"><i class="fa fa-check"></i><b>14.4</b> Models with interactions</a></li>
<li class="chapter" data-level="14.5" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#sparse-explanations"><i class="fa fa-check"></i><b>14.5</b> Sparse explanations</a></li>
<li class="chapter" data-level="14.6" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#additional-uses-of-model-exploration-and-explanation"><i class="fa fa-check"></i><b>14.6</b> Additional uses of model exploration and explanation</a></li>
<li class="chapter" data-level="14.7" data-path="summaryInstanceLevel.html"><a href="summaryInstanceLevel.html#comparison-of-models-champion-challenger-analysis"><i class="fa fa-check"></i><b>14.7</b> Comparison of models (champion-challenger analysis)</a></li>
</ul></li>
<li class="part"><span><b>III Dataset Level</b></span></li>
<li class="chapter" data-level="15" data-path="modelLevelExploration.html"><a href="modelLevelExploration.html"><i class="fa fa-check"></i><b>15</b> Introduction to Dataset-level Exploration</a></li>
<li class="chapter" data-level="16" data-path="modelPerformance.html"><a href="modelPerformance.html"><i class="fa fa-check"></i><b>16</b> Model-performance Measures</a>
<ul>
<li class="chapter" data-level="16.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntro"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceIntuition"><i class="fa fa-check"></i><b>16.2</b> Intuition</a></li>
<li class="chapter" data-level="16.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethod"><i class="fa fa-check"></i><b>16.3</b> Method</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCont"><i class="fa fa-check"></i><b>16.3.1</b> Continuous dependent variable</a></li>
<li class="chapter" data-level="16.3.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodBin"><i class="fa fa-check"></i><b>16.3.2</b> Binary dependent variable</a></li>
<li class="chapter" data-level="16.3.3" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCateg"><i class="fa fa-check"></i><b>16.3.3</b> Categorical dependent variable</a></li>
<li class="chapter" data-level="16.3.4" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceMethodCount"><i class="fa fa-check"></i><b>16.3.4</b> Count dependent variable</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="modelPerformance.html"><a href="modelPerformance.html#example"><i class="fa fa-check"></i><b>16.4</b> Example</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceApartments"><i class="fa fa-check"></i><b>16.4.1</b> Apartment prices</a></li>
<li class="chapter" data-level="16.4.2" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceTitanic"><i class="fa fa-check"></i><b>16.4.2</b> Titanic data</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceProsCons"><i class="fa fa-check"></i><b>16.5</b> Pros and cons</a></li>
<li class="chapter" data-level="16.6" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformanceR"><i class="fa fa-check"></i><b>16.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="16.7" data-path="modelPerformance.html"><a href="modelPerformance.html#modelPerformancePython"><i class="fa fa-check"></i><b>16.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="featureImportance.html"><a href="featureImportance.html"><i class="fa fa-check"></i><b>17</b> Variable-importance Measures</a>
<ul>
<li class="chapter" data-level="17.1" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntro"><i class="fa fa-check"></i><b>17.1</b> Introduction</a></li>
<li class="chapter" data-level="17.2" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceIntuition"><i class="fa fa-check"></i><b>17.2</b> Intuition</a></li>
<li class="chapter" data-level="17.3" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceMethod"><i class="fa fa-check"></i><b>17.3</b> Method</a></li>
<li class="chapter" data-level="17.4" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceTitanic"><i class="fa fa-check"></i><b>17.4</b> Example: Titanic data</a></li>
<li class="chapter" data-level="17.5" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceProsCons"><i class="fa fa-check"></i><b>17.5</b> Pros and cons</a></li>
<li class="chapter" data-level="17.6" data-path="featureImportance.html"><a href="featureImportance.html#featureImportanceR"><i class="fa fa-check"></i><b>17.6</b> Code snippets for R</a></li>
<li class="chapter" data-level="17.7" data-path="featureImportance.html"><a href="featureImportance.html#featureImportancePython"><i class="fa fa-check"></i><b>17.7</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html"><i class="fa fa-check"></i><b>18</b> Partial-dependence Profiles</a>
<ul>
<li class="chapter" data-level="18.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntro"><i class="fa fa-check"></i><b>18.1</b> Introduction</a></li>
<li class="chapter" data-level="18.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPIntuition"><i class="fa fa-check"></i><b>18.2</b> Intuition</a></li>
<li class="chapter" data-level="18.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPMethod"><i class="fa fa-check"></i><b>18.3</b> Method</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPs"><i class="fa fa-check"></i><b>18.3.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="18.3.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clusteredPDPs"><i class="fa fa-check"></i><b>18.3.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="18.3.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#groupedPDPs"><i class="fa fa-check"></i><b>18.3.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="18.3.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastivePDPs"><i class="fa fa-check"></i><b>18.3.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPExample"><i class="fa fa-check"></i><b>18.4</b> Example: apartment-prices data</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="18.4.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="18.4.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="18.4.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles"><i class="fa fa-check"></i><b>18.4.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPProsCons"><i class="fa fa-check"></i><b>18.5</b> Pros and cons</a></li>
<li class="chapter" data-level="18.6" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#PDPR"><i class="fa fa-check"></i><b>18.6</b> Code snippets for R</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.1</b> Partial-dependence profiles</a></li>
<li class="chapter" data-level="18.6.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#clustered-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.2</b> Clustered partial-dependence profiles</a></li>
<li class="chapter" data-level="18.6.3" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.3</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="18.6.4" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles-1"><i class="fa fa-check"></i><b>18.6.4</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="featureImportance.html"><a href="featureImportance.html#featureImportancePython"><i class="fa fa-check"></i><b>18.7</b> Code snippets for Python</a>
<ul>
<li class="chapter" data-level="18.7.1" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#grouped-partial-dependence-profiles-2"><i class="fa fa-check"></i><b>18.7.1</b> Grouped partial-dependence profiles</a></li>
<li class="chapter" data-level="18.7.2" data-path="partialDependenceProfiles.html"><a href="partialDependenceProfiles.html#contrastive-partial-dependence-profiles-2"><i class="fa fa-check"></i><b>18.7.2</b> Contrastive partial-dependence profiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html"><i class="fa fa-check"></i><b>19</b> Local-dependence and Accumulated Local Profiles</a>
<ul>
<li class="chapter" data-level="19.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntro"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPIntuition"><i class="fa fa-check"></i><b>19.2</b> Intuition</a></li>
<li class="chapter" data-level="19.3" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPMethod"><i class="fa fa-check"></i><b>19.3</b> Method</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#local-dependence-profile"><i class="fa fa-check"></i><b>19.3.1</b> Local-dependence profile</a></li>
<li class="chapter" data-level="19.3.2" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#accumulated-local-profile"><i class="fa fa-check"></i><b>19.3.2</b> Accumulated local profile</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#CDPExample"><i class="fa fa-check"></i><b>19.4</b> Example: apartment-prices data</a></li>
<li class="chapter" data-level="19.5" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPProsCons"><i class="fa fa-check"></i><b>19.5</b> Pros and cons</a></li>
<li class="chapter" data-level="19.6" data-path="accumulatedLocalProfiles.html"><a href="accumulatedLocalProfiles.html#ALPR"><i class="fa fa-check"></i><b>19.6</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html"><i class="fa fa-check"></i><b>20</b> Residual Diagnostics</a>
<ul>
<li class="chapter" data-level="20.1" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntroResidualDiagnostic"><i class="fa fa-check"></i><b>20.1</b> Introduction</a></li>
<li class="chapter" data-level="20.2" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#IntuitionResidualDiagnostic"><i class="fa fa-check"></i><b>20.2</b> Intuition</a></li>
<li class="chapter" data-level="20.3" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#MethodResidualDiagnostic"><i class="fa fa-check"></i><b>20.3</b> Method</a></li>
<li class="chapter" data-level="20.4" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ExampleResidualDiagnostic"><i class="fa fa-check"></i><b>20.4</b> Example: apartment-prices data</a></li>
<li class="chapter" data-level="20.5" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#ProsConsResidualDiagnostic"><i class="fa fa-check"></i><b>20.5</b> Pros and cons</a></li>
<li class="chapter" data-level="20.6" data-path="residualDiagnostic.html"><a href="residualDiagnostic.html#RcodeResidualDiagnostic"><i class="fa fa-check"></i><b>20.6</b> Code snippets for R</a></li>
</ul></li>
<li class="part"><span><b>IV Use-cases</b></span></li>
<li class="chapter" data-level="21" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html"><i class="fa fa-check"></i><b>21</b> FIFA 19</a>
<ul>
<li class="chapter" data-level="21.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAintro"><i class="fa fa-check"></i><b>21.1</b> Introduction</a></li>
<li class="chapter" data-level="21.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAdataprep"><i class="fa fa-check"></i><b>21.2</b> Data preparation</a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r"><i class="fa fa-check"></i><b>21.2.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.2.2" data-path="doItYourselfWithPython.html"><a href="doItYourselfWithPython.html#code-snippets-for-python"><i class="fa fa-check"></i><b>21.2.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAdataunderst"><i class="fa fa-check"></i><b>21.3</b> Data understanding</a></li>
<li class="chapter" data-level="21.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelassembly"><i class="fa fa-check"></i><b>21.4</b> Model assembly</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-1"><i class="fa fa-check"></i><b>21.4.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.4.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-1"><i class="fa fa-check"></i><b>21.4.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelaudit"><i class="fa fa-check"></i><b>21.5</b> Model audit</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-2"><i class="fa fa-check"></i><b>21.5.1</b> Code snippets for R</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAmodelunderst"><i class="fa fa-check"></i><b>21.6</b> Model understanding (dataset-level explanation)</a>
<ul>
<li class="chapter" data-level="21.6.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-3"><i class="fa fa-check"></i><b>21.6.1</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.6.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-2"><i class="fa fa-check"></i><b>21.6.2</b> Code snippets for Python</a></li>
</ul></li>
<li class="chapter" data-level="21.7" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAinstanceunderst"><i class="fa fa-check"></i><b>21.7</b> Instance-level explanation</a>
<ul>
<li class="chapter" data-level="21.7.1" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFALewy"><i class="fa fa-check"></i><b>21.7.1</b> Robert Lewandowski</a></li>
<li class="chapter" data-level="21.7.2" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-r-4"><i class="fa fa-check"></i><b>21.7.2</b> Code snippets for R</a></li>
<li class="chapter" data-level="21.7.3" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#code-snippets-for-python-3"><i class="fa fa-check"></i><b>21.7.3</b> Code snippets for Python</a></li>
<li class="chapter" data-level="21.7.4" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFACR7"><i class="fa fa-check"></i><b>21.7.4</b> CR7</a></li>
<li class="chapter" data-level="21.7.5" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFASzczesny"><i class="fa fa-check"></i><b>21.7.5</b> Wojciech Szczęsny</a></li>
<li class="chapter" data-level="21.7.6" data-path="UseCaseFIFA.html"><a href="UseCaseFIFA.html#FIFAMessi"><i class="fa fa-check"></i><b>21.7.6</b> Lionel Messi</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="session-info.html"><a href="session-info.html"><i class="fa fa-check"></i>Session Info</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/ModelOriented/DALEX" target="blank">DALEX website</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Explanatory Model Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modelDevelopmentProcess" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Model Development</h1>
<div id="MDPIntro" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>In general, we can distinguish between two approaches to statistical modelling: <em>explanatory</em> and <em>predictive</em> <span class="citation">(Leo Breiman <a href="#ref-twoCultures" role="doc-biblioref">2001</a><a href="#ref-twoCultures" role="doc-biblioref">b</a>)</span>, <span class="citation">(Shmueli <a href="#ref-Shmueli2010" role="doc-biblioref">2010</a>)</span>. In <em>explanatory modelling</em>, models are applied for inferential purposes, i.e., to test hypotheses resulting from some theoretical considerations related to the investigated phenomenon (for instance, related to the effect of a particular clinical factor on a probability of a disease). In <em>predictive modelling</em>, models are used for the purpose of predicting the value of a new or future observation (for instance, whether a person has got or will develop a disease). It is important to know what is the intended purpose of modelling, because it has got important consequences for the methods used in the model-development process.</p>
<p>In this book, we focus on predictive modelling. Thus, we present mainly the methods relevant for predictive models. Nevertheless, we also show selected methods used in case of explanatory models, in order to discuss, if relevant, substantive differences between the methods applied to the two approaches to modelling.</p>
<p>Predictive models are created for various purposes. For instance, a team of data scientists may spend months on developing a single model that will be used for scoring risks of transactions in a large financial company. In that case, every aspect of the model is important, as the model will be used on a large scale and will have important long-term consequences for the company. Hence, the model-development process may be lengthy and tedious. On the other hand, if a small pizza-delivery chain may want to develop a simple model to roughly predict the demand for deliveries, the development process may be much shorter and less complicated. In that case, the model may be quickly updated or even discarded, without major consequences.</p>
<p>Irrespectively of the goals of modelling, model-development process involves similar steps. In this chapter, we briefly discuss these steps.</p>
</div>
<div id="MDPprocess" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Model-development process</h2>
<p>Several approaches have been proposed to describe the process of model development. One of the most known general approaches is the Cross-industry Standard Process for Data Mining (CRISP-DM) <span class="citation">(Chapman et al. <a href="#ref-crisp1999" role="doc-biblioref">1999</a>; Wikipedia <a href="#ref-crisp2019wiki" role="doc-biblioref">2019</a>)</span>. Methodologies specific for predictive models have been introduced also by <span class="citation">Grolemund and Wickham (<a href="#ref-r4ds2019" role="doc-biblioref">2019</a>)</span>, <span class="citation">Hall (<a href="#ref-misconceptions2019" role="doc-biblioref">2019</a>)</span>, and <span class="citation">Biecek (<a href="#ref-mdp2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>The common goal of the approaches is to standardize the process. Standardization can help to plan resources needed to develop and maintain a model, and to make sure that no important steps are missed when developing the model.</p>
<p>CRISP-DM is a tool-agnostic procedure. It breaks the model-development process into six phases: <em>business understanding</em>, <em>data understanding</em>, <em>data preparation</em>, <em>modelling</em>, <em>evaluation</em>, and <em>deployment</em>. The phases can be iterated. Note that iterative phases are also considered by <span class="citation">Grolemund and Wickham (<a href="#ref-r4ds2019" role="doc-biblioref">2019</a>)</span> and <span class="citation">Hall (<a href="#ref-misconceptions2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>Figure <a href="modelDevelopmentProcess.html#fig:MDPwashmachine">2.1</a> presents a variant of the iterative process, divided into five steps. Data collection and preparation is needed prior to any modelling. One cannot hope for building a model with a good performance if the data are not of good quality. Once data have been collected, they have to be explored to understand their structure. Subsequently, a model can be selected and fitted to the data. The constructed model should be validated. The three steps: data understanding, model assembly, and model audit, are often iterated to arrive at a point when, for instance, a model with the best predictive performance is obtained. Once the “best” model has been obtained, it can be “delivered,” i.e., implemented in practice after performing tests and developing the necessary documentation.</p>

<div class="figure" style="text-align: center"><span id="fig:MDPwashmachine"></span>
<img src="figure/MDP_washmachine.png" alt="Lifecycle of a predictive model." width="60%" />
<p class="caption">
Figure 2.1: Lifecycle of a predictive model.
</p>
</div>
<p>The Model-development Process (MDP), proposed by <span class="citation">Biecek (<a href="#ref-mdp2019" role="doc-biblioref">2019</a>)</span>, has been motivated by Rational Unified Process for Software Development <span class="citation">(Kruchten <a href="#ref-rup1998" role="doc-biblioref">1998</a>; Jacobson, Booch, and Rumbaugh <a href="#ref-usdp1999" role="doc-biblioref">1999</a>; Boehm <a href="#ref-spiral1988" role="doc-biblioref">1988</a>)</span>. MDP can be seen as an extension of the scheme presented in Figure <a href="modelDevelopmentProcess.html#fig:MDPwashmachine">2.1</a>. MDP recognizes that fact that consecutive iterations are not identical, because the knowledge increases during the process and consecutive iterations are performed with different goals in mind. This is why MDP is presented in Figure <a href="modelDevelopmentProcess.html#fig:mdpGeneral">2.2</a> as an untangled version of Figure <a href="modelDevelopmentProcess.html#fig:MDPwashmachine">2.1</a>. The five phases, present in CRIPSP-DM, are shown in the rows. A single bar in each row represents an amount of resources (for instance, a week-worth workload) that can be devoted to the project at a specific time-point (indicated on the horizontal axis). For a particular phase, resources can be used in different amounts depending on the current stage of the process, as indicated by the height of the bars. The stages are indicated at the top of the diagram in Figure <a href="modelDevelopmentProcess.html#fig:mdpGeneral">2.2</a>: <em>problem formulation</em>, <em>crisp modelling</em>, <em>fine tuning</em>, and <em>maintenance and decomissioning</em>. Problem formulation aims at defining the needs for the model, defining datasets that will be used for training and validation, and deciding which performance measures will be used for the evaluation of the performance of the final model. Crisp modelling focuses on creation of first versions of the model that may provide an idea about, for instance, how complex may the model have to be to yield the desired solution? Fine tuning focuses on improving the initial version(s) of the model and selecting the best one according to the pre-defined metrics. Finally, maintainance and decommissioning aims at monitoring the performance of the model after its implementation. Note that, unlike in CRISP-DM, the diagram in Figure <a href="modelDevelopmentProcess.html#fig:mdpGeneral">2.2</a> indicates that the process may start with some resources being spent not on the data-preparation phase, but on the model-audit one. This is because, at the problem formulation stage, we may have to spend some time on defining the goals and model-performance metrics (that will be used in model benchmarking) before any attempt to collect the data.</p>
<p>Figure <a href="modelDevelopmentProcess.html#fig:mdpGeneral">2.2</a> also indicates that there may be several iterations of the different phases within each stage, as indicated at the bottom of the diagram. For instance, in the crisp-modelling stage, several versions of a model may be prepared in subsequent iterations.</p>
<!---
First iterations are usually focused on *formulation of the problem*. Sometimes the problem is well stated, however it's a rare situation valid maybe only for kaggle competitions. In most real-life problems the problem formulation requires lots of discussions and experiments. Once the problem is defined we can start building first prototypes, first *crisp versions of models*. These initial versions of models are needed to verify if the problem can be solved and how far we are form the solution. Usually we gather more information and go for the next phase, the *fine tuning*. We repeat these iterations until a final version of a model is developed. Then we move to the last phase *maintenance and* (one day) *decommissioning*.  
--->

<div class="figure" style="text-align: center"><span id="fig:mdpGeneral"></span>
<img src="figure/mdp_general.png" alt="Overview of the model-development process. The process is split into five different phases (rows) and four stages (indicated at the top of the diagram). Horizontal axis presents the time from the problem formulation to putting the model into practice (decommissioning). For a particular phase, resources can be used in different amounts depending on the current stage of the process, as indicated by the height of the bars. There may be several iterations of different phases withhin each stage, as indicated at the bottom of the diagram." width="99%" />
<p class="caption">
Figure 2.2: Overview of the model-development process. The process is split into five different phases (rows) and four stages (indicated at the top of the diagram). Horizontal axis presents the time from the problem formulation to putting the model into practice (decommissioning). For a particular phase, resources can be used in different amounts depending on the current stage of the process, as indicated by the height of the bars. There may be several iterations of different phases withhin each stage, as indicated at the bottom of the diagram.
</p>
</div>
<p>Methods presented in this book can be used to get a better understanding the data and the application domain (<em>exploration</em>), obtaining insight into model-based predictions (<em>model explanation</em>), and evaluation of model’s performance (<em>model examination</em>). Thus, refering to MDP in Figure <a href="modelDevelopmentProcess.html#fig:mdpGeneral">2.2</a>, the methods are suitable for data understanding, model assembly, and model audit phases.</p>
<!----
[TOMASZ: MOVED TO THE NEXT CHAPTER]
In this book we present various examples based on three use cases. Two introduced in Chapter \@ref(dataSetsIntro) (binary classification in surviving Titanic sinking and regression in apartments pricing) and one in Chapter \@ref(UseCaseFIFA) (estimation of soccer player value based on its skills). Due to space limitation we do not show the full life cycle of these problems, but we are focused on phases Crisp modelling and Fine tuning.
---->
<p>In the remainder of the chapter we provide a brief overview of the notation that will be used in the book, and the methods commonly used for data exploration, model fitting, and model validation.</p>
</div>
<div id="notation" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Notation</h2>
<p>Methods described in this book have been developed by different authors, who used different mathematical notations.
We have made an attempt at keeping the mathematical notation consistent throughout the entire book. In some cases this may result in formulas with a fairly complex system of indices.</p>
<p>In this section, we provide a general overview of the notation we use. Whenever necessary, parts of the notation will be explained again in subsequent chapters.</p>
<p>We use capital letters like <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> to denote (scalar) random variables. Observed values of these variables are denoted by lower-case letters like <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>. Vectors and matrices are distinguished by underlining the letter. Thus, <span class="math inline">\(\underline{X}\)</span> and <span class="math inline">\(\underline{x}\)</span> denote matrix <span class="math inline">\(X\)</span> and (column) vector <span class="math inline">\(x\)</span>, respectively. Note, however, that in some situations <span class="math inline">\(\underline{X}\)</span> may indicate a vector of (scalar) random variables. We explicitly mention this when needed. Transposition is indicated by the prime, i.e., <span class="math inline">\(\underline{x}&#39;\)</span> is the row vector resulting from transposition of a column vector <span class="math inline">\(\underline{x}\)</span>.</p>
<p>We use notation <span class="math inline">\(E(Y)\)</span> and <span class="math inline">\(Var(Y)\)</span> to denote the expected (mean) value and variance of random variable <span class="math inline">\(Y\)</span>. If needed, we use a subscript to indicate the distribution used to compute the parameters. Thus, for instance, we use
<span class="math display">\[E_{Y|X=x}(Y) = E_{Y|x}(Y) = E_{Y}(Y|X=x) \]</span>
to indicate the conditional mean of <span class="math inline">\(Y\)</span> given that random variable <span class="math inline">\(X\)</span> assumes the value of <span class="math inline">\(x\)</span>.</p>
<p>We assume that the data available for modelling consist of <span class="math inline">\(n\)</span> observations/instances. For the <span class="math inline">\(i\)</span>-th observation, we have got an observed value <span class="math inline">\(y_i\)</span> of a dependent (random) variable <span class="math inline">\(Y\)</span>. We assume that <span class="math inline">\(Y\)</span> is a scalar, i.e., a single number. In case of dependent categorical variable, we usually consider <span class="math inline">\(Y\)</span> to be a binary indicator of observing a particular category.</p>
<p>Additionally, each observation from a dataset is described by <span class="math inline">\(p\)</span> explanatory variables. We refer to the (column) vector of the explanatory variables, describing the <span class="math inline">\(i\)</span>-th observation, by <span class="math inline">\(\underline{x}_i\)</span>. We can thus consider observations as points in a <span class="math inline">\(p\)</span>-dimensional space <span class="math inline">\(\mathcal X \equiv \mathcal R^p\)</span>, with <span class="math inline">\(\underline{x}_i \in \mathcal X\)</span>. We often collect all explanatory-variable data in the <span class="math inline">\(n\times p\)</span> matrix <span class="math inline">\(\underline{X}\)</span> that contains, in the <span class="math inline">\(i\)</span>-th row, vector <span class="math inline">\(\underline{x}&#39;_i\)</span>.</p>
<p>When introducing some of the model-exploration methods, we often consider “an observation of interest,” for which the vector of explanatory variables is denoted by <span class="math inline">\(x_{*}\)</span>. As the observation may not necessarily belong to the analyzed dataset, we use of the asterisk in the subscript. Clearly, <span class="math inline">\(\underline{x}_{*} \in \mathcal X\)</span>.</p>
<p>We refer to the <span class="math inline">\(j\)</span>-th coordinate of vector <span class="math inline">\(\underline{x}\)</span> by using <span class="math inline">\(j\)</span> in superscript. Thus, <span class="math inline">\(\underline{x}_i = ({x}^1_i, \ldots , {x}^p_i)&#39;\)</span>, where <span class="math inline">\({x}^j_i\)</span> denotes the <span class="math inline">\(j\)</span>-th coordinate of vector <span class="math inline">\(\underline{x}_i\)</span> for the <span class="math inline">\(i\)</span>-th observation from the analyzed dataset. If a power (for instance, a square) of <span class="math inline">\({x}^j_i\)</span> is needed, it will be denoted by using parentheses, i.e., <span class="math inline">\(\left({x}^j_i\right)^2\)</span>.</p>
<p>If <span class="math inline">\(\mathcal J\)</span> denotes a subset of indices, then <span class="math inline">\(\underline{x}^{\mathcal J}\)</span> denotes the vector formed by the coordinates of <span class="math inline">\(\underline{x}\)</span> corresponding to the indices included in <span class="math inline">\(\mathcal J\)</span>.</p>
<p>We use <span class="math inline">\(\underline{x}^{-j}\)</span> to refer to a vector that results from removing the <span class="math inline">\(j\)</span>-th coordinate from vector <span class="math inline">\(\underline{x}\)</span>. By <span class="math inline">\(\underline{x}^{j|=z}\)</span>, we denote a vector in which all coordinates are equal to their values in <span class="math inline">\(\underline{x}\)</span>, except of the <span class="math inline">\(j\)</span>-th coordinate, which values is set equal to <span class="math inline">\(z\)</span>. Thus, <span class="math inline">\(\underline{x}^{j|=z} = ({x}^1, \ldots, {x}^{j-1}, z, {x}^{j+1}, \ldots, {x}^p)&#39;\)</span>.</p>
<p>Notation <span class="math inline">\(\underline{X}^{*j}\)</span> is used to denote a matrix with the values as in <span class="math inline">\(\underline{X}\)</span> except of the <span class="math inline">\(j\)</span>-th column, for which elements are permuted.</p>
<p>In this book, a model is a function <span class="math inline">\(f:\mathcal X \rightarrow \mathcal R\)</span> that transforms a point from <span class="math inline">\(\mathcal X\)</span> into a real number. In most cases, the presented methods can be used directly for multi-variate dependent variables; however, we use examples with uni-variate responses to simplify the notation.</p>
<p>Typically, during the model development, we create many competing models. Formally, we shall index models to refer to a specific version fitted to a dataset. However, for the sake of simplicity, we will omit the index when it is not important. For the same reason we ignore in the notation the fact that, in practice, we never know true model coefficients and use the estimated values.</p>
<p>We use the term “model residual” to indicate the difference between the observed value of the dependent variable <span class="math inline">\(Y\)</span> for the <span class="math inline">\(i\)</span>-th observation from a particular dataset and the model’s prediction for the observation:</p>
<p><span class="math display" id="eq:modelResiduals">\[\begin{equation}
r_i = y_i - f(x_i) = y_i - \hat y_i,
\tag{2.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat y_i\)</span> denotes the predicted (or fitted) value of <span class="math inline">\(y_i\)</span>. More information about residuals is provided in Chapter <a href="residualDiagnostic.html#residualDiagnostic">20</a>.</p>
</div>
<div id="dataunderstanding" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Data understanding</h2>
<p>As indicated in Figures <a href="modelDevelopmentProcess.html#fig:MDPwashmachine">2.1</a> and <a href="modelDevelopmentProcess.html#fig:mdpGeneral">2.2</a>, before starting construction of any models, we have got to understand the data. Toward this aim, tools for data exploration such as visualization techniques, tabular summaries, and statistical methods can be used. The choice of the tools depend on the character of variables included in a dataset.</p>
<p>The most known introduction to data exploration is the famous book by <span class="citation">Tukey (<a href="#ref-tukey1977" role="doc-biblioref">1977</a>)</span>. It introduces the (now classical) tools like, for example, box-and-whisker plots or stem-and-leaf plots. Good overviews of techniques for data exploration can also be found in books by <span class="citation">Nolan and Lang (<a href="#ref-Nolan2015" role="doc-biblioref">2015</a>)</span> and <span class="citation">Wickham and Grolemund (<a href="#ref-Wickham2017" role="doc-biblioref">2017</a>)</span>.</p>
<p>In this book, we rely on five visualization techniques for data exploration, schematically presented in Figure <a href="modelDevelopmentProcess.html#fig:UMEPEDA">2.3</a>. Two of them (histogram and empirical cumulative-distribution (ECD) plot) are used to summarize the distribution of a single random (explanatory or dependent) variable; the remaining three (box plot, mosaic plot, box plot, and scatter plot) are used to explore relationship between pairs of variables. Note that histogram can be used to explore the distribution of a continuous or a categorical variable, while the ECD plot and box plot are suitable for continuous variables. Mosaic plot is useful for exploring relationship between two categorical variables, while scatter plot can be applied for two continuous variables. It is worth noting that box plots can also be used for evaluating a relation between a categorical variable and a continuous one, as illustrated in Figure <a href="modelDevelopmentProcess.html#fig:UMEPEDA">2.3</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:UMEPEDA"></span>
<img src="figure/UMEPEDA.png" alt="Selected methods for visual data exploration applied in this book." width="75%" />
<p class="caption">
Figure 2.3: Selected methods for visual data exploration applied in this book.
</p>
</div>
<p>Exploration of data for the dependent variable usually focuses on the question related to the distribution of the variable. For instance, for a continuous variable, questions like approximate normality or symmetry of the distribution are most often of interest, because of the availability of many powerful methods and models that use the normality assumption. In case of asymmetry (skewness), a possibility of a transformation that could make the distirbution approximately symmetric or normal is usually investigated. For a categorical dependent variable, an important question is whether the proportion of observations in different categories is balanced or not. This is because, for instance, some methods related to the classification problem do not work well with if there is a substantial imbalance between the categories.</p>
<p>Exploration of data for explanatory variables may also include investigation of their distribution. This is because the results may reveal, for instance, that there is little variability in the observed values of a variable. As a consequence, the variable might be deemed not interesting from a model-construction point of view. Usually, however, the exploration focuses on the relationship between explanatory variables themselves on one hand, and their relationship with the dependent variable on the other hand. The results may have important consequences for the model construction. For instance, if an explanatory variable does not appear to be related to the dependent variable, it may be dropped from a model (variable selection/filtering). The exploration results may also suggest, for instance, a need for a transformation of an explanatory variable to make its relationship with the dependent variable linear (variable engineering). Detection of pairs of strongly-correlated explanatory variables is also of interest, as it may help in resolving issues with, for instance, instability of optimization algorithms used for fitting of a model.</p>
</div>
<div id="fitting" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Model assembly (fitting)</h2>
<p>In statistical modelling, we are interested in the distribution of a dependent variable <span class="math inline">\(Y\)</span> given <span class="math inline">\(\underline{x}\)</span>, the vector of values of explanatory variables. In the ideal world, we would like to know the entire conditional distribution. In practical applications, however, we usually do not evaluate the entire distribution, but just some of its characteristics, like the expected (mean) value, a quantile, or variance. Without loss of generality we will assume that we model the conditional expected value of <span class="math inline">\(Y\)</span>, i.e., <span class="math inline">\(E_{Y | \underline{x}}(Y)\)</span>.</p>
<p>Assume that we have got model <span class="math inline">\(f()\)</span>, for which <span class="math inline">\(f(\underline{x})\)</span> is an approximation of <span class="math inline">\(E_{Y | \underline{x}}(Y)\)</span>, i.e., <span class="math inline">\(E_{Y | \underline{x}}(Y) \approx f(\underline{x})\)</span>. Note that, in our book, we do not assume that it is a “good” model, nor that the approximation is precise. We simply assume that we have got a model that is used to estimate the conditional expected value and to form predictions of the values of the dependent variable. Our interest lies in the evaluation of the quality of the predictions. If the model offers a “good” approximation of the conditional expected value, it should be reflected in its satisfactory predictive performance.</p>
<p>Usually, when building a model, the available data are split into two parts. One part, often called a “training set” or “learning data,” is used for estimation of the model coefficients. The other part, called a “testing set” or “validation data,” is used for model validation. The splitting may be done repeatedly, as in k-fold cross-validation. We leave the topic of model validation for Chapter <a href="modelPerformance.html#modelPerformance">16</a>.</p>
<p>The process of estimation of model coefficients based on the training data, i.e., “fitting” of the model, differs for different models. In most cases, however, it can be seen as an optimization problem. Let <span class="math inline">\(\Theta\)</span> be the space of all possible values of model coefficients. Model fitting (or training) is a procedure of selecting a value <span class="math inline">\(\hat{\underline{\theta}} \in \Theta\)</span> that minimizes some loss function <span class="math inline">\(L()\)</span>:</p>
<p><span class="math display" id="eq:modelTrainingEq0">\[\begin{equation}
\hat{\underline{\theta}} = \arg \min_{\underline{\theta} \in \Theta}  L\{\underline{y}, f(\underline{\theta};\underline{X})\}, 
\tag{2.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\underline{y}\)</span> is the vector of observed values of the dependent variable and <span class="math inline">\(f(\underline{\theta}; \underline{X})\}\)</span> is the corresponding vector of the model’s predictions computed for model coefficients <span class="math inline">\(\underline{\theta}\)</span> and matrix <span class="math inline">\(\underline{X}\)</span> of values of explanatory variables for the observations from the training dataset. Denote the estimated form of the model by <span class="math inline">\(f(\hat{\underline{\theta}};\underline{X})\)</span>.</p>
<p>Consider predcition of a new observation for which the vector of explanatory variables assumes the value of <span class="math inline">\(\underline{x}_*\)</span>, i.e., <span class="math inline">\(f(\hat{\underline{\theta}};\underline{x}_*)\)</span>. Assume that <span class="math inline">\(E_{Y | \underline{x}_*}(Y) = f(\underline{\theta};\underline{x}_*)\)</span>. It can be shown <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie2009" role="doc-biblioref">2009</a>; Shmueli <a href="#ref-Shmueli2010" role="doc-biblioref">2010</a>)</span> that the expected squared-error of prediction can be expressed as follows:</p>
<p><span class="math display">\[\begin{equation}
E_{(Y,\hat{\underline{\theta}})|\underline{x}_*}\{Y-f(\hat{\underline{\theta}};\underline{x}_*)\}^2=
\phantom{E_{Y|\underline{x}_*}\{Y-f(\underline{\theta};\underline{x}_*)\}^2 + [f(\underline{\theta};\underline{x}_*)-E_{\underline{y}|\underline{x}_*}\{{f}(\hat{\underline{\theta}};\underline{x}_*)\}]^2 + E_{\underline{y}|\underline{x}_*}[{f}(\hat{\underline{\theta}};\underline{x}_*)-E_{\underline{y}|\underline{x}_*}\{{f}(\hat{\underline{\theta}};\underline{x}_*)\}]^2 }
\end{equation}\]</span>
<span class="math display" id="eq:EPE">\[\begin{eqnarray}
&amp;=&amp;E_{Y|\underline{x}_*}\{Y-f(\underline{\theta};\underline{x}_*)\}^2 + [f(\underline{\theta};\underline{x}_*)-E_{\hat{\underline{\theta}}|\underline{x}_*}\{{f}(\hat{\underline{\theta}};\underline{x}_*)\}]^2 + E_{\hat{\underline{\theta}}|\underline{x}_*}[{f}(\hat{\underline{\theta}};\underline{x}_*)-E_{\hat{\underline{\theta}}|\underline{x}_*}\{{f}(\hat{\underline{\theta}};\underline{x}_*)\}]^2\\
&amp;=&amp; Var_{Y|\underline{x}_*}(Y)+Bias^2+Var_{\hat{\underline{\theta}}|\underline{x}_*}\{\hat{f}(\underline{x}_*)\}.   
\tag{2.3} 
\end{eqnarray}\]</span></p>
<p>The first term on the right-hand-side of equation <a href="modelDevelopmentProcess.html#eq:EPE">(2.3)</a> is the variability of <span class="math inline">\(Y\)</span> around its conditonal expected value <span class="math inline">\(f(\underline{\theta};\underline{x}_*)\)</span>. In general, it cannot be reduced. The second term is the squared
difference between the expected value and its estimate, i.e., the squared bias. Bias results from misspecifying the model by, for instance, using a more parsimonious or a simpler model. The third term is the variance of the estimate, due to the fact that we use training data to estimate the model.</p>
<p>The decomposition presented in <a href="modelDevelopmentProcess.html#eq:EPE">(2.3)</a> underlines an important difference between the explanatory and predictive modelling. In the explanatory modelling, the goal is to minimize the bias, as we are interested in obtaining the most accurate representation of the investigated phenomenon and the related theory. In the predictive modelling, the focus is on minimization of the sum of the (squared) bias and the estimation variance, because we are interested in minimization of the prediction error. Thus, sometimes we can accept a certain amount of bias, if it leads to a substantial gain in precision of estimation and, consequently, in a smaller prediction error <span class="citation">(Shmueli <a href="#ref-Shmueli2010" role="doc-biblioref">2010</a>)</span>.</p>
<p>It follows that the choice of the loss function <span class="math inline">\(L()\)</span> in equation <a href="modelDevelopmentProcess.html#eq:modelTrainingEq0">(2.2)</a> may differ for explanatory and predictive modelling. For the former, it is common to assume some family of probability distributions for the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\underline{x}\)</span>. In such case, the loss function <span class="math inline">\(L()\)</span> may be defined as the negative logarithm of the likelihood function, where likelihood is the probability of observing <span class="math inline">\(\underline{y}\)</span>, given <span class="math inline">\(\underline{X}\)</span>, treated as a function of <span class="math inline">\(\underline{\theta}\)</span>. The resulting estimate of <span class="math inline">\(\underline{\theta}\)</span> is usually denoted by <span class="math inline">\(\hat{\underline{\theta}}\)</span>.</p>
<p>In predictive modelling, it is common to add term <span class="math inline">\(\lambda(\underline{\theta})\)</span> to the loss function that “penalizes” for the use of more complex models:</p>
<p><span class="math display" id="eq:modelTrainingEq1">\[\begin{equation}
\tilde{\underline{\theta}} = \arg \min_{\underline{\theta} \in \Theta} \left[L\{\underline{y}, f(\underline{\theta};\underline{X})\} + \lambda(\underline{\theta})\right]. 
\tag{2.4}
\end{equation}\]</span></p>
<p>For example, in linear regression we assume that the observed vector <span class="math inline">\(\underline{y}\)</span> follows a multi-variate Gaussian distribution:</p>
<p><span class="math display">\[
\underline{y} \sim \mathcal N(\underline{X}&#39; \underline{\beta}, \sigma^2\underline{I}_n),
\]</span>
where <span class="math inline">\(\underline{\theta}&#39; = (\underline{\beta}&#39;, \sigma^2)\)</span> and <span class="math inline">\(\underline{I}_n\)</span> denotes the <span class="math inline">\(n \times n\)</span> identity matrix. In this case, equation <a href="modelDevelopmentProcess.html#eq:modelTrainingEq1">(2.4)</a> becomes</p>
<p><span class="math display" id="eq:modelTrainingEq2">\[\begin{equation}
\tilde{\underline{\theta}} = \arg \min_{\underline{\theta} \in \Theta}\left\{  \frac{1}{n}||\underline{y} - \underline{X}&#39; \underline{\beta}||_{2} + \lambda(\underline{\beta}) \right\}= \arg \min_{\underline{\theta} \in \Theta} \left\{  \frac{1}{n}\sum_{i=1}^n (y_i-\underline{x}&#39;_i\underline{\beta})^2+ \lambda(\underline{\beta}) \right\}. 
\tag{2.5}
\end{equation}\]</span></p>
<p>For the classcal linear regression, the penalty term <span class="math inline">\(\lambda(\underline{\beta})\)</span> is equal to <span class="math inline">\(0\)</span>. In that case, the optimal parameters <span class="math inline">\(\hat{\underline{\beta}}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span>, obtained from <a href="modelDevelopmentProcess.html#eq:modelTrainingEq0">(2.2)</a>, can be expressed in a closed form:</p>
<p><span class="math display">\[\begin{eqnarray*}
\hat{\underline{\beta}} &amp;=&amp; (\underline{X}&#39;\underline{X})^{-1}\underline{X}&#39;\underline{y},\\
\hat{\sigma}^2 &amp;=&amp; \frac{1}{n}||\underline{y} - \underline{X}&#39; \hat{\underline{\beta}}||_{2} = \frac{1}{n}\sum_{i=1}^n (y_i-\underline{x}&#39;_i\hat{\underline{\beta}})^2= \frac{1}{n}\sum_{i=1}^n (y_i-\hat{y}_i)^2.
\end{eqnarray*}\]</span></p>
<p>On the other hand, in ridge regression, the penalty function is defined as follows:</p>
<p><span class="math display" id="eq:ridgePenalty">\[\begin{equation}
\lambda(\underline{\beta}) = \lambda \cdot ||\underline{\beta}||_2 = \lambda \sum_{k=1}^p (\beta^k) ^2.
\tag{2.6}
\end{equation}\]</span></p>
<p>In that case, the optimal parameters <span class="math inline">\(\tilde{\underline{\beta}}\)</span> and <span class="math inline">\(\tilde{\sigma}^2\)</span>, obtained from equation <a href="modelDevelopmentProcess.html#eq:modelTrainingEq1">(2.4)</a>, can also be expressed in a closed form:</p>
<p><span class="math display">\[\begin{eqnarray*}
\tilde{\underline{\beta}} &amp;=&amp;  (\underline{X}&#39;\underline{X} + \lambda  \underline{I}_n)^{-1}\underline{X}&#39;\underline{y},\\
\tilde{\sigma}^2 &amp;=&amp; \frac{1}{n}||\underline{y} - \underline{X}&#39; \tilde{\underline{\beta}}||_{2}.
\end{eqnarray*}\]</span></p>
<p>Note that ridge regression leads to non-zero squared-bias in equation <a href="modelDevelopmentProcess.html#eq:EPE">(2.3)</a>, but at the benefit of a reduced estimation variance <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie2009" role="doc-biblioref">2009</a>)</span>.</p>
<p>Another possible form of penalty, used in the Least Absolute Shrinkage and Selection Operator (LASSO) regression, is given by</p>
<p><span class="math display" id="eq:lassoPenalty">\[\begin{equation}
\lambda(\underline{\beta}) = \lambda \cdot ||\underline{\beta}||_1 = \lambda \sum_{k=1}^p |\beta^k|.
\tag{2.7}
\end{equation}\]</span></p>
<p>In that case, <span class="math inline">\(\tilde{\underline{\beta}}\)</span> and <span class="math inline">\(\tilde{\sigma}^2\)</span> have to be estimated by using a numerical optimization procedure.</p>
<p>For a binary dependent variable, i.e., a classification problem, the natural choice for the distribution of <span class="math inline">\(Y\)</span> is the Bernoulli distribution. The resulting loss function, based on the logarithm of the Bernoulli likelihood, is</p>
<p><span class="math display" id="eq:modelTrainingBernoulli">\[\begin{equation}
L(\underline{y},\underline{p})=-\frac{1}{n}\sum_{i=1}^n \{y_i\ln{p_i}+(1-y_i)\ln{(1-p_i)}\},
\tag{2.8}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> is equal to 0 or 1 in case of “no response” and “response” (or “failure” and “success”), and <span class="math inline">\(p_i\)</span> is the probability of <span class="math inline">\(y_i\)</span> being equal to 1. Function <a href="modelDevelopmentProcess.html#eq:modelTrainingBernoulli">(2.8)</a> is often called “log-loss” or “binary cross-entropy” in machine-learning literature.</p>
<p>A popular model for binary data is logistic regression, for which</p>
<p><span class="math display">\[
\ln{\frac{p_i}{1-p_i}}=\underline{x}_i&#39;\underline{\beta}.
\]</span></p>
<p>In that case, the loss function in equation <a href="modelDevelopmentProcess.html#eq:modelTrainingBernoulli">(2.8)</a> becomes equal to</p>
<p><span class="math display" id="eq:modelTrainingLogistic">\[\begin{equation}
L\{\underline{y},f(\underline{\beta},\underline{X})\}=-\frac{1}{n}\sum_{i=1}^n [y_i\underline{x}_i&#39;\underline{\beta}-\ln\{1+\exp(\underline{x}_i&#39;\underline{\beta})\}].
\tag{2.9}
\end{equation}\]</span></p>
<p>Optimal values of parameters <span class="math inline">\(\hat{\underline{\beta}}\)</span>, resulting from equation <a href="modelDevelopmentProcess.html#eq:modelTrainingEq0">(2.2)</a>, have to be found by numerical optimization algorithms.</p>
<p>Of course, one can combine the loss functions in equations <a href="modelDevelopmentProcess.html#eq:modelTrainingBernoulli">(2.8)</a> and <a href="modelDevelopmentProcess.html#eq:modelTrainingLogistic">(2.9)</a> with penalties <a href="modelDevelopmentProcess.html#eq:ridgePenalty">(2.6)</a> or <a href="modelDevelopmentProcess.html#eq:lassoPenalty">(2.7)</a> .</p>
<p>For a categorical dependent variable, i.e., a multilabel classification problem, the natural choice for the distribution of <span class="math inline">\(Y\)</span> is the multinomial distribution. The resulting loss function, in case of <span class="math inline">\(K\)</span> categories, is given by</p>
<p><span class="math display" id="eq:modelTrainingMultinomial">\[\begin{equation}
L(\underline{Y},\underline{P})=-\frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K y_{ik}\ln{p_{ik}},
\tag{2.10}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(y_{ik}=1\)</span> if the <span class="math inline">\(k\)</span>-th category was noted for the <span class="math inline">\(i\)</span>-th observation and 0 otherwise, and <span class="math inline">\(p_{ik}\)</span> is the probability of <span class="math inline">\(y_{ik}\)</span> being equal to 1. Function <a href="modelDevelopmentProcess.html#eq:modelTrainingMultinomial">(2.10)</a> is often called “categorical cross-entropy” in machine-learning literature. Also in this case, optimal parameters <span class="math inline">\(\hat{\underline{\beta}}\)</span>, resulting from equation <a href="modelDevelopmentProcess.html#eq:modelTrainingEq0">(2.2)</a>, have to be found by numerical optimization algorithms.</p>
<!---Apart from linear models for $y$ there is a large variety of predictive models. Find a good overview of different techniques for model development in ,,Modern Applied Statistics with S" [@MASSbook] or ,,Applied predictive modelling" [@Kuhn2013].
--->
</div>
<div id="validation" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Model audit</h2>
<p>As indicated in Figure <a href="modelDevelopmentProcess.html#fig:mdpGeneral">2.2</a>, the modelling process starts with some crisp early versions that are fine tuned in consecutive iterations. To arrive at a final model, we usually have got to evaluate (audit) numerous candidate models that. In this book, we introduce techniques that allow:</p>
<ul>
<li>decomposing a model’s predictions into components that can be attributed to particular explanatory variables (Chapters <a href="breakDown.html#breakDown">7</a>–<a href="LIME.html#LIME">10</a>);</li>
<li>conducting sensitivity analysis for a model’s predictions (Chapter <a href="ceterisParibus.html#ceterisParibus">11</a>–<a href="localDiagnostics.html#localDiagnostics">13</a>);</li>
<li>summarizing the predictive performance of a model (Chapter <a href="modelPerformance.html#modelPerformance">16</a>). In particular, the presented measures are usually used to trace the progress in model development.</li>
<li>assessing the importance of an explanatory variable (Chapter <a href="featureImportance.html#featureImportance">17</a>). The techniques can be helpful in reducing the set of explanatory variables to be included in a model in the fine-tuning stage.</li>
<li>evaluating the effect of an explanatory variable on a model’s predictions (Chapters <a href="partialDependenceProfiles.html#partialDependenceProfiles">18</a>–<a href="accumulatedLocalProfiles.html#accumulatedLocalProfiles">19</a>).</li>
<li>detailed examination of both overall and instance-specific model performance (Chapter <a href="residualDiagnostic.html#residualDiagnostic">20</a>). These are residual-diagnostic tools that can help in identifying potential causes that may lead to issues with model performance.</li>
</ul>
<p>All those techniques can be used to evaluate the current version of a model and to get suggestions for possible improvements. The improvements may be developed and evaluated in the next crisp-modelling or fine-tuning phase.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-mdp2019">
<p>Biecek, Przemyslaw. 2019. “Model Development Process.” <em>CoRR</em> abs/1907.04461. <a href="http://arxiv.org/abs/1907.04461">http://arxiv.org/abs/1907.04461</a>.</p>
</div>
<div id="ref-spiral1988">
<p>Boehm, Barry. 1988. <em>A Spiral Model of Software Development and Enhancement</em>. <em> IEEE Computer, IEEE, 21(5):61-72</em>.</p>
</div>
<div id="ref-twoCultures">
<p>Breiman, Leo. 2001b. “Statistical Modeling: The Two Cultures.” <em>Statistical Science</em> 16 (3): 199–231. <a href="https://doi.org/10.1214/ss/1009213726">https://doi.org/10.1214/ss/1009213726</a>.</p>
</div>
<div id="ref-crisp1999">
<p>Chapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas Reinartz, Colin Shearer, and Rudiger Wirth. 1999. <em>The CRISP-DM 1.0 Step-by-step data mining guide</em>. <a href="ftp://ftp.software.ibm.com/software/analytics/spss/support/Modeler/Documentation/14/UserManual/CRISP-DM.pdf">ftp://ftp.software.ibm.com/software/analytics/spss/support/Modeler/Documentation/14/UserManual/CRISP-DM.pdf</a>.</p>
</div>
<div id="ref-r4ds2019">
<p>Grolemund, Garrett, and Hadley Wickham. 2019. <em>R for Data Science</em>. <a href="https://r4ds.had.co.nz/">https://r4ds.had.co.nz/</a>.</p>
</div>
<div id="ref-misconceptions2019">
<p>Hall, Patrick. 2019. <em>On Explainable Machine Learning Misconceptions and a More Human-Centered Machine Learning</em>. <a href="https://github.com/jphall663/xai_misconceptions/blob/master/xai_misconceptions.pdf">https://github.com/jphall663/xai_misconceptions/blob/master/xai_misconceptions.pdf</a>.</p>
</div>
<div id="ref-Hastie2009">
<p>Hastie, T., R. Tibshirani, and J. Friedman. 2009. <em>The Elements of Statistical Learning. Data Mining, Inference, and Prediction.</em> 2nd ed. New York, NY, USA: Springer.</p>
</div>
<div id="ref-usdp1999">
<p>Jacobson, Ivar, Grady Booch, and James Rumbaugh. 1999. <em>The Unified Software Development Process</em>.</p>
</div>
<div id="ref-rup1998">
<p>Kruchten, Philippe. 1998. <em>The Rational Unified Process</em>.</p>
</div>
<div id="ref-Nolan2015">
<p>Nolan, Deborah, and Duncan Temple Lang. 2015. <em>Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving</em>. Chapman &amp; Hall/CRC.</p>
</div>
<div id="ref-Shmueli2010">
<p>Shmueli, G. 2010. “To explain or to predict?” <em>Statistical Science</em> 25: 289–310.</p>
</div>
<div id="ref-tukey1977">
<p>Tukey, John W. 1977. <em>Exploratory Data Analysis</em>. Addison-Wesley.</p>
</div>
<div id="ref-Wickham2017">
<p>Wickham, Hadley, and Garrett Grolemund. 2017. <em>R for Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. 1st ed. O’Reilly Media, Inc.</p>
</div>
<div id="ref-crisp2019wiki">
<p>Wikipedia. 2019. <em>CRISP DM: Cross-industry standard process for data mining</em>. <a href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining">https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ChapIntroduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="doItYourselfWithR.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ema.pdf", "ema.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
