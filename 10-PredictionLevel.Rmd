# Prediction level explanations {-}

```{r localDALEXsummary, echo=FALSE, fig.cap="(fig:localDALEXsummary) Summary of three approaches to local model exploration and explanation.", out.width = '99%', fig.align='center'}
knitr::include_graphics("figure/DALEX_local.png")
```


# Introduction {#PredictionExplainers}

Prediction level explainers help to understand how the model works for a single prediction. This is the main difference from the model level explainers that were focused on the model as a whole and on model population for whole population. Prediction level explainers work in the context of single observations.

Think about following use-cases

- One wants to attribute effects of variables to a model predictions. Think about model for heart accident Having a final score for a particular patient one wants to understand how much of this score can be attributed to smoking or age or gender. 
- One wants to understand how the model response would change if some inputs are changed. Again, think about model for heart accident How the model response would change if a patient cuts the number of cigarettes per day by half. Or if he introduces a low-carbon diet.
- Model is not working correctly for a particular point and one wants to understand why predictions for this point are wrong. Think about patient that had heart accident but his risk score is very low. One wants to understand which factors may be overlooked.


## Approaches to prediction explanations

There are many different tools that may be used to explore model around a single point $x^*$. Model is a function that takes $p$ dimensional vector as an input. Thus to plot this function we would need $p+1$ dimensions. 

An toy example with $p=2$ is presented in Figure \@ref(fig:cutsSurfaceReady). We will use it as an illustration of key ideas.

```{r cutsSurfaceReady, echo=FALSE, fig.cap="(fig:cutsSurfaceReady) Model response surface. Here the model is a function of two variables.  We are interested in understanding the response of a model in a single point x*", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/cuts_surface_ready_punkt.png")
```

In following sections we will describe the most popular approaches to exploration of such function. They can be divided into three classes. 


* One approach to exploration of model response is to investigate how the model response would change if single variable in the model input would change. This way we may observe profiles seen as a function of a single variable. Such profiles are usually called Ceteris Paribus Profiles. We will present them in detail in the Section \@ref(ceterisParibus).
This is useful for What-If scenarios.  See an example in Figure \@ref(fig:cutsTechnikiReady) panel A. 
* Other approach is to analyze model curvature around point of interest. Again we treat the model as a function and we are interested in the local behavior of this function around the point of interest. We approximate the black-box model with a simpler white-box model around point $x^*$. See an example in Figure \@ref(fig:cutsTechnikiReady) panel B. In the Section \@ref(LIME) we present the LIME method that exploits the concept of local model. 
* Yet another approach is to analyze how the model response in point $x^*$ is different from the average model response. And how the difference can be distributed between model behavior along different dimensions. See an example in Figure \@ref(fig:cutsTechnikiReady) panel C. In the Section \@ref(variableAttributionMethods) we present two methods for variable contributions, sequential conditioning and average conditioning (called also Shapley values). 


```{r cutsTechnikiReady, echo=FALSE, fig.cap="(fig:cutsTechnikiReady) Intuitions behind different approached to prediction level explainers. Panel A presents an idea behind What-If analysis with Ceteris Paribus profiles. Keeping all other variables unchanged we trace model response along changes in a single variable. Panel B presents an idea behind local models like LIME. A simpler white-box model is fitted around the point of interest. It describes the local behaviour of the complex model. Panel C presents an idea behind variable attributions. Additive effects of each variable show how the model response differs from population average.", out.width = '99%', fig.align='center'}
knitr::include_graphics("figure/cuts_techniki_ready.png")
```


## A bit of philosophy: Three Laws for Prediction Level Explanations {#three-single-laws}

76 years ago Isaac Asimov devised [Three Laws of Robotics](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics): 1) a robot may not injure a human being, 2) a robot must obey the orders given it by human beings and 3) A robot must protect its own existence. These laws impact discussion around [Ethics of AI](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence). Today’s robots, like cleaning robots, robotic pets or autonomous cars are far from being conscious enough to be under Asimov’s ethics.

Today we are surrounded by complex predictive algorithms used for decision making. Machine learning models are used in health care, politics, education, judiciary and many other areas. Black box predictive models have far larger influence on our lives than physical robots. Yet, applications of such models are left unregulated despite many examples of their potential harmfulness. See *Weapons of Math Destruction* by Cathy O'Neil [@ONeil] for an excellent overview of selected problems.

It's clear that we need to control algorithms that may affect us. Such control is in our civic rights. Here we propose three requirements that any predictive model should fulfill.

-	**Prediction's justifications**. For every prediction of a model one should be able to understand which variables affect the prediction and how strongly. Variable attribution to final prediction.
-	**Prediction's speculations**. For every prediction of a model one should be able to understand how the model prediction would change if input variables were changed. Hypothesizing about what-if scenarios.
-	**Prediction's validations** For every prediction of a model one should be able to verify how strong are evidences that confirm this particular prediction.

There are two ways to comply with these requirements. 
One is to use only models that fulfill these conditions by design. White-box models like linear regression or decision trees. In many cases the price for transparency is lower performance. 
The other way is to use approximated explainers – techniques that find only approximated answers, but work for any black box model. Here we present such techniques.



