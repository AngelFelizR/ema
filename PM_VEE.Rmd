--- 
title: "Predictive Models: Visualisation, Exploration and Explanation"
author: "Przemyslaw Biecek and Tomasz Burzykowski"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Introduction

Machine Learning (ML) models have a wide range of applications in classification or regression problems. Due to the increasing computational power of computers and complexity of data sources, ML models are becoming more and more sophisticated. Models created with the use of techniques such as boosting or bagging of neural networks are parametrized by thousands of coefficients. They are obscure; it is hard to trace the link between input variables and model outcomes - in fact they are treated as black boxes. They are used because of their elasticity and high performance, but their deficiency in interpretability is one of their weakest sides.

In many applications we need to know, understand or prove how the input variables are used in the model. We need to know the impact of particular variables on the final model predictions. Thus we need tools that extract useful information from thousands of model parameters.

Fully trained models

## Model Lifecycle

![Workflow of a typical machine learning modeling. 
A) Modeling is a process in which domain knowledge and data are turned into models. 
B) Models are used to generate predictions. 
C) Understanding of a model structure may increase our knowledge, and in consequence it may lead to a better model. DALEX helps here.
D) Understanding of drivers behind a particular model’s predictions may help to correct wrong decisions, and in consequence it leads to a better model. DALEX helps here.](figure/mp_understanding.png)



Variable importance

Model response as a function of a variable

Model performance / diagnostic / validation


## Why do we need model explainers?

AutoML

Feature Extraction

Model Improvement

## Black-box models vs White-box models

[@R-DALEX]

## Model agnostic vs Model specific

## Glossary / Notation

feature / variable


Let $f_{M}(x): \mathcal R^{d} \rightarrow \mathcal R$ denote a predictive model, i.e. function that takes $d$ dimensional vector and calculate numerical score. In section in which we work with larger number of models we use subscript $M$ to index models. But to simplify notation, this subscript is omitted if profiles for only one model are considered. 

Symbol $x \in \mathcal R^d$ refers to a point in the feature space. We use subscript $x_i$ to refer to a different data points and superscript $x^j$ to refer to specific dimensions. Additionally, let $x^{-j}$ denote all coordinates except $j$-th and let $x|^j=z$ denote a data point $x^*$ with all coordinates equal to $x$ except coordinate $j$ equal to value $z$. I.e. $\forall_{i \neq {j}} x^i = x^{*,i}$ and $x^j = z$. In other words $x|^j=z$ denote a $x$ with $j$th coordinate changed to $z$.

Now we can define Ceteris Paribus Profile for model $f$, variable $j$ and point $x$ as

$$
CP^{f, j, x}(z) := f(x|^j = z).
$$
I.e. CP profile is a model response obtained for observations created based on $x$ with $j$ coordinated changes and all other coordinates kept unchanged.

It is convenient to use an alternative name for this plot: What-If Plots. CP profiles show what would happen if only a single variable is changed.

Figure 5.1 shows an example of Ceteris Paribus profile. The black dot stands for prediction for a single observation. Grey line show how the model response would change if in this single observation coordinate `surface` will be changes to selected value. From this profile one may read that the model response is non monotonic. If `construction.year` for this observation would be below 1935 the model response would be higher, but if construction year were between 1935 and 1995 the model response would be lower.

## Thanks to {#thanksto}

We are using the **bookdown** package [@R-bookdown] in this sample book

This is chapter \@ref(thanksto) and figure \@ref(fig:nice-fig) and table \@ref(tab:nice-tab).
 
```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(iris, 20), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```




<!--chapter:end:index.Rmd-->

# Prediction level explanations {-}

# Introduction {PredictionExplainers}

Prediction level explainers help to understand how the model works for a single prediction. This is the main difference from the model level explainers that were focused on the model in general. Prediction level explainers are always in context of a single observation.

Think about following use-cases

- One wants to attribute effects of variables to a model predictions. Think about model for hart attack. Having a final score for a patient one wants to understand how much of this score come from smoking or age or gender. 
- One wants to understand how the model response would change if some inputs are changed. Think about model for hart attack. How the model response would change if a patient cuts the number of smoked cigarettes by half.
- Model is not working correctly for a particular point and one wants to understand why predictions for this point are wrong. 

## Variable atribution vs What-if analysis

There are many different tools that may be used to explore model around a single data point and in following sections we will describe the most popular approaches. They can be divided into two classes.

* Analysis of the model curvature. Here we treat the model as a function and we are interested in the curvature of this function around the point of interest (see Figure \@ref(fig:modelResponseCurve)). In section \@ref(LIME) we present the LIME method that approximates the black-box model in a point of interest while in section \@ref(ceterisParibus) we present Ceteris Paribus profiles that are more focused on conditional changes of model response given only one coordinate is modified. 
* Analysis of the probabilistic behavior of the model. Here we are interested in decomposition of the model response to parts that can be attributed to particular features.


```{r modelResponseCurve, echo=FALSE, fig.cap="(fig:modelResponseCurve) Model response surface. We are interested in understanding the model behavior in a single point", out.width = '70%'}
knitr::include_graphics("figure/model_response.png")
```

## When to use? 

There are several use-cases for such explainers. Think about following.

* Model improvement. If model works particular bad for a selected observation (the residual is very high) then investigation of model responses for miss fitted points may give some hints how to improve the model. For individual predictions it is easier to notice that selected variable should have different a effect.
* Additional domain specific validation. Understanding which factors are important for model predictions helps to be critical about model response. If model contributions are against domain knowledge then we may be more skeptical and willing to try another model. On the other hand, if the model response is aligned with domain knowledge we may trust more in these responses. Such trust is important in decisions that may lead to serious consequences like predictive models in medicine.
* Model selection. Having multiple candidate models one may select the final response based on model explanations. Even if one model is better in terms of global model performance it may happen that locally other model is better fitted. This moves us towards model consultations that identify different options and allow human to select one of them. 

## A bit of philosophy {#three-single-laws}

76 years ago Isaac Asimov devised [Three Laws of Robotics](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics): 1) a robot may not injure a human being, 2) a robot must obey the orders given it by human beings and 3) A robot must protect its own existence. These laws impact discussion around [Ethics of AI](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence). Today’s robots, like cleaning robots, robotic pets or autonomous cars are far from being conscious enough to be under Asimov’s ethics.

Today we are surrounded by complex predictive algorithms used for decision making. Machine learning models are used in health care, politics, education, judiciary and many other areas. Black box predictive models have far larger influence on our lives than physical robots. Yet, applications of such models are left unregulated despite many examples of their potential harmfulness. See *Weapons of Math Destruction* by Cathy O'Neil for an excellent overview of potential problems.

It's clear that we need to control algorithms that may affect us. Such control is in our civic rights. Here we propose three requirements that any predictive model should fulfill.

-	**Prediction's justifications**. For every prediction of a model one should be able to understand which variables affect the prediction and how strongly. Variable attribution to final prediction.
-	**Prediction's speculations**. For every prediction of a model one should be able to understand how the model prediction would change if input variables were changed. Hypothesizing about what-if scenarios.
-	**Prediction's validations** For every prediction of a model one should be able to verify how strong are evidences that confirm this particular prediction.

There are two ways to comply with these requirements. 
One is to use only models that fulfill these conditions by design. White-box models like linear regression or decision trees. In many cases the price for transparency is lower performance. 
The other way is to use approximated explainers – techniques that find only approximated answers, but work for any black box model. Here we present such techniques.


## Example: Promoted or Fired?

In this chapter we will use artificial dataset from Human Resources department in a call center to present pros and cons for different techniques of prediction level explainers. At the end of each section there is a collection of examples that shows how to use described techniques in R and Python.

The dataset is available in the `DALEX` package [@R-DALEX]. Each row corresponds to a single employee of a call center. Features like gender, age, average number of working hours per week, grade from the last evaluation and level of salary are used as predictive features.

The problem here is to first build a model, that will determine when to fires and when to promote an employer, so it's a classification problem with three classes. 
But having a model we will use prediction level explainers to better understand how the model works for selected cases.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(HR)
```

In this book we are focused on model exploration rather than model building, thus for sake ok simplicity we will use two default models created with random forest [@R-randomForest] and generalized linear model [@R-nnet].

```{r, warning=FALSE, message=FALSE }
set.seed(59)
library("randomForest")
model_rf <- randomForest(status ~ gender + age + hours + evaluation + salary, data = HR)

library("nnet")
model_glm <- multinom(status ~ gender + age + hours + evaluation + salary, data = HR)
```




# Break Down

[@R-breakDown]

```{r, warning=FALSE, message=FALSE}
library("ceterisParibus")
model = model_rf

pred1 <- function(m, x)   predict(m, x, type = "prob")[,1]
pred2 <- function(m, x)   predict(m, x, type = "prob")[,2]
pred3 <- function(m, x)   predict(m, x, type = "prob")[,3]

explainer_rf_fired <- explain(model, data = HR[,1:5], 
                              y = HR$status == "fired", 
                              predict_function = pred1, label = "fired")
explainer_rf_ok <- explain(model, data = HR[,1:5], 
                              y = HR$status == "ok", 
                              predict_function = pred2, label = "ok")
explainer_rf_promoted <- explain(model, data = HR[,1:5], 
                               y = HR$status == "promoted", 
                               predict_function = pred3, label = "promoted")
```



```{r, warning=FALSE, message=FALSE}
library(breakDown)
```

Variable Atribution

## Interactions

# Shapley Values

Variable Atribution



<!--chapter:end:10-PredictionLevel.Rmd-->

# LIME: Local Interpretable Model-Agnostic Explanations {#LIME}

[@R-lime]
[@R-live]

```{r, warning=FALSE, message=FALSE}
library(lime)
library(live)
```

Sparse model approximation / variable selection / feature ranking

live: Local Interpretable (Model-Agnostic) Visual Explanations 


<!--chapter:end:15-LIME.Rmd-->

# Ceteris Paribus Principle {#ceterisParibus}

In this section we introduce tools based on Ceteris Paribus principle. The main goal for these tools is to help understand how changes in model input affect changes in model output. 

Presented explainers are linked with the second law introduced in Section \@ref(three-single-laws), i.e. law for prediction's speculations. This is why these explainers are also known as *What-If model analysis* or *Individual Conditional EXpectations* [@ICEbox]. It turns out that it is easier to understand how blacx-box model is working if we can play with it by changing variable by variable. 

Think of following usecases:

- Think about a model for hart attack. How the model response would change if a patient cuts the number of smoked cigarettes by half or increase physical activity.
- Think about a model for credit scoring. A customer gets a low score and is asking what he needs to change to increase this score to a certain level, to pass the bank criteria.
- Think about a model for apartment prices. An investor wants to know how much the price may increase if apartment standard is upgraded.



## Introduction

*Ceteris paribus* is a Latin phrase meaning "other things held constant" or "all else unchanged". Using this principle we examine input variable per variable separatly, asumming that effects of all other variables are unchanged. See Figure \@ref(fig:modelResponseCurveLine) 


```{r modelResponseCurveLine, echo=FALSE, fig.cap="(fig:modelResponseCurveLine) A) Model response surface. Ceteris Paribus profiles marked with black curves helps to understand the curvature of the model response by updating only a single variable. B) CP profiles are individual conditional model responses", out.width = '70%', fig.align='center'}
knitr::include_graphics("figure/model_response_line.png")
```

Similar to the LIME method introduced in the section \@ref(LIME), Ceteris Paribus profiles examine curvature of a model response function. The difference between these two methods that LIME approximates the model curvature with a simpler white-box model that is easier to present. Usually the LIME model is sparse, thus our attention may be limited to smaller number of dimensions. In contrary, the CP plots show conditional model response for every variable. In the last subsection we discuss pros and cons of this approach.

## 1D profiles {#ceterisParibus1d}

Let $f_{M}(x): \mathcal R^{d} \rightarrow \mathcal R$ denote a predictive model, i.e. function that takes $d$ dimensional vector and calculate numerical score. 
Symbol $x \in \mathcal R^d$ refers to a point in the feature space. We use subscript $x_i$ to refer to a different data points and superscript $x^j$ to refer to specific dimensions. Additionally, let $x^{-j}$ denote all coordinates except $j$-th and let $x|^j=z$ denote a data point $x^*$ with all coordinates equal to $x$ except coordinate $j$ equal to value $z$. I.e. $\forall_{i \neq {j}} x^i = x^{*,i}$ and $x^j = z$. In other words $x|^j=z$ denote a $x$ with $j$th coordinate changed to $z$.

Now we can define uni-dimensional Ceteris Paribus Profile for model $f$, variable $j$ and point $x$ as

$$
CP^{f, j, x}(z) := f(x|^j = z).
$$
I.e. CP profile is a model response obtained for observations created based on $x$ with coordinate $j$ changed and all other coordinates kept unchanged.

A natural way to visualise CP profiles is to use a profile plot as in Figure \@ref(fig:HRCPFiredHours).



Figure \@ref(fig:HRCPFiredHours) shows an example of Ceteris Paribus profile. The black dot stands for prediction for a single observation. Grey line show how the model response would change if in this single observation coordinate `hours` will be changed to selected value. One thing that we can read is that the model response is not smooth and there is some variability along the profile. Second thing is that for this particular observation the model response would drop significantly if the variable `hours` will be higher than 45.

```{r HRCPFiredHours, echo=FALSE, fig.cap="(fig:HRCPHiredHours) Ceteris Paribus profile for Random Forest model that assess the probability of being fired in call center as a function of average number of working hours", out.width = '50%', fig.align='center'}
knitr::include_graphics("figure/HR_cp_fired_hours.png")
```

Since in the example dataset we are struggling with model for three classes, one can plot CP profiles for each class in the same panel. See an example in the Figure  \@ref(fig:HRCPAllHours).

```{r HRCPAllHours, echo=FALSE, fig.cap="(fig:HRCPAllHours) Ceteris Paribus profiles for three classess predicted by the Random Forest model as a function of average number of working hours", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/HR_cp_all_hours.png")
```

Usually model input consist many variables, then it is beneficial to show more variables at the same time. The easiest way to do so is to plot consecutive variables on separate panels. See an example in Figure \@ref(fig:HRCPFiredAll).

```{r HRCPFiredAll, echo=FALSE, fig.cap="(fig:HRCPFiredAll) Ceteris Paribus profiles for all continuous variables", out.width = '70%', fig.align='center'}
knitr::include_graphics("figure/HR_cp_fired_all.png")
```


## Profile oscillations {#oscillations}

Visual examination of variables is insightful, but for large number of variables we end up with large number of panels, most of which are flat.
This is why we want to asses variable importance and show only profiles for important variables. The advantage of CP profiles is that they lead to a very natural and intuitive way of assessing the variable importance for a single prediction. The intuition is: the more important variable the larger are changes along the CP profile. If variable is not important then model response will barely change. If variable is important the CP profile change a lot for different values of a variable. 

Let's write it down in a more formal way.

Let $vip^{CP}_j(x)$ denotes variable importance calculated based on CP profiles in point $x$ for variable $j$.

$$
vip^{CP}_j(x) = \int_{-\inf}^{inf} |CP^{f,j,x}(z) - f(x)| dz
$$

So it's an absolute deviation from $f(x)$. Note that one can consider different modification of this coefficient:

1. Deviations can be calculated not as a distance from $f(x)$ but from average $\bar CP^{f,j,x}(z)$. 
2. The integral may be weighted based on the density of variable $x^j$. 
3. Instead of absolute deviations one may use root from average squares.

TODO: we need to verify which approach is better. Anna Kozak is working on this

The straightforward estimator for $vip^{CP}_j(x)$ is


$$
\widehat{ vip^{CP}_j(x)} = \frac 1n \sum_{i=1}^n |CP^{f,j,x}(x_i) - f(x)|.
$$

Figure \@ref(fig:CPVIPprofiles) shows the idea behind measuring oscillations. The larger the highlighted area the more important is the variable.

```{r CPVIPprofiles, echo=FALSE, fig.cap="(fig:CPVIPprofiles) CP oscillations are average deviations between CP profiles and the model response", out.width = '50%', fig.align='center'}
knitr::include_graphics("figure/CP_VIP_profiles.png")
```

Figure \@ref(fig:CPVIP1) summarizes variable oscillations. Such visuals help to quickly grasp how large are model oscillations around a specific point.

```{r CPVIP1, echo=FALSE, fig.cap="(fig:CPVIP1) Variable importance plots calculated for Ceteris Paribus profiles for observation ID: 1001", out.width = '40%', fig.align='center'}
knitr::include_graphics("figure/cp_vip_1.png")
```


**NOTE**

Variable importance for single prediction may be very different than variable importance for the full model. 

For example, consider a model 
$$
f(x_1, x_2) = x_1 * x_2
$$
where variables $x_1$ and $x_2$ takes values in $[0,1]$. 

From the global perspective both variables are equally important. 

But local variable importance is very different. Around point $x = (0, 1)$ the importance of $x_1$ is much larger than $x_2$. This is because profile for $f(z, 1)$ have larger oscillations than $f(0, z)$.


## 2D profiles

The definition of ceteris paribus profiles given in section \@ref(ceterisParibus1d) may be easily extended to two and more variables. Also definition of CP oscillations \@ref(oscillations) have straight forward generalization for larger number of dimensions. Such generalisations are usefull when model is non additive. Presence of pairwise interactions may be detected with 2D Ceteris Paribus plots.

Let's define two-dimensional Ceteris Paribus Profile for model $f$, variables $j$ and $k$ and point $x$ as

$$
CP^{f, (j,k), x}(z_1, z_2) := f(x|^{(j,k)} = (z_1,z_2)).
$$
I.e. CP profile is a model response obtained for observations created based on $x$ with $j$ and $k$ coordinates changed to $(z_1, z_2)$ and all other coordinates kept unchanged.

A natural way to visualise 2D CP profiles is to use a level plot as in Figure \@ref(fig:CP2Dsurflor).

```{r CP2Dsurflor, echo=FALSE, fig.cap="(fig:CP2Dsurflor) Ceteris Paribus plot for a pair of variales. Black cross marks coordinated for the observation of interest. Presented model estimates price of an appartment", out.width = '60%', fig.align='center'}
knitr::include_graphics("figure/cp_2d_surf_floor.png")
```

If number of variables is small or moderate thein it is possible to present all pairs of variables. See an example in Figure \@ref(fig:CP2Dall).


```{r CP2Dall, echo=FALSE, fig.cap="(fig:CP2Dall) Ceteris Paribus plot for all pairs of variales.", out.width = '90%', fig.align='center'}
knitr::include_graphics("figure/cp_2d_all.png")
```


# Local model fidelity

 Local fit


## Pros and cons

Ceteris Paribus principle gives a uniform and extendable approach to model exploration. Below we summarize key strengths and weaknesses of this approach. 

**Pros**

- Graphical representation of Ceteris Paribus profile is easy to understand.
- Ceteris Paribus profiles are compact and it is easy to fit many models or many variables in a small space.
- Ceteris Paribus profiles helps to understand how model response would change and how stable it is
- Oscillations calculated for CP profiles helps to select the most important variables.
- 2D Ceteris Paribus profiles help to identify pairwise interactions between variables.

**Cons**

- If variables are correlated (like surface and number of rooms) then the '*everything else kept unchanged*' approach leads to unrealistic settings.
- Interactions between variables are not visible in 1D plots.
- This tool is not suited for very wide data, like hundreds or thousands of variables.
- Visualization of categorical variables is non trivial.


## Code snippets for R

In this section we present key features of the `ceterisParibus` package for R [@R-ceterisParibus]. This package covers all features presented in this chapter. It is is available on CRAN and GitHub. Find more examples at the website of this package `https://pbiecek.github.io/ceterisParibus/`.

**Model preparation**

In this section we will present examples based on the `apartments` dataset. See section TODO for more details.

```{r, warning=FALSE, message=FALSE}
library("DALEX")
head(apartments)
```

The problem here is to predict average price for square meter for an apartment. Let's build a random forest model with `randomForest` package  [@R-randomForest].

```{r, warning=FALSE, message=FALSE}
library("randomForest")
rf_model <- randomForest(m2.price ~ construction.year + surface + floor +
      no.rooms, data = apartments)
rf_model
```

Model exploration with `ceterisParibus` package is performed in four steps. 

1. Create an explainer - wrapper around model and validation data.

Since all other functions work in a model agnostic fashion, first we need to define a wrapper around the model. Here we are using the `explain()` function from `DALEX` package [@R-DALEX].

```{r, warning=FALSE, message=FALSE}
library("DALEX")
explainer_rf <- explain(rf_model,
      data = apartmentsTest, y = apartmentsTest$m2.price)
explainer_rf
```

2. Define point of interest. Certeris Paribus profiles explore model around a single point.

```{r, warning=FALSE, message=FALSE}
new_apartment <- data.frame(construction.year = 1965, no.rooms = 5, surface = 142, floor = 8)
new_apartment
predict(rf_model, new_apartment)
```

3. Calculate CP explainer

The `ceteris_paribus()` function calculates CP profiles for selected model around selected observation. 

By default CP profiles are calculated for all numerical variables. Use the `variables` argument to select subset of interesting variables.
The result from  `ceteris_paribus()`function is a data frame with model predictions for modified points around the point of interest.

```{r, warning=FALSE, message=FALSE}
cp_rf <- ceteris_paribus(explainer_rf, new_apartment, 
                            variables = c("construction.year", "floor"))
cp_rf
```

4. Plot the CP profile

Generic `plot()` function plot CP profiles. It returns a `ggplot2` object that can be polished if needed. Use additional arguments of this function to select colors and sizes for elements visible in the plot.

```{r, warning=FALSE, message=FALSE}
plot(cp_rf) 
```


One of very useful features of `ceterisParibus` explainers is that profiles for two or more models may be superimposed in a single plot. This helps in model comparisons. 

Let's create a linear model for this dataset and repeat steps 1-3 for the lm model.

```{r, warning=FALSE, message=FALSE}
lm_model <- lm(m2.price ~ construction.year + surface + floor +
      no.rooms, data = apartments)
explainer_lm <- explain(lm_model,
      data = apartmentsTest, y = apartmentsTest$m2.price)
cp_lm <- ceteris_paribus(explainer_lm, new_apartment, 
                            variables = c("construction.year", "floor"))
```

Now we can use function `plot()` to compare both models in a single chart. Additional argument `color = "_label_"` set color as a key for model.

```{r, warning=FALSE, message=FALSE}
plot(cp_rf, cp_lm, color = "_label_")
```

The `calculate_oscillations()` function calculates oscillations for CP profiles.

```{r, warning=FALSE, message=FALSE}
cp_rf_all <- ceteris_paribus(explainer_rf, new_apartment)
co_rf_all <- calculate_oscillations(cp_rf_all)
co_rf_all
plot(co_rf_all)
```

And the `what_if_2d()` function calculates 2D CP profiles.

```{r, warning=FALSE, message=FALSE}
wi_rf_2d <- what_if_2d(explainer_rf, observation = new_apartment, 
                 selected_variables = c("surface","floor", "construction.year"))
plot(wi_rf_2d)
```






















```{r, warning=FALSE, message=FALSE, echo=FALSE, eval=FALSE}
###########
#
# This is not used anymore

wi_rf_2d <- what_if_2d(explainer_rf, observation = new_apartment, grid_points = 201, selected_variables = c("construction.year", "floor"))

wi_rf_2d %>% 
  filter(vname1 == "construction.year", vname2 == "floor") %>%
  select(y_hat, new_x1, new_x2) %>%
  spread(new_x2, y_hat) -> wi_mat
wi_mat <- as.matrix(wi_mat[, -1])
attr(wi_mat, "dimnames") = NULL

wireframe(-wi_mat, shade = TRUE, xlab="construction.year", ylab="floor", zlab="y hat")

wi_mat[100,] = NA
wi_mat[,100] = NA
wireframe(-wi_mat, shade = TRUE, xlab="construction.year", ylab="floor", zlab="y hat")



wi_rf_2d %>% 
  filter(vname1 == "surface", vname2 == "floor") %>%
  select(y_hat, new_x1, new_x2) %>%
  spread(new_x2, y_hat) -> wi_mat
wi_mat <- as.matrix(wi_mat[, -1])
attr(wi_mat, "dimnames") = NULL

#wireframe(wi_ma

library("ceterisParibus")
library("randomForest")
set.seed(59)

model <- randomForest(status ~ gender + age + hours + evaluation + salary, data = HR)
pred1 <- function(m, x)   predict(m, x, type = "prob")[,1]
explainer_rf_fired <- explain(model, data = HR[,1:5],
   y = HR$status == "fired",
   predict_function = pred1, label = "fired")

pred2 <- function(m, x)   predict(m, x, type = "prob")[,2]
explainer_rf_ok <- explain(model, data = HR[,1:5],
   y = HR$status == "ok",
   predict_function = pred2, label = "ok")

pred3 <- function(m, x)   predict(m, x, type = "prob")[,3]
explainer_rf_promoted <- explain(model, data = HR[,1:5],
   y = HR$status == "promoted",
   predict_function = pred3, label = "promoted")


new_emp <- HR[1, ]
new_emp


cp_rf_fired <- ceteris_paribus(explainer_rf_fired, new_emp, y = new_emp$status == "fired", variables = "hours")
cp_rf_ok <- ceteris_paribus(explainer_rf_ok, new_emp, y = new_emp$status == "ok", variables = "hours")
cp_rf_promoted <- ceteris_paribus(explainer_rf_promoted, new_emp, y = new_emp$status == "promoted", variables = "hours")

plot(cp_rf_fired, cp_rf_ok, cp_rf_promoted, show_profiles = TRUE, show_observations = TRUE, show_rugs = FALSE,
               alpha = 1, size_points = 3, color = "_label_", color_points = "black",
     as.gg = TRUE) + theme_bw() +ylab("y hat") + xlab("")

plot(cp_rf_fired, show_profiles = TRUE, show_observations = TRUE, show_rugs = FALSE,
               alpha = 1, size_points = 3, color = "_label_", color_points = "black",
     as.gg = TRUE) + theme_bw() + theme(legend.position = "none") + ylab("y hat") + xlab("")



cp_rf_fired <- ceteris_paribus(explainer_rf_fired, new_emp, y = new_emp$status == "fired")
cp_rf_ok <- ceteris_paribus(explainer_rf_ok, new_emp, y = new_emp$status == "ok")
cp_rf_promoted <- ceteris_paribus(explainer_rf_promoted, new_emp, y = new_emp$status == "promoted")

plot(cp_rf_fired, show_profiles = TRUE, show_observations = TRUE, show_rugs = FALSE,
               alpha = 1, size_points = 3, color = "_label_", color_points = "black",
     as.gg = TRUE) + theme_bw() + theme(legend.position = "none") + ylab("y hat") + xlab("")



wi_rf_2d <- what_if_2d(explainer_rf_fired, observation = new_emp)
wi_rf_2d

library("dplyr")
library("tidyr")
wi_rf_2d %>% 
  filter(vname1 == "age", vname2 == "salary") %>%
  select(y_hat, new_x1, new_x2) %>%
  spread(new_x2, y_hat) -> wi_mat
wi_mat <- as.matrix(wi_mat[, -1])
attr(wi_mat, "dimnames") = NULL

library("lattice")
wireframe(wi_mat, shade = TRUE, xlab="age", ylab="salary", zlab="y hat")


wi_rf_2d %>% 
  filter(vname1 == "age", vname2 == "hours") %>%
  select(y_hat, new_x1, new_x2) %>%
  spread(new_x2, y_hat) -> wi_mat
wi_mat <- as.matrix(wi_mat[, -1])
attr(wi_mat, "dimnames") = NULL

wireframe(wi_mat[101:1,101:1], shade = TRUE, xlab="age", ylab="hours", zlab="y hat")


plot(wi_rf_2d)





# -------------

apartments_rf_model <- randomForest(m2.price ~ construction.year + surface + floor +
      no.rooms + district, data = apartments)

explainer_rf <- explain(apartments_rf_model,
      data = apartmentsTest[,2:6], y = apartmentsTest$m2.price)

new_apartment <- apartmentsTest[100, ]
new_apartment

new_apartment$construction.year = 1965
new_apartment$no.rooms = 5


wi_rf_2d <- what_if_2d(explainer_rf, observation = new_apartment)

wi_rf_2d %>% 
  filter(vname1 == "construction.year", vname2 == "surface") %>%
  select(y_hat, new_x1, new_x2) %>%
  spread(new_x2, y_hat) -> wi_mat
wi_mat <- as.matrix(wi_mat[, -1])
attr(wi_mat, "dimnames") = NULL

wireframe(wi_mat[101:1,101:1], shade = TRUE, xlab="construction.year", ylab="surface", zlab="y hat")


plot(wi_rf_2d)

plot(wi_rf_2d)+ theme_bw() + scale_x_continuous(expand = c(0,0)) + scale_y_continuous(expand = c(0,0)) 


wi_rf_2d <- what_if_2d(explainer_rf, observation = new_apartment, selected_variables = c("surface","floor"))
plot(wi_rf_2d, bins = 0)+ theme_bw() + xlab("surface") + ylab("floor") +scale_x_continuous(expand = c(0,0)) + scale_y_continuous(expand = c(0,0)) 



cp_rf_y1 <- ceteris_paribus(explainer_rf, new_apartment, y = new_apartment$m2.price, variables = c("construction.year", "floor"))
# plot(cp_rf_y1, show_profiles = TRUE, show_observations = TRUE, show_rugs = FALSE,
#                alpha = 1, size_points = 3, color = "_vname_", color_points = "black")
plot(cp_rf_y1, show_profiles = TRUE, show_observations = TRUE, show_rugs = FALSE,
               alpha = 1, size_points = 3, color = "_vname_", color_points = "black",
     as.gg = TRUE) + theme_bw() + 
  theme(legend.position = "none") + scale_y_reverse(name = "y hat") + xlab("")




wi_rf_2d <- what_if_2d(explainer_rf, observation = new_apartment, grid_points = 201, selected_variables = c("construction.year", "floor"))

wi_rf_2d %>% 
  filter(vname1 == "construction.year", vname2 == "floor") %>%
  select(y_hat, new_x1, new_x2) %>%
  spread(new_x2, y_hat) -> wi_mat
wi_mat <- as.matrix(wi_mat[, -1])
attr(wi_mat, "dimnames") = NULL

wireframe(-wi_mat, shade = TRUE, xlab="construction.year", ylab="floor", zlab="y hat")

wi_mat[100,] = NA
wi_mat[,100] = NA
wireframe(-wi_mat, shade = TRUE, xlab="construction.year", ylab="floor", zlab="y hat")



wi_rf_2d %>% 
  filter(vname1 == "surface", vname2 == "floor") %>%
  select(y_hat, new_x1, new_x2) %>%
  spread(new_x2, y_hat) -> wi_mat
wi_mat <- as.matrix(wi_mat[, -1])
attr(wi_mat, "dimnames") = NULL

#wireframe(wi_mat[101:1,101:1], shade = TRUE, xlab="surface", ylab="floor", zlab="y hat")

#wireframe(wi_mat[101:1,101:1], xlab="surface", ylab="floor", zlab="y hat")

#plot(wi_rf_2d, add_raster = FALSE, bins = 5) + theme_dark()


```


<!--chapter:end:17-ceterisParibus.Rmd-->

# Model level explanations {-}

# Introduction

## Example: Price prediction

[@R-DALEX]


In this chapter we show examples for three predictive models trained on `apartments` dataset from the `DALEX` package. Random Forest model (elastic but biased), Support Vector Machines model (large variance on boundaries) and Linear Model (stable but not very elastic). 
Presented examples are for regression (prediction of square meter price), but the CP profiles may be used in the same way for classification.


```{r, warning=FALSE, message=FALSE}
library("DALEX")
# Linear model trained on apartments data
model_lm <- lm(m2.price ~ construction.year + surface + floor + 
                      no.rooms + district, data = apartments)

library("randomForest")
set.seed(59)
# Random Forest model trained on apartments data
model_rf <- randomForest(m2.price ~ construction.year + surface + floor + 
                      no.rooms + district, data = apartments)

library("e1071")
# Support Vector Machinesr model trained on apartments data
model_svm <- svm(m2.price ~ construction.year + surface + floor + 
                         no.rooms + district, data = apartments)
```

For these models we use `DALEX` explainers created with `explain()` function. There exapliners  wrap models, predict functions and validation data.

```{r, warning=FALSE, message=FALSE}
explainer_lm <- explain(model_lm, 
                       data = apartmentsTest[,2:6], y = apartmentsTest$m2.price)
explainer_rf <- explain(model_rf, 
                       data = apartmentsTest[,2:6], y = apartmentsTest$m2.price)
explainer_svm <- explain(model_svm, 
                       data = apartmentsTest[,2:6], y = apartmentsTest$m2.price)
```

Examples presented in this chapter are generated with the `ceterisParibus` package in version `r installed.packages()["ceterisParibus","Version"]`.

```{r, warning=FALSE, message=FALSE}
library("ceterisParibus")
```




# Variable Importance

Feature selection

# Marginal Response

Feature extraction

## Partial Dependency Plots

Accumulated Local Effects (ALE) Plots

[@R-ALEPlot]

```{r, warning=FALSE, message=FALSE}
library(ALEPlot)
```


Interactions - extraction

## Merging Path Plots

[@R-factorMerger]

```{r, warning=FALSE, message=FALSE}
library(factorMerger)
```

# Performance Diagnostic

Model selection

# Residual Diagnostic

Model validation

# Other topics

<!--chapter:end:20-ModelLevel.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:40-References.Rmd-->

