# Break Down for Additive Variable Attributions {#breakDown}

In the Section \@ref(ceterisParibusOscillations) we introduced a method for assessment of local variable importance based on Ceteris Paribus Profiles. But the main disadvantage of this method is that importance scores do not sum up to final model predictions.

In this chapter we introduce Break Down Plots which solve this problem. Note that the described method is also similar to the EXPLAIN algorithm introduced in  [@explainPaper] and implemented in [@explainPackage] package.

## Intuition

For any model we may repeat the intuition presented in the Section \@ref(variableAttributionMethods) to calculate variable contribution as shifts in expected model response after conditioning over consecutive variables.
This intuition is presented in Figure \@ref(fig:BDPrice4). 

Panel A shows distribution of model responses. The row `all data` shows the model response of the validation dataset. The red dot stands for average model response and it is an estimate of expected model response $E [f(x)]$.

Since we want to calculate effects of particular values of selected variables we then condition over these variables in a sequential manner. 
The next row in panel A corresponds to average model prediction for observations with variable `class` fixed to value `1st`. The next for corresponds to average model prediction with variables `class` set to `1st` and `age` set to `0`, and so on. The last row corresponds to model response for $x^*$.

Black lines in the panel A show how prediction for a single point changes after coordinate $i$ is replaced by the $x^*_i$. But finally we are not interested in particular changes, not even in distributions but only in averages - expected model responses.

The most minimal form that shows important information is presented in the panel C.
Positive values are presented with green bars while negative differences are marked with red bar. They sum up to final model prediction, which is denoted by a violet bar in this example.


```{r BDPrice4, echo=FALSE, fig.cap="(fig:BDPrice4) Break Down Plots show how variables move the model prediction from population average to the model prognosis for a single observation. A) The first row shows distribution of model predictions. Next rows show conditional distributions, every row a new variable is added to conditioning. The last row shows model prediction for a single point. Red dots stand for averages. B) Red dots stands for average conditional model response. C) Only variable contributions are presented, i.e. differences between consecutive conditional expectations. ", out.width = '80%', fig.align='center'}
knitr::include_graphics("figure/break_down_distr.png")
```


## Method


Again, as in previous chapter, let $v(f, x^*, i)$ stands for the contribution of variable $x_i$ to prediction of model $f()$ in point $x^*$. 

We expect that such contribution will sum up to the model prediction in a given point (property called *local accuracy*), so
$$
f(x^*) = baseline + \sum_{i=1}^p v(f, x^*, i)
$$
where $baseline$ stands for average model response.

Note that the equation above may be rewritten as

$$
E [f(X)|X_1 = x_1^*, \ldots, X+p = x_p^*] = E[f(X)] + \sum_{i=1}^p v(f, x^*, i)
$$
what leads to quite natural proposition for $v(f, x^*_i, i)$, such as

$$
v(f, x^*_i, i) = E [f(X) | X_1 = x_1^*, \ldots, X_i = x_i^*] - E [f(X) | X_1 = x_1^*, \ldots, X_{i-1} = x_{i-1}^*] 
$$
In other words the contribution of variable $i$ is the difference between expected model response conditioned on first $i$ variables minus the model response conditioned on first $i-1$ variables.

Such proposition fulfills the *local accuracy* condition.

Unfortunately, for non-additive models, variable contributions depend on the ordering of variables. See for example Figure \@ref(fig:ordering). In the first ordering the contribution of variable `age` is calculated as 0.01, while in the second the contribution is calculated as 0.13. Such differences are related to the lack of additivness of the model $f()$. 
 
```{r ordering, echo=FALSE, fig.cap="(fig:ordering) Two different paths between average model prediction and the model prediction for a selected observation. Black dots stand for conditional average, red arrows stands for changes between conditional averages.", out.width = '100%', fig.align='center'}
knitr::include_graphics("figure/ordering.png")
```


There are different attempts to solve the problem with the ordering. 

A. choose an ordering in which variables with largest contributions are first. In this chapter we will describe a heuristic behind this approach.
B. identify interactions that causes difference in attributions for different orderings and show these interactions. In the chapter \@ref(iBreakDown) we will describe a heuristic behind this idea.
C. calculate average across all possible orderings. There is $p!$ possible orderings, be the may quite accurately approximate the average. This approach will be presented in the chapter \@ref(shapley).


So, let's start with approach A. 
The easiest way to solve this problem is to use two-step procedure. In the first step variables are ordered and in the second step the consecutive conditioning is applied to ordered variables.

First step of this algorithm is to determine the order of variables for conditioning. 
It seems to be reasonable to include first variables that are likely to be most important, leaving the noise variables at the end.
This leads to order based on following scores

$$
score(f, x^*, i) = \left| E [f(X)] - E [f(X)|X_i = x^*_i] \right|
$$
Note, that the absolute value is needed as variable contributions can be both positive and negative. 

Once the ordering is determined in the second step variable contributions are calculated as

$$
v(f, x^*_i, i) = E [f(X) | X_{I \cup \{i\}} = x_{I \cup \{i\}}^*] - E [f(X) | X_{I} = x_{I}^*] 
$$
where $I$ is the set of variables that have scores smaller than score for variable $i$.

$$
I = \{j: score(f, x^*, j) < score(f, x^*, i)\}
$$

The time complexity of the first step id $O(p)$ where $p$ is the number of variables and the time complexity of the second step is also $O(p)$.


## Example: Titanic data

PBI: in this section, should we replicate figures and data already presented in Figure fig:BDPrice4 ? 

Old:

Let us consider a random forest model created for HR data. The average model response is $\bar f(x) = 0.385586$. For a selected observation $x^*$ the table below presents scores for particular variables.

|           |  Ei f(X)|    scorei|
|:----------|--------:|---------:|
|hours      | 0.616200|  0.230614|
|salary     | 0.225528|  0.160058|
|evaluation | 0.430994|  0.045408|
|age        | 0.364258|  0.021328|
|gender     | 0.391060|  0.005474|

Once we determine the order we can calculate sequential contributions

|variable         | cumulative| contribution|
|:----------------|-----------:|------------:|
|(Intercept)      |    0.385586|     0.385586|
|* hours = 42     |    0.616200|     0.230614|
|* salary = 2     |    0.400206|    -0.215994|
|* evaluation = 2 |    0.405776|     0.005570|
|* age = 58       |    0.497314|     0.091538|
|* gender = male  |    0.778000|     0.280686|
|final_prognosis  |    0.778000|     0.778000|


## Pros and cons

Break Down approach is model agnostic, can be applied to any predictive model that returns a single number. It leads to additive variable attribution. Below we summarize key strengths and weaknesses of this approach. 


**Pros**

- Break Down Plots are easy to understand and decipher.
- Break Down Plots are compact; many variables may be presented in a small space.
- Break Down Plots are model agnostic yet they reduce to intuitive interpretation for linear Gaussian and generalized models.
- Complexity of Break Down Algorithm is linear in respect to the number of variables.

**Cons**

- If the model is non-additive then showing only additive contributions may be misleading.
- Selection of the ordering based on scores is subjective. Different orderings may lead to different contributions.
- For large number of variables the Break Down Plot may be messy with many variables having small contributions.



## Code snippets for R

In this section we present key features of the `iBreakDown` package for R [@iBreakDownRPackage] which is a part of `DrWhy.AI` universe. This package covers all features presented in this chapter. It is available on CRAN and GitHub. Find more examples at the website of this package `https://modeloriented.github.io/iBreakDown/`.

In this section, we use a random forest classification model developed in the chapter \@ref(TitanicDataset), namely the `titanic_rf_v6` model. It is trained to predict probability of survival from sinking of Titanic. Instance level explanations are calculated for a single observation `henry` - 47 years old passenger that travels 1st class.

`DALEX` explainers for both models and the Henry data are retrieved via `archivist` hooks as listed in Chapter \@ref(ListOfModelsTitanic). 

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library("randomForest")
explain_rf_v6 <- archivist::aread("pbiecek/models/9b971")

library("DALEX")
johny_d <- archivist::aread("pbiecek/models/e3596")
johny_d
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library("randomForest")
library("DALEX")
load("models/explain_rf_v6.rda")
load("models/johny_d.rda")
```

### Basic usage for the `break_down` function

The `iBreakDown::break_down()` function calculates Break Down contributions for a selected model around a selected observation. 

The result from `break_down()` function is a data frame with additive attributions for selected observation.

The simplest use case is to set only the arguments - model explainers and observation of interest.

```{r, warning=FALSE, message=FALSE}
library("iBreakDown")
bd_rf <- break_down(explain_rf_v6,
                 johny_d)
bd_rf
```

The generic `plot()` function creates Break Down plots. 

```{r, warning=FALSE, message=FALSE}
plot(bd_rf) 
```

### Advanced usage for the `break_down` function

The function `break_down()` can take more arguments. The most commonly used are:

* `x` a wrapper over a model created with function `DALEX::explain()`, 
* `new_observation` an observation to be explained is should be a data frame with structure that matches the training data, 
* `order` if specified then it can be a vector of characters (column names) or integers (column indexes) that specify order of variable conditioning. If not specified (default) then a one-step heuristic is used to determine the order, 
* `keep_distributions` logical value. 	
if `TRUE`, then additional diagnostic information is about conditional distributions is stored and can be plotted with the generic `plot()` function.

Let's see these additional arguments in action.

First we will specify order. You can use integer indexes or variable names. Note that the second option is in most cases better because of higher readability.  Additionally, to reduce clutter in the plot we set `max_features = 3` argument in the `plot()` function. 

```{r, warning=FALSE, message=FALSE}
library("iBreakDown")
bd_rf_order <- break_down(explain_rf_v6,
                 johny_d,
                 order = c("class", "age", "gender", "fare", "parch", "sibsp", "embarked"))
plot(bd_rf_order, max_features = 3) 
```

The `plot_distributions = TRUE` argument of `break_down()` function enriches model response with additional information about conditional distribution.

It can be presented after setting `plot_distributions = TRUE` in the `plot()` function. Conditional distributions are presented as vioplots. Red dots stand for conditional average model response. Thin black lines between vioplots correspond to predictions for individual observations. With them we can trace how model predictions change after consecutive conditionings.

```{r, warning=FALSE, message=FALSE}
bd_rf_distr <- break_down(explain_rf_v6,
                 johny_d,
                 order = c("class", "age", "gender", "fare", "parch", "sibsp", "embarked"),
                 keep_distributions = TRUE)
plot(bd_rf_distr, plot_distributions = TRUE) 
```

