# Summary of Instance-level Explainers {#summaryInstanceLevel}

In the first part of the book we introduced a number of techniques for exploration and explanation of model responses around individual instances. In this section we will compare these approaches. Discuss their strengths and weaknesses taking into account different possible applications.

## Number of variables, size of the model input

One of the most importance criteria for selection of model explainer is the size of important variables in the model.

![figure/instanceExplainer.jpg](figure/instanceExplainer.jpg)

### Low to medium number of variables

Small number of features usually means that particular features are interpretable.

In cases in which only few variables influence the model output the most detailed information about the model is presented by Ceteris Paribus profiles. 

In such cases the most common approach to model exploration is to validate which variables are most influential when it comes to model response, for example with Variable Oscillations presented in \@ref(ceterisParibusOscillations) and then present individual variables with individual Ceteris Paribus profiles as presented in \@ref(ceterisParibus).

Examples for such cases: 

* Predictive model for Titanic data, each feature is interpretable, there is less than dozen features.

### Medium to large number of variables

When the number of variables in model input increase it is harder to show each profile for each individual variable.
In most cases medium number of features means that features are still interpretable.

In such cases the most common approach would be to use Break Down (presented in \@ref(breakDown)) or Shapley values (presented in \@ref(shapley)). We can still read if particular feature has positive or negative effect, we can judge the size of the effect. If necessary we can limit the plot only to the most important features.

Examples for such cases: 

* Credit scoring model based on behavioral data, created on 100+ variables. Features are interpretable, but it makes no sense to plot all 100 ceteris paribus plots.


### Large to very large number of variables

When input space is very large, in many cases single input has no interpretation, thus it makes no sense to consider inputs independently.

Think about individual pixels in image processing, or individual characters in text analysis. Both, due to the lack of interpretability of a single variables as well as due to problems with computational complexity, it is not feasible to plot Ceteris Profiles, or Break Down Profiles not calculate Shapley values.

The most common approach is to use LIME method (presented in \@ref(LIME)) that works on context relevant groups of features (aka aspects).


Examples for such cases: 

* Image classification with images as single features.


## Independent / correlated variables

In most methods we assumed that variables are independent.
But it in many datasets we have some correlated variables. Think about examples like number of rooms and surface of an apartment (the larger apartment the larger number of rooms and larger surface). 

For such cases it may have no sense to consider instances with low probability of occurrence, like very large number of rooms for very little apartments.
For this reason techniques that rely on independent permutations or conditioning will lead to biased results. 

Two most common approaches in such cases are

* create new features that are independent (sometimes it's possible due to domain knowledge, sometimes PCA or similar techniques may be useful),
* permute features in blocks in order to keep correlation structures, as it is described in the \@ref(LIME) chapter. 


## Additive / non additive models

In non additive models effect of one variable may depend on values of other variables. 



## Sparse / non sparse explanations






In Chapters \@ref(ceterisParibus)-\@ref(localDiagnostics) we discussed methods related to the concept of feature oriented exploration, aka. Ceteris Paribus Profils. 
Chapters \@ref(breakDown)-\@ref(shapley) show methods that calculate contributions of individual feature to model predictions. 
Chapter \@ref(LIME) presens an alternative approach focused on sparse explanations.
All these chapters were method-oriented. In this chapter we will compare these methods, discuss options how they can be used together and when they serves a different needs.





## Questions in mind




TODO  compare pros and cons of different techniques

TODO: Sparse model approximation / variable selection / feature ranking

TODO comparison of difrerent approach for Johny D

TODO Champion-Challenger explainers

![figure/localExplainers.png](figure/localExplainers.png)

## When to use? 

There are several use-cases for such explainers. Think about following.

* Model improvement. If model works particular bad for a selected observation (the residual is very high) then investigation of model responses for miss fitted points may give some hints how to improve the model. For individual predictions it is easier to notice that selected variable should have different a effect.
* Additional domain specific validation. Understanding which factors are important for model predictions helps to be critical about model response. If model contributions are against domain knowledge then we may be more skeptical and willing to try another model. On the other hand, if the model response is aligned with domain knowledge we may trust more in these responses. Such trust is important in decisions that may lead to serious consequences like predictive models in medicine.
* Model selection. Having multiple candidate models one may select the final response based on model explanations. Even if one model is better in terms of global model performance it may happen that locally other model is better fitted. This moves us towards model consultations that identify different options and allow human to select one of them. 




Enslaving the Algorithm: From a ‘Right to an Explanation’ to a ‘Right to Better Decisions’?
[@Edwards_Veale_2018]




