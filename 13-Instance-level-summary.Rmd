# Summary of Instance-level Explainers {#summaryInstanceLevel}

This part of the book was devoted to tools for exploration, explanation and debugging of predictive models around single prediction/single instance. 

In Chapters \@ref(ceterisParibus)-\@ref(localDiagnostics) we discussed methods related to the concept of feature oriented exploration, aka. Ceteris Paribus Profils. 
Chapters \@ref(breakDown)-\@ref(shapley) show methods that calculate contributions of individual feature to model predictions. 
Chapter \@ref(LIME) presens an alternative approach focused on sparse explanations.
All these chapters were method-oriented. In this chapter we will compare these methods, discuss options how they can be used together and when they serves a different needs.


## Number of features, size of the data





## Questions in mind






TODO  compare pros and cons of different techniques

TODO: Sparse model approximation / variable selection / feature ranking

TODO comparison of difrerent approach for Johny D

TODO Champion-Challenger explainers


## When to use? 

There are several use-cases for such explainers. Think about following.

* Model improvement. If model works particular bad for a selected observation (the residual is very high) then investigation of model responses for miss fitted points may give some hints how to improve the model. For individual predictions it is easier to notice that selected variable should have different a effect.
* Additional domain specific validation. Understanding which factors are important for model predictions helps to be critical about model response. If model contributions are against domain knowledge then we may be more skeptical and willing to try another model. On the other hand, if the model response is aligned with domain knowledge we may trust more in these responses. Such trust is important in decisions that may lead to serious consequences like predictive models in medicine.
* Model selection. Having multiple candidate models one may select the final response based on model explanations. Even if one model is better in terms of global model performance it may happen that locally other model is better fitted. This moves us towards model consultations that identify different options and allow human to select one of them. 




Enslaving the Algorithm: From a ‘Right to an Explanation’ to a ‘Right to Better Decisions’?
[@Edwards_Veale_2018]



