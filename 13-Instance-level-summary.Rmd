# Summary of Instance-level Exploration {#summaryInstanceLevel}

## Introduction {#summaryInstanceLevelIntro}

In the second part of the book, we introduced a number of techniques for exploration and explanation of model predictions for individual instances. Each chapter was devoted to a single technique. In practice, these techniques rarely are used separately. Rather, it may be more informative to combine different insights offered by each technique into a more holistic overview.

Figure \@ref(fig:instanceLevelExplainers) offers a graphical illustration of the idea. The graph includes results of four different instance-level explanation techniques applied to the random-forest model (Section \@ref(model-titanic-rf)) for the Titanic data (Section \@ref(TitanicDataset)). The instance of interest is Johny D, an 8-year-old boy who embarked in Southampton and travelled in the 1st class with no parents nor siblings, and with a ticket costing 72 pounds (Section \@ref(predictions-titanic)). Recall that the goal is to predict the probability of survival of Johny D. 

The plots in the first row of Figure \@ref(fig:instanceLevelExplainers) show results of application of various variable-attiribution and variable-importance methods like break-down (BD) plots (Chapter \@ref(breakDown)), Shapley values (Chapter \@ref(shapley)), and local interpretable model-agnostic explanations (LIME, see Chapter \@ref(LIME)). The results consistently suggest that the most important explanatory variables from a point of view of prediction of the probability of survival for Johny D are *age*, *gender*, *class*, and *fare*. Note, however, that the picture offered by the additive decompositions may not be enturely correct, because *fare* and *class* are correlated, and there may be an interaction between the efects of *age* and *gender*.

The plots in the second row of Figure \@ref(fig:instanceLevelExplainers) show ceteris-paribus (CP) profiles (see Chapter \@ref(ceterisParibus)) for these four most important explanatory variables for Johny D. The profiles suggest that increasing age or changing the travel class to the second class or to "restaurant staff" would decrease the predicted probability of survival. On the other hand, decreasing fare, changing gender to female or changing the travel class to "deck crew" would increase the probability.

The plots in the third row of Figure \@ref(fig:instanceLevelExplainers) summarize univariate distributions of the four explanatory variables. We see, for instance, that the fare of 72 pounds, which was paid for Johny D's ticket, was very high and that there were few children among the passengers of Titanic. 

Figure \@ref(fig:instanceLevelExplainers) nicely illustrates that perspectives offered by the different techniques complement each other and, when combined, allow obtaining a more profound insight into the origins of the model prediction for the instance of interest.   

(ref:instanceLevelExplainersDesc) Results of instance-level-explanation techniques for the random-forest model `titanic_rf_v6` for the Titanic data and passenger Johny D.

```{r instanceLevelExplainers, echo=FALSE, fig.cap='(ref:instanceLevelExplainersDesc)', out.width = '100%', fig.align='center'}
knitr::include_graphics("figure/instance_level.png")
```

<!---
In Chapter \@ref(UseCaseFIFA), we will present an example illustrating how instance-level explanations may be combined with dataset-level explanations.
--->

While combining various techniques for instance-level explanation can provide additional insights, it is worth remembering that the techniques are, indeed, different and their suitability may depend on the problem at hand. This is what we discuss in the reaminder of the chapter. 

## Number of explanatory variables in the model

One of the most important criteria for selection of the exploration and explanation methods is the number of explanatory variables in the model.

### Low to medium number of explanatory variables

A low number of variables usually implies that the particular variables have a very concrete meaning and interpretation. An example are variables used in models for the Titanic data presented in Sections \@ref(model-titanic-lmr) and \@ref(model-titanic-gbm).
 
In such a situation, the most detailed information about the influence of the variables on the model predictions is provided by the CP profiles. In particular, the variables that are most influential for model predictions are selected by considering CP-profile oscillations (see Chapter \@ref(ceterisParibusOscillations)) and then illustrated graphically with the help of individual-variable CP profiles (see Chapter \@ref(ceterisParibus)).

### Medium to large number of explanatory variables

In models with a medium or large number of variables, it is still possible that most (or all) of them are interpretable. An example of such a model is a car-insurance pricing model in which we want to estimate the value of an insurance based on behavioral data that includes 100+ variables about characteristics of the driver and characteristics of the car. 

When the number of explanatory variables increases, it becomes harder to show the CP profile for each individual variable. In such situation, the most common approach is to use BD plots, presented in Chapter \@ref(breakDown), or plots of Shapley values, discussed in Chapter \@ref(shapley). They allow a quick evaluation whether a particular variable has got a positive or negative effect on model's prediction; we can also assess the size of the effect. If necessary, it is possible to limit the plots only to the variables with the largest effects.

### Very large number of explanatory variables

When the number of explanatory variables is very large, it may be difficult to interpret the role of each single variable. An example of such situation are models for processing of images or texts. In that case, explanatory variables may be individual pixels in image processing or individual characters in text analysis. As such, their individual interpretation is limited. Due to additional issues with computational complexity, it is not feasible to use CP profiles, BD plots, nor Shapley values to evaluate influence of individual values on model's predictions. Instead, the most common approach is to use LIME, presented in Chapter \@ref(LIME), which works on the context-relevant groups of variables.

## Correlated explanatory variables

When deriving properties for the methods presented in Part II of this book, we often assumed that explanatory variables are independent. Obviously, this is not always the case. For instance, in the case of the data on apartment prices (see Section \@ref(exploration-apartments)), the number of rooms and surface of an apartment will most likely be positively associated. A similar conclusion can be drawn for the travel class and fare for the Titanic data (see Section \@ref(exploration-titanic)). 

Of course, technically speaking, all the presented methods can be applied also when explanatory variables are correlated. However, in such a case the results may be misleading or unrealistic.

To address the issue, one could consider creating new variables that would be independent. This is sometimes possible using the application-domain knowledge or by usinig suitable statistical techniques like principal-components analysis. An alternative is to construct two-dimensional CP plots (see Section \@ref(CPProsCons)) or permute variables in blocks to preserve the correlation structure of variables when computing Shapley values (see Chapter \@ref(shapley)). [TOMASZ: CORRECT TO LINK THE BLOCK PERMUTATIONS TO SHAP?] 

## Models with interactions

In models with interactions, the effect of one explanatory variable may depend on values of other variables. For example, the probability of survival for Titanic passengers may decrease with age, but the effect may be different for different travel classes. 

In such a case, to explore and explain model's predictions, we have got to consider not individual variables, but sets of variables included in interactions. To identify interactions, we can use iBD plots as described in Chapter \@ref(iBreakDown). To show effects of an interaction, we may use a set of CP profiles. In particular, for the Titanic example, we may use the CP profiles for the *age* variable for instances that differ only in gender. The less parallel are such profiles, the larger the effect of an interaction.

## Sparse explanations

Predictive models may use hundreds of explanatory variables to yield a prediction for a particular instance. However, for a meaningful interpretation and illustration, most of human beings can handle only a very limited (say, less than 10) number of variables. Thus, sparse explanations are of interest. The most common method that is used to construct such explanations is LIME (Chapter \@ref(LIME)). However, constructing a sparse explanation for a complex model is not trivial and may be misleading. Hence, care is needed when applying LIME to very complex models.

## Additional uses of model exploration and explanation 

In the previous chapters of Part II of the book, we focused on the application of the presented methods to exploration and explanation of predictive models. However, the methods can also be used for other purposes:

* *Model improvement*. If a model prediction is particularly bad for a selected observation, then the investigation of the reasons for such a bad performance may provide hints about how to improve the model. In case of instance predictions, it is easier to detect that a selected explanatory variable should have a different effect than the observed one.

* *Additional domain-specific validation*. Understanding which factors are important for model predictions helps in evaluation of the plausibility of the model. If the effects of some explanatory variables on the predictions are observed to be inconsistent with the domain knowledge, this may provide a ground for criticising the model and, eventually, replacing it by another one. On the other hand, if the influence of the variables on model predictions is consistent with prior expectations, the user may become more confident with the model. Such a confidence is fundamental when the model predictions are used as a support for taking decisions that may lead to serious consequences, like in the case of, for example, predictive models in medicine.

* *Model selection*. In case of multiple candidate models, one may use results of the model explanation techniques to select one of the candidates. It is possible that, even if two models are similar in terms of a overall performance, one of them may perform much better locally. Consider the following, highly hypothetical example. Assume that a model is sought to predict whether it will rain on a particular day in a region where it rains on a half of the days. Two models are considered: one which simply predicts that it will rain every other day, and another that predicts that it will rain every day since October till March. Arguably, both models are rather unsophisticated (to say the least), but they both predict that, on average, half of the days will be rainy. However, investigation of the instance predictions (for individual days) may lead to a preference for one of them. 

## Comparison of models (champion-challenger analysis)

The techniques for explaining and exploring models have many applications. One of them is the opportunity to compare models. 

There are situations when we may be interested in the champion-challenger analysis. Let us assume that some institution uses a predictive model, but wants to know if it could get a better model using other modeling techniques. For example, the risk department in a bank may be using logistic regression to assess credit risk. The model may perform satisfactorily and, hence, be considered as the "champion", i.e., the best model in the class of logistic-regression models. However, the department may be interested in checking whether a "challenger", i.e., a more complex model developed by using, for instance, boosting or random trees, will not perfomr better. And if it is performin better, the question of interest is how the challenger differs from the champion?

Another reason why we may want to compare models is because of the modeling process is iterative itself (see \@ref(MDPprocess)). During the process many versions of models are created, often with different structures, and  sometimes with a very similar performance. Comparative analysis allows for better understanding how these models differ from each other.

Below we present an example of a comparative analysis for the random-forest model `titanic_rf_v6` (Section \@ref(model-titanic-rf)), logistic-regression model `titanic_lmr_v6` (Section \@ref(model-titanic-lmr)), boosting model of `titanic_gbm_v6` (Section \@ref(model-titanic-gbm)), and support-vector machine (SVM) model  `titanic_svm_v6` (Section \@ref(model-titanic-svm)). We consider Johny D (see Section \@ref(predictions-titanic)) as the instance of interest. 

Note that the models do importantly differ. The random-forest and boosting models are tree-based, with a stepped response (prediction) curve. They are complex due to a large number of trees used for prediction. The logistic regression and SVM models lead to  continuous and smooth response curves. Their complexity stems from the fact that the logistic-regression model includes spline transformations, while the SVM model uses a non-linear kernel function. The differences result in different predicted values of the probability of surival for Johny D. In particular, the predicted value of the probability is equal to 0.42, 0.77, 0.66, and XXX for the random-forest, logistic-regression, boosting, and SVM model, respectively (see Section \@ref(predictions-titanic)). [TOMASZ: WOULD BE GOOD TO HAVE SVM VALUE IN CHAPTER ON DATA.] 

Figure \@ref(fig:championChallengerSHAP) shows the Shapley values (see Chapter \@ref(shapley)) for the four models for Johny D. For the random-forest, boosting, and logistic-regression models, similar variables are indicated as important: *class*, *age*, and *gender*. For the SVM model, the most important variable is *gender*, followed by *age* and *parch*.

```{r championChallengerSHAP, warning=FALSE, message=FALSE, echo=FALSE, fig.width=10, fig.height=5, fig.cap="Shapley values for four different models for the Titanic data and passenger Johny D.", out.width = '100%', fig.align='center'}
library("iBreakDown")
library("randomForest")
library("gbm")
library("e1071")
library("rms")
library("DALEX")
load("models/models_titanic.rda")
load("models/explain_rf_v6.rda")
explain_titanic_rf <- explain_rf_v6
load("models/johny_d.rda")

set.seed(1313)
sp_rf_johny <- variable_attribution(explain_titanic_rf, johny_d, type = "shap")
sp_gbm_johny <- variable_attribution(explain_titanic_gbm, johny_d, type = "shap")
sp_svm_johny <- variable_attribution(explain_titanic_svm, johny_d, type = "shap")
sp_lmr_johny <- variable_attribution(explain_titanic_lmr, johny_d, type = "shap")

library(patchwork)
scale_y <- scale_y_continuous("", limits = c(-0.16, 0.32))
(plot(sp_rf_johny) + scale_y | plot(sp_gbm_johny) + scale_y) / 
  (plot(sp_svm_johny) + scale_y | plot(sp_lmr_johny) + scale_y) +
  plot_annotation(title = "Shapley values for Johny D", 
                  theme = DALEX::theme_drwhy())

```

As it was mentioned in Chapter \@ref(shapley), Shapley values show additive contributions of explanatory variables to model predictions. However, the values may be misleading if there are interactions. In that case, iBD plots, discussed in Chapter \@ref(iBreakDown), might be more appropriate. Figure \@ref(fig:championChallengerBD) presents the plots for the four models under consideration.

For the SVM model, the most important variable is *gender*, while for the other models the most imprtatn variables are *age* and *class*. Importantly, the iBD plot for the random-forest model includes interaction of *fare* and *class*, while the SVM model includes the interaction of *fare* and *age*.


```{r championChallengerBD, warning=FALSE, message=FALSE, echo=FALSE, fig.width=10, fig.height=5, fig.cap="Interaction break-down plots for four different models for the Titanic data and passenger Johny D.", out.width = '100%', fig.align='center'}
set.seed(1)
bd_rf_johny <- variable_attribution(explain_titanic_rf, johny_d, type = "break_down_interactions")
bd_gbm_johny <- variable_attribution(explain_titanic_gbm, johny_d, type = "break_down_interactions")
bd_svm_johny <- variable_attribution(explain_titanic_svm, johny_d, type = "break_down_interactions")
bd_lmr_johny <- variable_attribution(explain_titanic_lmr, johny_d, type = "break_down_interactions")

library(patchwork)
scale_y <- scale_y_continuous("", limits = c(0,1))
(plot(bd_rf_johny) + scale_y | plot(bd_gbm_johny) + scale_y) / 
  (plot(bd_svm_johny) + scale_y | plot(bd_lmr_johny) + scale_y) +
  plot_annotation(title = "Interaction break-down plots for Johny D", 
                  theme = DALEX::theme_drwhy())
```

Figure \@ref(fig:championChallengerCP) shows CP profiles for *age* and *fare* the the four compared models. For *fare*, the logistic-regression and SVM regression models show little effect. A similar conclusion can be drawn for the boosting model, though for this model the profile shows considerable oscillations. The profile for the random-forest model suggests a decrease in the predicted probability of survival when the fare increases over about 37 pounds.  

For *age*, the CP profile for the SVM model shows, again, little effect. For the other three models, the effect of the variable is substantial, with the predicted probability of survival decreasing with increasing age. The effect is most pronounced for the logistic-regression model.

```{r championChallengerCP, warning=FALSE, message=FALSE, echo=FALSE, fig.width=8, fig.height=5, fig.cap="Ceteris-paribus plots for age and fare for four different models for the Titanic data and passenger Johny D.", out.width = '100%', fig.align='center'}
splits <- list(age = seq(0,70,0.1), fare = seq(0,100,0.2))

cp_rf_johny <- individual_profile(explain_titanic_rf, johny_d, 
                                  variable_splits = splits)
cp_gbm_johny <- individual_profile(explain_titanic_gbm, johny_d, 
                                   variable_splits = splits)
cp_svm_johny <- individual_profile(explain_titanic_svm, johny_d, 
                                   variable_splits = splits)
cp_lmr_johny <- individual_profile(explain_titanic_lmr, johny_d, 
                                   variable_splits = splits)

plot(cp_rf_johny, cp_gbm_johny, cp_svm_johny, cp_lmr_johny, variables = c("age", "fare"),
     color = "_label_") +
  guides(col = guide_legend(nrow = 1)) + scale_color_discrete("Model:") +
  ggtitle("Ceteris-paribus profiles for Johny D")
```
 
[TOMASZ: I THINK WE LACK AN OVERALL CONCLUSON FOR THE COMPARATIVE ANALYSIS. WHICH OF THE FOUR MODELS SEEMS TO BE THE BEST/REASONABLE? WHERE DOES THE DIFFERENCE IN THE PREDICTED PROBABILIT OF SURVIVAL COME FROM?]

<!---
The compilation of the operating profile of the models side-by-side allows for a better understanding of the similarities and differences in the signals that these models have learned.
--->
