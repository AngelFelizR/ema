# Summary of Instance-level Explainers {#summaryInstanceLevel}

In the first part of the book we introduced a number of techniques for exploration and explanation of model responses around individual instances. In this section we will compare these approaches. Discuss their strengths and weaknesses taking into account different possible applications.

## Number of variables, size of the model input

One of the most importance criteria for selection of model explainer is the size of important variables in the model.

![figure/instanceExplainer.jpg](figure/instanceExplainer.jpg)

### Low to medium number of variables

Small number of features usually means that particular features are interpretable.

In cases in which only few variables influence the model output the most detailed information about the model is presented by Ceteris Paribus profiles. 

In such cases the most common approach to model exploration is to validate which variables are most influential when it comes to model response, for example with Variable Oscillations presented in \@ref(ceterisParibusOscillations) and then present individual variables with individual Ceteris Paribus profiles as presented in \@ref(ceterisParibus).

Examples for such cases: 

* Predictive model for Titanic data, each feature is interpretable, there is less than dozen features.

### Medium to large number of variables

When the number of variables in model input increase it is harder to show each profile for each individual variable.
In most cases medium number of features means that features are still interpretable.

In such cases the most common approach would be to use Break Down (presented in \@ref(breakDown)) or Shapley values (presented in \@ref(shapley)). We can still read if particular feature has positive or negative effect, we can judge the size of the effect. If necessary we can limit the plot only to the most important features.

Examples for such cases: 

* Credit scoring model based on behavioral data, created on 100+ variables. Features are interpretable, but it makes no sense to plot all 100 ceteris paribus plots.


### Large to very large number of variables

When input space is very large, in many cases single input has no interpretation, thus it makes no sense to consider inputs independently.

Think about individual pixels in image processing, or individual characters in text analysis. Both, due to the lack of interpretability of a single variables as well as due to problems with computational complexity, it is not feasible to plot Ceteris Profiles, or Break Down Profiles not calculate Shapley values.

The most common approach is to use LIME method (presented in \@ref(LIME)) that works on context relevant groups of features (aka aspects).


Examples for such cases: 

* Image classification with images as single features.


## Independent / correlated variables

In most methods we assumed that variables are independent.
But it in many datasets we have some correlated variables. Think about examples like number of rooms and surface of an apartment (the larger apartment the larger number of rooms and larger surface). 

For such cases it may have no sense to consider instances with low probability of occurrence, like very large number of rooms for very little apartments.
For this reason techniques that rely on independent permutations or conditioning will lead to biased results. 

Two most common approaches in such cases are

* create new features that are independent (sometimes it's possible due to domain knowledge, sometimes PCA or similar techniques may be useful),
* permute features in blocks in order to keep correlation structures, as it is described in the \@ref(LIME) chapter. 


## Additive / non additive models

In non additive models effect of one variable may depend on values of other variables. For example probability of survival on Titanic may decrease with age but the effect may be different for different classes of passengers. So effect of one variable is modulated by another variable.

This means that to understand non-additive models we need to consider not only features independently but also in sets of potential interactions.

For pairwise interactions one can plot 2-dimensional Ceteris Paribus profiles calculated in the same way as introduced in \@ref(ceterisParibus). 

In order to identify interactions in model response one can use Break Down plots with interactions as introduces in \@ref(iBreakDown).

## Sparse / non sparse explanations

Predictive models may use hundreds variables to construct response for an individual instance. Number of variables does not mater since these are just functions and may operate on input of any size.

But humans cannot easily keep too large context in mind. Few variables at max. Easy to understand explanations need to be sparse.

It's kind of tricky problem, looking for sparse explanations for non sparse models. It is easy to trick yourself. 

One can use Variable Oscillations (presented in \@ref(ceterisParibusOscillations)) or Break Down (presented in \@ref(breakDown)) or Shapley values (presented in \@ref(shapley)) to better understand how many features affect model response.

The most common method that produce sparse explanations is LIME (presented in \@ref(LIME)). But be careful to use this method for non sparse models.



## When to use? 

In previous chapters we show how to use model explainers for exploration, explanation and debugging of predictive models.
But number of possible use cases is much larger.
Think about following.

* Model improvement. If model works particular bad for a selected observation (the residual is very high) then investigation of model responses for miss fitted points may give some hints how to improve the model. For individual predictions it is easier to notice that selected variable should have different a effect.

* Additional domain specific validation. Understanding which factors are important for model predictions helps to be critical about model response. If model contributions are against domain knowledge then we may be more skeptical and willing to try another model. On the other hand, if the model response is aligned with domain knowledge we may trust more in these responses. Such trust is important in decisions that may lead to serious consequences like predictive models in medicine.

* Model selection. Having multiple candidate models one may select the final response based on model explanations. Even if one model is better in terms of global model performance it may happen that locally other model is better fitted. This moves us towards model consultations that identify different options and allow human to select one of them. 


