--- 
title: "Predictive Models: Visualisation, Exploration and Explanation"
author: "Przemyslaw Biecek and Tomasz Burzykowski"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Introduction

Machine Learning (ML) models have a wide range of applications in classification or regression problems. Due to the increasing computational power of computers and complexity of data sources, ML models are becoming more and more sophisticated. Models created with the use of techniques such as boosting or bagging of neural networks are parametrized by thousands of coefficients. They are obscure; it is hard to trace the link between input variables and model outcomes - in fact they are treated as black boxes. They are used because of their elasticity and high performance, but their deficiency in interpretability is one of their weakest sides.

In many applications we need to know, understand or prove how the input variables are used in the model. We need to know the impact of particular variables on the final model predictions. Thus we need tools that extract useful information from thousands of model parameters.


**This book is about**

* We present techniques to examine particular predictions from ML models. In this book you will find theory and examples that explains model locally like break down, ceteris paribus, LIME or Shapley.
* We present techniques to examine fully trained ML models as a whole. In this book you will find theory and examples that explains model globally like Partial Dependency Plots, Variable Importance Plots and others.
* We present tools and methods for model comparison.

**This book is NOT about.**

* We do not focus on any specific model. Presented techniques are model agnostic and do not have any assumptions related to model structure.
* We do not focus on the data exploration. There are very good books and techniques related to this, like R for Data Science http://r4ds.had.co.nz/ or TODO
* We do not focus on the process of model building. There are also very good books about this, see An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani http://www-bcf.usc.edu/~gareth/ISL/ or TODO
* We do not focus on particular tools for model building, see Applied Predictive Modeling By Max Kuhn and Kjell Johnson http://appliedpredictivemodeling.com/



## Model Lifecycle

![Workflow of a typical machine learning modeling. 
A) Modeling is a process in which domain knowledge and data are turned into models. 
B) Models are used to generate predictions. 
C) Understanding of a model structure may increase our knowledge, and in consequence it may lead to a better model. DALEX helps here.
D) Understanding of drivers behind a particular model’s predictions may help to correct wrong decisions, and in consequence it leads to a better model. DALEX helps here.](figure/mp_understanding.png)



Variable importance

Model response as a function of a variable

Model performance / diagnostic / validation


## Why do we need model explainers?

Developer perspective:

  AutoML
  
  Feature Extraction
  
  Model Improvement

  Complex mthods are not being adopted quickly because people do not understand them. Model explainers can change this and open new applications

User perspective:

  Justification of model decisions - civic rights
  
  Model debudding / auditing
  
  [@ONeil]

Cathy O'Neil: 
The era of blind faith 
in big data must end

“You don’t see a lot of skepticism,” she says. “The algorithms are like shiny new toys that we can’t resist using. We trust them so much that we project meaning on to them.”
Ultimately algorithms, according to O’Neil, reinforce discrimination and widen inequality, “using people’s fear and trust of mathematics to prevent them from asking questions”.

But as Cathy O’Neil reveals in this urgent and necessary book, the opposite is true. The models being used today are opaque, unregulated, and uncontestable, even when they’re wrong. Most troubling, they reinforce discrimination: If a poor student can’t get a loan because a lending model deems him too risky (by virtue of his zip code), he’s then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a “toxic cocktail for democracy.” Welcome to the dark side of Big Data.




Bias w modelach:

[@2017arXiv171107076L]

Models may be biased. Examples for 
[@2017arXiv171006169T]

We propose a method to audit black-box risk models for potential
bias by using model distillation.
We demonstrate
the methods on four public datasets: COMPAS, Lending Club,
Stop-and-Frisk, and Chicago Police


## How model exploration is different from data exploration?

Similarities:

* Models and data are related to some truth that we want to understand
* We use visuals to quicker understand complex relations


Differences:

* Data comes from some population; we treat them as a random sample. And since there is a sample there is also some randomness
* Models are just functions. Most models are not stochastic and we consider only deterministic models.
* We believe that data is noisy, but there is truth there. Data cannot be bad of misfitted, may be biased. Models can be bad, misfitted, unaccurate



## Black-box models vs White-box models

[@R-DALEX]

## Model agnostic vs Model specific

Tools designed to deal with specific models

* [@R-randomForestExplainer]
* [@R-xgboostExplainer]
* Specific for neural networks

Model agnostic

* DALEX, lime and all other presented in this book

```{r, message=FALSE, warning=FALSE}
library("xgboostExplainer")
```

## Glossary / Notation

model healthcheck


Let $f_{M}(x): \mathcal R^{d} \rightarrow \mathcal R$ denote a predictive model, i.e. function that takes $d$ dimensional vector and calculate numerical score. In section in which we work with larger number of models we use subscript $M$ to index models. But to simplify notation, this subscript is omitted if profiles for only one model are considered. 

Symbol $x \in \mathcal R^d$ refers to a point in the feature space. We use subscript $x_i$ to refer to a different data points and superscript $x^j$ to refer to specific dimensions. Additionally, let $x^{-j}$ denote all coordinates except $j$-th and let $x|^j=z$ denote a data point $x^*$ with all coordinates equal to $x$ except coordinate $j$ equal to value $z$. I.e. $\forall_{i \neq {j}} x^i = x^{*,i}$ and $x^j = z$. In other words $x|^j=z$ denote a $x$ with $j$th coordinate changed to $z$.

Now we can define Ceteris Paribus Profile for model $f$, variable $j$ and point $x$ as

$$
CP^{f, j, x}(z) := f(x|^j = z).
$$
I.e. CP profile is a model response obtained for observations created based on $x$ with $j$ coordinated changes and all other coordinates kept unchanged.

It is convenient to use an alternative name for this plot: What-If Plots. CP profiles show what would happen if only a single variable is changed.

Figure 5.1 shows an example of Ceteris Paribus profile. The black dot stands for prediction for a single observation. Grey line show how the model response would change if in this single observation coordinate `surface` will be changes to selected value. From this profile one may read that the model response is non monotonic. If `construction.year` for this observation would be below 1935 the model response would be higher, but if construction year were between 1935 and 1995 the model response would be lower.

Glossary:

* Black-box model
* White-box model
* Feature
* Variable
* Continuous variable
* Nominal variable
* Model-agnostic / Model specific



## Thanks to {#thanksto}

We are using the **bookdown** package [@R-bookdown] in this sample book

Chris Drake and Janusz Holyst

