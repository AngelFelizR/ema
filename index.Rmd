--- 
title: "Predictive Models: Visualisation, Exploration and Explanation"
subtitle: 'With examples in R and Python'  
author: "Przemyslaw Biecek and Tomasz Burzykowski"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Introduction

Predictive models are used to automatically guess (statisticians would say: estimate) one interesting variable based on other variables. Think about prediction of sales based on historical data, prediction of risk of heart disease based on patient characteristics, prediction of political attitudes based on facebook comments.

Predictive models were constructed through the whole human history. Think about Flooding of the Nile for example. More rigorous approach to model construction may be attributed to the method of least squares, published by Legendre in 1805, and by Gauss in 1809, more than two centuries ago. Number of applications in economy, medicine, biology, agriculture was growing. The term *regression* was coined by Francis Galton
in 1886, initially was referring to biological applications, today is used for various models that predict continues variable. Prediction of nominal variables is called *classification*, and its beginning may be attributed to works of Ronald Fisher in 1936.

During the last century we observed lot of developments in predictive models like linear models, generalized models, regression and classification trees, rule based models and many others. Developments in mathematical foundations of predictive models were boosted by increasing computational power of personal computers and availability of large datasets. So called the era of big data. 

Increasing demand on predictive models favour models that are elastic, able to perform internally some feature engineering and leads to high precision of predictions.
Robust models are now created with ensembles of models. Techniques like bagging, boosting or model stacking gather hundreds or thousands of small model into a one super model. Large deep neural models have over bilion of parameters. 

There is a cost of this progress. Large models are opaque, obscure, they act like black boxes. On one hand, human is unable to understand how thousands of coefficients affect the model response, one the another hand the model itself may be threated as a trade secret. And the worst part of this is that these models are not as good as we wish them to be.

Decieved by model performance, big names of model producer we tend to believe that these black boxes are unerring oracles. The thruth is that they are not. And we have more and more examples that model performance deteriorate with time or is biased in some sense.

An overview of real problems with large black box models may be found in an excellent book of Cathy O'Neil [@ONeil] on in her TED Talk ,,*The era of blind faith 
in big data must end*''. Variouse examples show how models that are opaque and unregulated lead to higher discrimination of inequality. And there is more examples of such problems, see ,,*Google and the flu: how big data will help us make gigantic mistakes*'', ,,*Report: IBM Watson delivered 'unsafe and inaccurate' cancer recommendations*''.

Today the true bottleneck for predictive modelling is not the lack of data, nor lack of computational power, nor lack of elastic models. It's lack of tools for model validation, explorations and explanations of model decisions. 

In this book we present collection of methods that may be used for this purpose. It is a very active area of research and for sure more methods will be developed in this area. However here we present the mind-set, key problems and methods that are used in model exploration.



**This book is about**

* We show how to determine features that affect model response for a selected observation. In this book you will find theory and examples that explains prediction level methods like break down plots, ceteris paribus profiles, Local model approximations or Shapley values.
* We present techniques to examine fully trained Machine Learning models as a whole. In this book you will find theory and examples that explains model globally like Partial Dependency Plots, Variable Importance Plots and others.
* We present charts that are used to present key information in a quick way. 
* We present tools and methods for model comparison.
* We present example code snippets for R and python that show how to use described methods.

**This book is NOT about.**

* We do not focus on any specific model. Presented techniques are model agnostic and do not have any assumptions related to model structure.
* We do not focus on the data exploration. There are very good books and techniques related to this, like R for Data Science http://r4ds.had.co.nz/ or TODO
* We do not focus on the process of model building. There are also very good books about this, see An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani http://www-bcf.usc.edu/~gareth/ISL/ or TODO
* We do not focus on particular tools for model building, see Applied Predictive Modeling By Max Kuhn and Kjell Johnson http://appliedpredictivemodeling.com/


**This book has following structure**

* In the Section 1 we introduce notation and vocabulary that will be used in this book. Same concepts have often different names in statistics and different in machine learning, thus show how to translate between these two worlds. In this section we also set expectations.
* In the part *Prediction level explainers* we present techniques for exploration and explanations single model predictions.
* In the part *Model level explainers* we present techniques for exploration and explanations model as a whole.

Every method for model exploration is described in a separate section. In each such section you will find. 

* Subsection *Introduction*, that explains the goal of the method and the general idea behind this method.
* Subsection *The Algorithm*, that shows mathematical or computational details related to this methods. You can skip this section if you are not interested in details.
* Subsection *Example*, that show an example application of this method with discussion of results.
* Subsection *Pros and Cons*, that summarize pros and cons of this method and also give some guides when to use this method.
* Subsection *Code snippets*, that show how to use this method in R and python. You can skip this section if you are not interested in implementation.



## White-box models vs Black-box models

In this book we focus on black box models, i.e. models with complex structure, high number of coefficients that are hard to trace and understand by humans.

The opposite are white box models, that have structure easy to understand even for non specialist. Two most common classess of white box models are decision or regression trees (see an example in Figure \@ref(fig:BILLCD8)) or models with additive structure, like this model for relative risk.

$$
RelativeRisk = 1 + 3.6 * [Breslow > 2] - 2 * [TILs > 0] 
$$

For white box models it is easy to understand how they are working from the model structure. It may be complicated to build the model, collect necessary data, do model validation, but once the model is derived the understanding is easy.

Understanding of the model structure gives us few benefits 

* For any model prediction we can easily link model prediction with variables
* We see which variables are used in the model and which are not, thus we may question the model, which variable X is not included.
* We may challenge the model against the domain knowledge. As wee see how each variable influence the model prediction we may verify if it's along domain knowledge.


```{r BILLCD8, echo=FALSE, fig.cap="(fig:BILLCD8) Example tree model for melanoma risk", out.width = '50%', fig.align='center'}
knitr::include_graphics("figure/wbBILL8model.png")
```


Note, that being a white box model is not only about the structure, but also about the number of parameters. A classification tree with 100s of nodes is hard to understand, linear model with 100s of parameters is hard to understand.

Things that are natural for white box models are hard for black box models. For complex models we may not understand how and which features influence the model decision, is the model consistent with the domain knowledge. Tools presented in this book help to extract such information even from complex models.


## Model agnostic vs Model specific

Some classes of models attract higher interest than others or are developed for longer period of time. Thus some classes of models are quipped with very good tools for model exploration or visualisation. 
Just to mention some of them:

* Linear models have lots of tools for model diagnostic and validation of model assumptions. Assumptions are defined in a strict way (normality, linear structure, homogenous variance) and can be validated with normality tests or plots (qq plot), diagnostic plots, tests for model structure (RESET test), tools for identification of outliers etc.
* More complex models with additive structure, like proportional hazards models have tools that verifies model assumptions.
* Random Forest model is equipped with out of bag method of evaluation of performance and few tools for measuring variable importance [@R-randomForest]. Some tools were developed to extract information about possible interactions from the model structure  [@R-randomForestExplainer]. Similar tools are developed to other ensembles of trees, like xgboost models [@R-xgboostExplainer].
* Neural Networks have large collection of dedicated explainers that use Layer-wise Relevance Propagation technique [@BachLWRP] or Saliency Maps technique [@SaliencyMaps] or a mixed approach.

List of model classes is much longer, and for every class there is a collection of tools to use. 
But this variety of approaches leads to problems. (1) One cannot easily compare explanations for two models with different structures. 
(2) Every time when a new architecture of new ensemble of models is proposed, we need to look for new methods of model exploration. (3) For some new models we may not have yet any tool for model explanation.

This book is focused on model agnostic techniques. Thus we do not assume anything about model structure. The model will be threated as a black box and the only operation that we will be able to perform is evaluation of a model in a selected point.

Hoever, even if we do not assume anything about the model, we have some assumptions about data. We assume that the model is a function of a form
$$
f: R^p \rightarrow R
$$
i.e. operates on a vector of $p$ values. This assumption is most suited with tabular data, is held for images, text data video and so on. It may not be suited to models with states, or models with memory as there the model output depends not only on model inputs.

Note also that if $p$ is high dimensional then techniques described here may not be enough to fully understand models with such large number of degrees of freedom.


## Why do we need model explainers?


Machine Learning models have a wide range of applications in classification or regression problems. Due to the increasing computational power of computers and complexity of data sources, ML models are becoming more and more sophisticated. Models created with the use of techniques such as boosting or bagging of neural networks are parametrized by thousands of coefficients. They are obscure; it is hard to trace the link between input variables and model outcomes - in fact they are treated as black boxes. They are used because of their elasticity and high performance, but their deficiency in interpretability is one of their weakest sides.

In many applications we need to know, understand or prove how the input variables are used in the model. We need to know the impact of particular variables on the final model predictions. Thus we need tools that extract useful information from thousands of model parameters.

Tools for model exploration and model understanding have many applications. They may be useful during every phase of a model lifecycle.

Below we summaries how such tools will be useful during the model development, model deployment or model maintenance.


```{r modelLifetime, echo=FALSE, fig.cap="(fig:modelLifetime) Example applications of explainers in different phases of model lifetime", out.width = '90%', fig.align='center'}
knitr::include_graphics("figure/modelLifetime.png")
```

**Model development**

Model building or model development is a phase in which one is looking for best available model. 

In the Section \@ref(partialDependence) we present tools for extraction of relations between features and target variable. Such methods may be used for feature engineering (assisted learning in which elastic black box model is used to learn features for the white box model). Learning from ML models may lead to model improvment.

In the Section \@ref(modelComparisons) we present tools that helps to compare models.

In the Section \@ref(modelAuditing) we present tools that help to validate model, audit model residuals, identify potential strange behaviors.

If for some observations we observe lack of fit, then through tools introduced in the Section \@ref(variableAttributionMethods) we may verify which variables do and which do not influence model decisions. This may help to identify some problem in the model fit and in the end will help to correct the model. 

Since the AutoML methods are being more and more popular, model explainers may actually help to understand how the model identified by AutoML method is working.

If we identify cases on which model is not working properly, then model explainers will help in model debugging. See an examples in the Section TODO.


**Model deployment**

Model deployment is a phase in which one wants final use trust in model decisions, understand these decisions and act accordingly. In some areas complex models are not being adopted because people do not understand nor trust them. Model explainers can change this and increase rate of aquisition of new models

Since most people would not trust in recommendations, that they do not understand, the key element here is to increase understanding related to features that affect model decisions. 

In the Section \@ref(variableAttributionMethods) we introduce tools that identify key features that drive model decisions. 

In the Section \@ref(ceterisParibus) we introduce tools for what-if analysis of model decisions.

In some areas there may be leagal expectations or regulations that requires that model predictions are explainable (see the right to explanation). See [@2017arXiv171107076L] or [@2017arXiv171006169T] for example methods that identify bias in the data.

  


**Model maintenance**

Model maintenance is a phase in which one wants to make sure that model is still valid and suited to the new data. Due to concept drift or similar problems that may happen after some time, we need to monitor the model performance.

In the section \@ref(partialDependence) we present tools that may compare how thw model response behaves on the new dataset. This helps to detect flaws in model assumptions and biases in the data.


## How model exploration is different from data exploration?

Exploration and visualization of models is not that known as exploration and visualization of data.
As we will see in following chapters in both cases we may use similar charts and similar way to express ideas. This is because many people are already familiar with techniques for data visualization and when we do model visualization we want to take advantage of this knowledge.

Both in data visualization and in model visualization we use graphical representation to deliver some messages quicker in a form that is easier to digest.
Despite all similarites we need to keep in mind few key differences between these two wrods.

* Data is generated by some unknown phenomena and with data exploration and visualization we want to understand this phenomena. Models are created based on the data but in most cases we do not know how close are these models to unknown phenomena. So we do model exploration to validate if the model is correct. If the model is valid. In most cases we do not validate data if they are correct. So we are more skeptical about models than about data.
* Data comes from some population; we treat data as a random sample, and there is some inherited randomness related to the sampling. On the opposite, models are just functions. Most models are not stochastic (or at least some models are not stochastic) and the randomness (if any) come from the fitting procedure not from the sampling.
* Models may be inaccurate or biased. When we ask question like ,,How the model is working?'' we also ask ,,Is the model accurate, can I trust it, does it behave well, how much I can trust it?''. Similar question related to data do not question the data but question our understating of the data.

To summaries. We may use similar techniques. But they are used to answer different questions.


## Code snippets

TODO: Here we should tell why we present examples for DALEX.
And mention that there are also other functions that can be used.



## Glossary / Notation


Let $f_{M}(x): \mathcal R^{d} \rightarrow \mathcal R$ denote a predictive model, i.e. function that takes $d$ dimensional vector and calculate numerical score. 
Dimenstions of the vector $x$ refer to different variables (aka features).

In sections in which we work with larger number of models we use subscript $M$ to index models. But to simplify notation, this subscript is omitted if profiles for only one model are considered. 

Symbol $x \in \mathcal R^d$ refers to a point in the feature space. We use subscript $x_i$ to refer to a different data points and superscript $x^j$ to refer to specific dimensions. Additionally, let $x^{-j}$ denote all coordinates except $j$-th and let $x|^j=z$ denote a data point $x^*$ with all coordinates equal to $x$ except coordinate $j$ equal to value $z$. I.e. $\forall_{i \neq {j}} x^i = x^{*,i}$ and $x^j = z$. In other words $x|^j=z$ denote a $x$ with $j$th coordinate changed to $z$.


* *Black-box model* is a model with structure that is hard to understand for humans. Usually it refers to the number of model parameters. As different humans may be better in understanding more or less complex models, there is no strict threshold that makes model a black-box. But in practice for most humans this threshold is closed to 10 rather than 100.
* *White-box model*, opposite to Black-box model, is a model that is easy to understand to human. Maybe not for every human. Consider small linear models and small CAR trees as white box models.
* *Feature* or *Variable*, part of the model input space. Without large loss of generality we can assume that one feature is a single dimension in the input space. There are exceptions (among them: polynomials, interactions between variables, nominal variables), but they do not change the intuition.
* *Continuous variable*, a variable that can be presented as a number and the ordering makes some sense (zip codes or phone numbers are not considered as continuous variables). It does not need to be continuous in a mathematical sense. Counting variables (number of floors, steps) counts here as well.
* *Nominal variable*, opposite to *Continuous variables*, finite set of values that will not the threaded as a numeric 



## Acknowledgements {#thanksto}

Authors of the **bookdown** package [@R-bookdown]

Janusz Holyst and the RENOIR project

Chris Drake for hospitality


