--- 
title: "Predictive Models: Visualisation, Exploration and Explanation"
subtitle: 'With examples in R and Python'  
author: "Przemyslaw Biecek and Tomasz Burzykowski"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Introduction

Predictive models are used to automatically guess (statisticians would say: estimate) one interesting variable based on other variables. Think about prediction of sales based on historical data, prediction of risk of heart disease based on patient characteristics, prediction of political attitudes based on facebook comments.

Predictive models were constructed through the whole human history. Think about Flooding of the Nile for example. More rigorous approach to model construction may be attributed to the method of least squares, published by Legendre in 1805, and by Gauss in 1809, more than two centuries ago. Number of applications in economy, medicine, biology, agriculture was growing. The term *regression* was coined by Francis Galton
in 1886, initially was referring to biological applications, today is used for various models that predict continues variable. Prediction of nominal variables is called *classification*, and its beginning may be attributed to works of Ronald Fisher in 1936.

During the last century we observed lot of developments in predictive models like linear models, generalized models, regression and classification trees, rule based models and many others. Developments in mathematical foundations of predictive models were boosted by increasing computational power of personal computers and availability of large datasets. So called the era of big data. 

Increasing demand on predictive models favour models that are elastic, able to perform internally some feature engineering and leads to high precision of predictions.
Robust models are now created with ensembles of models. Techniques like bagging, boosting or model stacking gather hundreds or thousands of small model into a one super model. Large deep neural models have over bilion of parameters. 

There is a cost of this progress. Large models are opaque, obscure, they act like black boxes. On one hand, human is unable to understand how thousands of coefficients affect the model response, one the another hand the model itself may be threated as a trade secret. And the worst part of this is that these models are not as good as we wish them to be.

Decieved by model performance, big names of model producer we tend to believe that these black boxes are unerring oracles. The thruth is that they are not. And we have more and more examples that model performance deteriorate with time or is biased in some sense.

An overview of real problems with large black box models may be found in an excellent book of Cathy O'Neil [@ONeil] on in her TED Talk ,,*The era of blind faith 
in big data must end*''. Variouse examples show how models that are opaque and unregulated lead to higher discrimination of inequality. And there is more examples of such problems, see ,,*Google and the flu: how big data will help us make gigantic mistakes*'', ,,*Report: IBM Watson delivered 'unsafe and inaccurate' cancer recommendations*''.

Today the true bottleneck for predictive modelling is not the lack of data, nor lack of computational power, nor lack of elastic models. It's lack of tools for model validation, explorations and explanations of model decisions. 

In this book we present collection of methods that may be used for this purpose. It is a very active area of research and for sure more methods will be developed in this area. However here we present the mind-set, key problems and methods that are used in model exploration.



**This book is about**

* We show how to determine features that affect model response for a selected observation. In this book you will find theory and examples that explains prediction level methods like break down plots, ceteris paribus profiles, Local model approximations or Shapley values.
* We present techniques to examine fully trained Machine Learning models as a whole. In this book you will find theory and examples that explains model globally like Partial Dependency Plots, Variable Importance Plots and others.
* We present charts that are used to present key information in a quick way. 
* We present tools and methods for model comparison.
* We present example code snippets for R and python that show how to use described methods.

**This book is NOT about.**

* We do not focus on any specific model. Presented techniques are model agnostic and do not have any assumptions related to model structure.
* We do not focus on the data exploration. There are very good books and techniques related to this, like R for Data Science http://r4ds.had.co.nz/ or TODO
* We do not focus on the process of model building. There are also very good books about this, see An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani http://www-bcf.usc.edu/~gareth/ISL/ or TODO
* We do not focus on particular tools for model building, see Applied Predictive Modeling By Max Kuhn and Kjell Johnson http://appliedpredictivemodeling.com/


**This book has following structure**

* In the Section 1 we introduce notation and vocabulary that will be used in this book. Same concepts have often different names in statistics and different in machine learning, thus show how to translate between these two worlds. In this section we also set expectations.
* In the part *Prediction level explainers* we present techniques for exploration and explanations single model predictions.
* In the part *Model level explainers* we present techniques for exploration and explanations model as a whole.

Every method for model exploration is described in a separate section. In each such section you will find. 

* Subsection *Introduction*, that explains the goal of the method and the general idea behind this method.
* Subsection *The Algorithms*, that shows mathematical or computational details related to this methods. You can skip this section if you are not interested in details.
* Subsection *Example*, that show an example application of this method with discussion of results.
* Subsection *Pros and Cons*, that summarize pros and cons of this method and also give some guides when to use this method.
* Subsection *Code snippets*, that show how to use this method in R and python. You can skip this section if you are not interested in implementation.



## Black-box models vs White-box models

[@R-DALEX]


## Model agnostic vs Model specific

Tools designed to deal with specific models

* [@R-randomForestExplainer]
* [@R-xgboostExplainer]
* Specific for neural networks

Model agnostic

* DALEX, lime and all other presented in this book

```{r, message=FALSE, warning=FALSE}
library("xgboostExplainer")
```

## When do we need model explainers?



Developer perspective:

  AutoML
  
  Feature Extraction
  
  Model Improvement

  Complex mthods are not being adopted quickly because people do not understand them. Model explainers can change this and open new applications

User perspective:

  Justification of model decisions - civic rights
  
  Model debudding / auditing
  
  
Bias w modelach:

[@2017arXiv171107076L]

Models may be biased. Examples for 
[@2017arXiv171006169T]

We propose a method to audit black-box risk models for potential
bias by using model distillation.
We demonstrate
the methods on four public datasets: COMPAS, Lending Club,
Stop-and-Frisk, and Chicago Police



## Model Lifecycle



Machine Learning models have a wide range of applications in classification or regression problems. Due to the increasing computational power of computers and complexity of data sources, ML models are becoming more and more sophisticated. Models created with the use of techniques such as boosting or bagging of neural networks are parametrized by thousands of coefficients. They are obscure; it is hard to trace the link between input variables and model outcomes - in fact they are treated as black boxes. They are used because of their elasticity and high performance, but their deficiency in interpretability is one of their weakest sides.

In many applications we need to know, understand or prove how the input variables are used in the model. We need to know the impact of particular variables on the final model predictions. Thus we need tools that extract useful information from thousands of model parameters.

![Workflow of a typical machine learning modeling. 
A) Modeling is a process in which domain knowledge and data are turned into models. 
B) Models are used to generate predictions. 
C) Understanding of a model structure may increase our knowledge, and in consequence it may lead to a better model. DALEX helps here.
D) Understanding of drivers behind a particular modelâ€™s predictions may help to correct wrong decisions, and in consequence it leads to a better model. DALEX helps here.](figure/mp_understanding.png)



Variable importance

Model response as a function of a variable

Model performance / diagnostic / validation


## How model exploration is different from data exploration?

Similarities:

* Models and data are related to some truth that we want to understand
* We use visuals to quicker understand complex relations


Differences:

* Data comes from some population; we treat them as a random sample. And since there is a sample there is also some randomness
* Models are just functions. Most models are not stochastic and we consider only deterministic models.
* We believe that data is noisy, but there is truth there. Data cannot be bad of misfitted, may be biased. Models can be bad, misfitted, unaccurate




## Glossary / Notation

model healthcheck


Let $f_{M}(x): \mathcal R^{d} \rightarrow \mathcal R$ denote a predictive model, i.e. function that takes $d$ dimensional vector and calculate numerical score. In section in which we work with larger number of models we use subscript $M$ to index models. But to simplify notation, this subscript is omitted if profiles for only one model are considered. 

Symbol $x \in \mathcal R^d$ refers to a point in the feature space. We use subscript $x_i$ to refer to a different data points and superscript $x^j$ to refer to specific dimensions. Additionally, let $x^{-j}$ denote all coordinates except $j$-th and let $x|^j=z$ denote a data point $x^*$ with all coordinates equal to $x$ except coordinate $j$ equal to value $z$. I.e. $\forall_{i \neq {j}} x^i = x^{*,i}$ and $x^j = z$. In other words $x|^j=z$ denote a $x$ with $j$th coordinate changed to $z$.

Now we can define Ceteris Paribus Profile for model $f$, variable $j$ and point $x$ as

$$
CP^{f, j, x}(z) := f(x|^j = z).
$$
I.e. CP profile is a model response obtained for observations created based on $x$ with $j$ coordinated changes and all other coordinates kept unchanged.

It is convenient to use an alternative name for this plot: What-If Plots. CP profiles show what would happen if only a single variable is changed.

Figure 5.1 shows an example of Ceteris Paribus profile. The black dot stands for prediction for a single observation. Grey line show how the model response would change if in this single observation coordinate `surface` will be changes to selected value. From this profile one may read that the model response is non monotonic. If `construction.year` for this observation would be below 1935 the model response would be higher, but if construction year were between 1935 and 1995 the model response would be lower.

Glossary:

* Black-box model
* White-box model
* Feature
* Variable
* Continuous variable
* Nominal variable
* Model-agnostic / Model specific



## Acknowledgements {#thanksto}

We are using the **bookdown** package [@R-bookdown] in this sample book

Chris Drake and Janusz Holyst

